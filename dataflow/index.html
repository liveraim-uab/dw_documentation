<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../img/favicon.ico">

    
    <title>Overview - LiverAim Data Warehouse</title>
    

    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/v4-shims.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3.3.0/build/web/hack.min.css">
    <link href='//rsms.me/inter/inter.css' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,400,300,600,700&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
    <link href="../css/base.min.css" rel="stylesheet">
    <link href="../css/cinder.min.css" rel="stylesheet">

    
        
        <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/styles/github.min.css">
        
    

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
            <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
            <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
        <![endif]-->

    

     
</head>

<body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->

            
              <a class="navbar-brand" href="..">LiverAim Data Warehouse</a>
            
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                
                
                    <li >
                        <a href="..">Home</a>
                    </li>
                
                
                
                    <li class="active">
                        <a href="./">Overview</a>
                    </li>
                
                
                
                    <li >
                        <a href="../liveraim_data_warehouse_specifications/">LIVERAIM Data Warehouse Specifications</a>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Modules documentation <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../modules_documentation/file_reading_utils_doc/">File Reading utils</a>
</li>

                        
                            
<li >
    <a href="../modules_documentation/data_processing_utils_doc/">Preprocessing utils</a>
</li>

                        
                            
<li >
    <a href="../modules_documentation/cohort_utils_doc/">Cohort utils</a>
</li>

                        
                            
<li >
    <a href="../modules_documentation/biomarker_utils_doc/">Biomkarker utils</a>
</li>

                        
                            
<li >
    <a href="../modules_documentation/qc_checks_utils_doc/">Quality Control utils</a>
</li>

                        
                            
<li >
    <a href="../modules_documentation/file_exporting_utils_doc/">File Exporting utils</a>
</li>

                        
                            
<li >
    <a href="../modules_documentation/sql_exporting_utils_doc/">SQL Exporting utils</a>
</li>

                        
                            
<li >
    <a href="../modules_documentation/configuration_module/">Configuration module</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li >
                        <a href="../quick_start_guide/">Quick Start Guide</a>
                    </li>
                
                
                </ul>

            <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                            <i class="fas fa-search"></i> Search
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="..">
                            <i class="fas fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="next" href="../liveraim_data_warehouse_specifications/">
                            Next <i class="fas fa-arrow-right"></i>
                        </a>
                    </li>
            </ul>
        </div>
    </div>
</div>

    <div class="container">
        
        
        <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="first-level active"><a href="#overview">Overview</a></li>
            <li class="second-level"><a href="#dataflow">Dataflow</a></li>
                
            <li class="second-level"><a href="#data-reading">Data Reading</a></li>
                
            <li class="second-level"><a href="#data-preprocessing">Data Preprocessing</a></li>
                
                <li class="third-level"><a href="#creation-of-vardata-object">Creation of VarData object</a></li>
                <li class="third-level"><a href="#merging-database-versions">Merging database versions</a></li>
                <li class="third-level"><a href="#id-transformations">ID transformations</a></li>
                <li class="third-level"><a href="#ids-dropping">IDs dropping</a></li>
                <li class="third-level"><a href="#addition-of-new-variables">Addition of new variables</a></li>
                <li class="third-level"><a href="#combination-of-variables">Combination of variables</a></li>
                <li class="third-level"><a href="#cohort-specific-transformations">Cohort-specific transformations</a></li>
            <li class="second-level"><a href="#cohort-instantiation-and-processing">Cohort Instantiation and Processing</a></li>
                
            <li class="second-level"><a href="#cohort-quality-control">Cohort Quality Control</a></li>
                
            <li class="second-level"><a href="#cohort-merging">Cohort Merging</a></li>
                
            <li class="second-level"><a href="#biomarker-data-management">Biomarker data management</a></li>
                
                <li class="third-level"><a href="#data-reading_1">Data Reading</a></li>
                <li class="third-level"><a href="#data-processing">Data Processing</a></li>
                <li class="third-level"><a href="#provider-specific-transformations">Provider-specific transformations</a></li>
            <li class="second-level"><a href="#calculation-of-computed-biomarkers">Calculation of computed Biomarkers</a></li>
                
            <li class="second-level"><a href="#exportation-to-files">Exportation to Files</a></li>
                
            <li class="second-level"><a href="#exportation-to-mysql-database">Exportation to MySQL Database</a></li>
                
    </ul>
</div></div>
        <div class="col-md-9" role="main">

<h1 id="overview">Overview</h1>
<h2 id="dataflow">Dataflow</h2>
<p>This section describes the data flow and ETL process, from the data reading to data export in various formats.</p>
<p>The data processing follows these steps, which are explained in detail in this document:</p>
<ul>
<li><a href="#data-reading"><strong>Data Reading</strong></a>: Reading of all data required for the creation of the data warehouse. Specifically, the files described in <a href="../liveraim_data_warehouse_specifications/#initial-data-and-configuration-data">initial_data_configuration</a> are read.</li>
<li><a href="#data-preprocessing"><strong>Data Preprocessing</strong></a>: An initial transformation of the data is performed, where some variables are added and combined. This process allows each cohort to be treated individually and specific changes to be applied to that cohort.</li>
<li><a href="#cohort-instantiation-and-processing"><strong>Cohort Instantiation and Processing</strong></a>: For each cohort, a <code>Cohort</code> object is created. This centralizes the formatting and homogenization of the cohorts to produce a curated database with a consistent format across all cohorts.</li>
<li><a href="#cohort-quality-control"><strong>Cohort Quality Control</strong></a>: Quality control is performed for each cohort to validate the transformations carried out in the previous steps.</li>
<li><a href="#cohort-merging"><strong>Cohort Merging</strong></a>: Once the different cohorts with homogenized data have been created, they are merged into a new <code>Cohort</code> object (named <code>liveraim</code>). Two important actions are performed in this cohort:<ul>
<li>The variable <code>liveraim_id</code> is added, a common format identifier.</li>
<li>The different panels (in <code>DataFrame</code> format) that will later be exported are created.</li>
</ul>
</li>
<li><a href="#biomarker-data-management"><strong>Biomarker data management</strong></a>: </li>
<li><a href="#exportation-to-files"><strong>Exportation to Files</strong></a>: The created panels are exported as <code>.feather</code> and <code>.csv</code> files for further analysis.</li>
<li><a href="#exportation-to-mysql-database"><strong>Exportation to MySQL Database</strong></a>: A connection is made to MySQL, where the various tables and relationships of the schema are created, and the data is exported to SQL format.</li>
</ul>
<p>The following image describes schematically the ETL process:</p>
<p><img alt="ETL-scheme" src="../images/ETL-scheme_v1.png" /></p>
<h2 id="data-reading">Data Reading</h2>
<p>Data reading is handled through the <a href="../modules_documentation/file_reading_utils_doc/"><code>DataReader</code></a> class. This class is responsible for reading the following files for each cohort:</p>
<ul>
<li><strong><code>var_data (.xlsx)</code></strong>: A .xlsx file that is loaded into the code as a <code>DataFrame</code>.</li>
<li><strong><code>level_data (.xlsx)</code></strong>: A .xlsx file that is loaded into the code as a <code>DataFrame</code>.</li>
<li><strong><code>comb_var_data (.json)</code></strong>: A .json file that is loaded into the code as a dictionary.</li>
<li><strong><code>databases (variable)</code></strong>: For each cohort, there will be one or more versions of the database. <code>DataReader</code> reads each version in its respective format (<code>.dta</code>, <code>.sav</code>, ...) and loads them into the code as <code>pandas</code> <code>DataFrames</code>. These dataframes are stored in a dictionary where the <em>key</em> is the version name (i.e., the date when the DB version was received) and the <em>value</em> is the DataFrame containing the database.</li>
</ul>
<p>Additionally, it reads the data related to the final structure of the panels. This is stored in the <code>panel_metadata (.xlsx)</code> file. Since this is a .xlsx file with multiple sheets, it is loaded into the code as a dictionary where the <em>key</em> is the sheet name (i.e., the name of the panel) and the <em>value</em> is a <code>DataFrame</code> with the content of the sheet.</p>
<p>These data are stored in the <code>DataReader.all_data</code> attribute, a dictionary with the following structure:</p>
<pre><code class="language-yaml">all_data:
  &lt;cohort_name_1&gt;:                      # Name of the cohort
    data: &lt;database_versions_dictionary&gt;    # Dictionary containing each database version
    var_data: &lt;var_data_file&gt;           # Excel file containing var_data
    level_data: &lt;level_data_file&gt;       # File containing level_data
    comb_var_data: &lt;comb_var_data_file&gt; # File containing comb_var_data
  &lt;cohort_name_2&gt;:                      # Name of the next cohort
    data: &lt;database_versions_dictionary&gt;    # Dictionary containing each database version
    var_data: &lt;var_data_file&gt;           # Excel file containing var_data
    level_data: &lt;level_data_file&gt;       # File containing level_data
    comb_var_data: &lt;comb_var_data_file&gt; # File containing comb_var_data
  ...
  panel_data: &lt;panel_data_file&gt;         # Dictionary with panel_data information  
</code></pre>
<p>This process is carried out for the data of each WP. In particular, each WP will contain not only the various data related to the cohort databases and configuration files but will also have its own panel configuration document, which may differ from those of other WPs. Therefore, when using the <code>DataReader</code> class, one must specify both the list of cohort names to be read and the WP they belong to (currently, only <code>WP1</code> and <code>WP2</code> can be specified).</p>
<blockquote>
<p><strong>Note 1</strong>: Currently, only data from the same WP1 can be read at once due to the structure of the code. In future versions, reading all cohorts at once will be possible (probably if no WP is provided).</p>
<p><strong>Note 2</strong>: The data warehouse will be created using all versions of the cohort databases, up to the most recent one. This allows for more detailed tracking of patients who have entered and exited in each version. How the different versions are managed is explained in the next section, <a href="#data-preprocessing">Data Preprocessing</a>.</p>
</blockquote>
<p>These files described above are essential for the correct processing of the data. To see the structure of these files, refer to the section <a href="../liveraim_data_warehouse_specifications/#initial-data-and-configuration-data">initial_data_configuration</a>.</p>
<p>The variables that store the filenames to be read, as well as the directories where these files are located, are defined in the module <a href="../modules_documentation/configuration_module/#main_config-module"><code>main_config</code></a>.</p>
<p>Refer to <a href="../modules_documentation/file_reading_utils_doc/"><code>data_reading_utils</code></a> for more details on the internal workings of <code>DataReader</code> and the data reading process.</p>
<h2 id="data-preprocessing">Data Preprocessing</h2>
<p>Once the initial data is loaded, an instance of the <code>DataPreprocessor</code> class is created for each cohort. This instance takes as parameters all the elements from <code>all_data[cohort_name]</code>, as well as the name of the respective cohort. This data is stored in the atributes <code>cohort_databases</code>, <code>vara_data</code>, <code>level_data</code>, <code>var_comb_data</code> and <code>cohort_name</code> respectively. </p>
<p><code>DataPreprocessor</code> performs the following actions:</p>
<ul>
<li><a href="#creation-of-vardata-object">Creation of <code>VarData</code> object</a></li>
<li><a href="#merging-database-versions">Merging database versions</a></li>
<li><a href="#id-transformations">ID transformations</a></li>
<li><a href="#addition-of-new-variables">Addition of new variables</a></li>
<li><a href="#combination-of-variables">Combination of variables</a></li>
<li><a href="#ids-dropping">IDs dropping</a></li>
<li><a href="#cohort-specific-transformations">Cohort-specific transformations</a></li>
</ul>
<p>The aim of this class is to prepare the data from each cohort for the formatting and homogenization process, which requires a specific structure. This class is designed to handle each cohort individually and allows for specific adjustments to be made for each cohort, adapting to the particularities of each one.</p>
<h4 id="creation-of-vardata-object">Creation of <code>VarData</code> object</h4>
<p>The <code>VarData</code> object is responsible for centralizing all the configuration data and metadata of the core variables (both the variables from the original cohort and those calculated during execution). It allows easy access to every variable property and facilitates the merging of different cohorts into one. For further information on the <code>VarData</code> object, its configuration, and functionality, please check the section <a href="../modules_documentation/metadata_utils_doc/#vardata">VarData</a>.</p>
<p>This object is instantiated for each cohort using the data contained in the <code>var_data</code> and <code>level_data</code> dataframes, as well as the <code>var_comb_dict</code> dictionary. Once created, it will be used to perform the proper transformation of the raw data (i.e., the previously mentioned objects <code>var_data</code>, <code>level_data</code>, and <code>var_comb_data</code> will no longer be used).</p>
<p>This object will be stored in the attribute <code>DataPreprocessor.varData</code> of the class to be used during data harmonization, quality control, and exportation.</p>
<h4 id="merging-database-versions">Merging database versions</h4>
<p>The dictionary containing the cohort database versions is stored in the attribute <code>DataPreprocessor.cohort_databases</code>. From the dataframes of each version, a <strong>single dataframe</strong> will be generated that contains, for each patient present in at least one of the versions, the latest available data (i.e., the data from the most recent version in which they appear).</p>
<p>Additionally, the variable <code>status</code> is added. This categorical variable can take 3 values:</p>
<ul>
<li><code>finished</code>: Patients who appear in the latest version of the cohort database, and the cohort is already closed.</li>
<li><code>ongoing</code>: Patients who appear in the latest version of the cohort database, and the cohort is still ongoing.</li>
<li><code>withdrawn</code>: Patients who do not appear in the latest version of the cohort database.</li>
</ul>
<p>The variable <code>date_version (str)</code> is also added, indicating the version from which each patient's data is extracted.</p>
<blockquote>
<p><strong>Note</strong>: During this process, it is required that the variables in the databases (and their names) do not change between versions. Checks are performed to ensure compatibility between versions, and if they are not compatible, only the DataFrame form the latest version will be used, and the previous ones will be ignored.</p>
</blockquote>
<p>To obtain this <strong>single dataframe</strong>, the following steps are followed:</p>
<ul>
<li>The <code>date_version</code> column is assigned to each dataframe from each version. This column will contain the version to which that data corresponds (i.e., all columns in the same dataframe will have the same value).</li>
<li>All versions of the database are concatenated (vertically). This requires the structure of the dataframes to be compatible.</li>
<li>The rows are sorted according to the <code>date_version</code> variable.</li>
<li>Rows with duplicate values in the <code>id_var</code> column (the patient identifier) are removed, keeping the latest <code>date_version</code>.</li>
<li>The <code>status</code> variable is added by verifying if the <code>date_version</code> of each patient corresponds to the latest version.</li>
</ul>
<p>Since <code>status</code> will be one of the core variables in the data warehouse, the <code>varData</code> attribute is updated to include this variable.</p>
<p>this new dataframe is stored in <code>data</code> attribute.</p>
<h4 id="id-transformations">ID transformations</h4>
<p>The format of patient identification variables can vary significantly between cohorts (even at the intra-cohort level), and the identifiers from laboratories may present (and do present) certain inconsistencies and errors compared to the cohort identifiers.<br />
To address this issue, the ID variables of all cohorts are transformed, ensuring that there is a correspondence between the IDs from the cohorts and those from the laboratory data. The following transformations are performed:</p>
<ul>
<li><strong><code>Addition of 0's</code></strong>: A significant number of IDs contain a hyphen in the middle of the identifier (<em>e.g.</em>, <code>1055-12</code>). This leads to errors if, for example, the same patient is registered as <code>1055-0012</code> in another database. To prevent this, zeros are added after the hyphen to ensure that all identifiers have the same length (after the hyphen). In the current version, this length has been set to 5. Thus, after the transformation, both identifiers would be converted to <code>1055-00012</code>.</li>
<li><strong><code>Deletion of blank spaces</code></strong>: All blank spaces in the IDs are removed.</li>
<li><strong><code>Deletion of typos</code></strong>: Some symbols, such as <code>º, $</code>, etc., are considered typos and are removed from the ID if present.</li>
<li><strong><code>Uppercasing</code></strong>: All letters in the identifiers are converted to uppercase (if they are not already). Thus, the identifiers such as <code>glu1234</code> and <code>GLU1234</code> would both be transformed into <code>GLU1234</code>.</li>
</ul>
<h4 id="ids-dropping">IDs dropping</h4>
<p>Some patients must be removed from the database, often because they have duplicate entries for certain values. This may occur, for example, due to errors when entering IDs into the databases. The file <code>ids_to_drop.txt</code>, located in the <code>data</code> directory (see <a href="../quick_start_guide/#structure-of-the-project-directory">Structure of the Project Directory</a>), contains the list of IDs that must be removed from the final DW.  </p>
<p>During the preprocessing of each cohort, the system checks whether the identifiers listed in this file are present in the database, and if so, their entries are deleted.</p>
<h4 id="addition-of-new-variables">Addition of new variables</h4>
<p>Once a single dataframe with the database is created, the following variables are added to the <code>data</code> dataframe attribute:</p>
<ul>
<li><code>birth_date</code>: Obtained by combining the patient's age with the date of entry into the cohort.</li>
<li><code>exit_date</code>: Date of exit from the cohort if the patient is not ongoing, or the current date at the time of code execution otherwise.</li>
<li><code>cohort</code>: The name of the cohort to which the data belongs.</li>
<li><code>work_package</code>: The work package which the cohort comes from. </li>
</ul>
<blockquote>
<p><strong>Note</strong>: Just as with the <code>status</code> variable, since these new variables will be core variables, the <code>varData</code> attribute will be updated accordingly (adding the metadata of the respective variable).</p>
</blockquote>
<h4 id="combination-of-variables">Combination of variables</h4>
<p>If the <code>var_comb_data</code> parameter is passed during the initialization of <code>DataPreprocessor</code>, it will be used to combine the variables specified in that parameter. The primary goal here is to reduce the number of missing values by using variables that refer to the same magnitude but have different units.</p>
<p>To achieve this, <code>DataPreprocessor</code> utilizes the <code>VarCombiner</code> class, which is instantiated with a dataframe (in this case, the <code>data</code> attribute) and a dictionary that defines the variable combinations (in this case the <code>var_comb_dict</code> attribute). It is recommended to consult the section <a href="../liveraim_data_warehouse_specifications/#comb_var_data-file"><code>comb_var_data</code> file</a> for details on the internal structure of the dictionary. Additionally, refer to the section <a href="../modules_documentation/data_processing_utils_doc/#class-varcombiner"><code>Class VarCombiner</code></a> for more information on how the class works.</p>
<p>For each variable to be combined, <code>VarCombiner</code> sequentially checks whether a variable with a certain magnitude is missing. If it is not missing, the value of that variable is taken as the final value (applying the appropriate conversion factor). If the value is missing, it moves on to the next variable referring to the same magnitude (but with different units) and performs the same check. If all variables in the iteration are missing, <code>NaN</code> is assigned as the final value.</p>
<p>To illustrate this, suppose the <code>var_comb_data</code> dictionary is as follows:</p>
<pre><code>{
    "glc": {
        "glc": 1,
        "glc_mg_dl": 0.055
    },
    "crea_mg_dl": {
        "crea_mg_dl": 1,
        "creat": 0.017
    }
}
</code></pre>
<p>This indicates that two variables should be combined: <code>glc</code> (using <code>glc</code> and <code>glc_mg_dl</code>) and <code>crea_mg_dl</code> (using <code>creat</code> and <code>crea_mg_dl</code>). <code>VarCombiner</code> will first iterate over the pair <code>"glc": {"glc": 1, "glc_mg_dl": 0.055}</code>. The <code>glc</code> variable (the first key) will be calculated as described above: if <code>glc</code> (in this case, the key in the pair <code>"glc": 1</code>) is not missing, this value will be used for the <code>glc</code> variable. Otherwise, it will check if <code>glc_mg_dl</code> is missing. If it is not missing, the value of <code>glc_mg_dl</code> multiplied by 0.055 will be used as the final value (where 0.055 is the conversion factor to convert <code>glc_mg_dl</code> units to <code>glc</code> units).</p>
<p>The processing for <code>crea_mg_dl</code> is entirely analogous.</p>
<p>The final result is that in the <code>data</code> attribute, the <code>glc</code> and <code>crea_mg_dl</code> variables will be populated with values from other <em>compatible</em> variables.</p>
<h4 id="cohort-specific-transformations"><strong>Cohort-specific transformations</strong></h4>
<p>The following section describes the specific modifications applied to each cohort:</p>
<h5 id="liverscreen"><strong>Liverscreen</strong></h5>
<p><em>Temporary</em>: Currently, one of the database versions (<code>20241223</code>) includes the <code>Ever smoker</code> level in the <code>smoke</code> variable, which was not present in previous versions. This level is encoded with the value <code>1.5</code>. These values are mapped to the string <code>"Ever smoker"</code>, a notation that is more consistent with the rest of the versions.  </p>
<h5 id="decide"><strong>Decide</strong></h5>
<p>Some patients in the Decide cohort are also present in the Liverscreen cohort (those whose ID starts with <code>1056</code>). These patients are removed from the cohort to avoid duplicates.</p>
<h5 id="alcofib"><strong>Alcofib</strong></h5>
<p>Some heterogeneity in variables with percentage units can be found at the cohort level. This means that, for the same variable (with percentage units), some values are given as actual <code>%</code>, while others appear to be given per unit (i.e., as a proportion). To address this issue, the <code>harmonize_percent</code> function is called, which transforms all the values into percentages. </p>
<p>To achieve this, the function multiplies by 100 all the values below a certain threshold (currently set to 1), assuming that any value lower than 1 is given as a proportion rather than a percentage.</p>
<blockquote>
<p><strong>Warning</strong>: This "two units" situation is inferred from the nature of the data and the variables. This means that there has been no strict confirmation of this issue yet. However, as it is the best explanation for the inconsistency, the transformation is applied. Ideally, this should be verified with the individuals responsible for the cohorts where these transformations are performed.</p>
</blockquote>
<h5 id="glucofib"><strong>Glucofib</strong></h5>
<p>Similarly to the Alcofib cohort, some heterogeneity in variables with percentage units can be found at the cohort level. This means that, for the same variable (with percentage units), some values are given as actual <code>%</code>, while others appear to be given per unit (i.e., as a proportion). To address this issue, the <code>harmonize_percent</code> function is called, which transforms all the values into percentages. </p>
<p>To achieve this, the function multiplies by 100 all the values below a certain threshold (currently set to 1), assuming that any value lower than 1 is given as a proportion rather than a percentage.</p>
<blockquote>
<p><strong>Warning</strong>: This "two units" situation is inferred from the nature of the data and the variables. This means that there has been no strict confirmation of this issue yet. However, as it is the best explanation for the inconsistency, the transformation is applied. Ideally, this should be verified with the individuals responsible for the cohorts where these transformations are performed.</p>
</blockquote>
<h5 id="marina1"><strong>Marina1</strong></h5>
<p>No transformations specific to this cohort are currently performed.</p>
<h5 id="metronord"><strong>Metronord</strong></h5>
<p>No transformations specific to this cohort are currently performed.</p>
<h5 id="gala-ald"><strong>Gala-ald</strong></h5>
<p>No transformations specific to this cohort are currently performed.</p>
<h2 id="cohort-instantiation-and-processing">Cohort Instantiation and Processing</h2>
<p>Once the cohort data has been preprocessed, it is formatted so that all cohorts have a homogeneous structure and can be subsequently combined. The central element of this processing is the <code>Cohort</code> class, and the <code>VarData</code> object mentioned earlier is essential for its correct operation.</p>
<p>For each cohort, a <code>Cohort</code> object is instantiated, which uses the (already preprocessed) attributes of <code>DataPreprocessor</code>: <code>data</code> (as the <code>raw_data</code> parameter), and <code>VarData</code>. It is important to note that the <code>VarData</code> object contains a dictionary of <code>Var</code> objects (for further information, check the section <a href="../modules_documentation/metadata_utils_doc/#var">Var</a>). Each of these <code>Var</code> objects contains data from a particular variable that will be used during the harmonization process. It contains the following information (it is not an exhaustive list):</p>
<ul>
<li>Initial variable name in the cohort.</li>
<li>Final variable name in the common database.</li>
<li>Final variable datatype.</li>
<li>Conversion factor (if needed).</li>
<li>mapping dictionary for categorical variables</li>
<li>...</li>
</ul>
<p>Additionally, the cohort name (parameter <code>cohort_name</code>), the ID variable name (parameter <code>id_variable</code>), the inclusion date variable name (parameter <code>date_variable</code>), and the cohort status (parameter <code>status</code>) are introduced as parameters. Optionally, a dictionary with the structure of <code>comb_var_data</code> can also be provided. For more detailed information about these objects, refer to the section <a href="../liveraim_data_warehouse_specifications/#comb_var_data-file"><code>initial_data_configuration</code></a>.</p>
<p>The instantiation of the cohort concludes with a call to the <code>Cohort.homogenize_data</code> method. This method creates a copy of the <code>raw_data</code> attribute (which was initialized with the preprocessed cohort database, i.e., the <code>raw_data</code> parameter), formats it, and stores it in the <code>homogeneous_data</code> attribute. In general terms, the data formatting follows these steps:</p>
<ol>
<li><strong>Selection</strong> of the subset of <code>raw_data</code> with the core variables.</li>
<li><strong>Translation</strong> of the variable names (columns of the DataFrame) to the final names defined in <code>VarData</code>.</li>
<li><strong>Formatting</strong> of the variables. It iterates over the <code>VarData</code> for each of the selected variables and:<ol>
<li>Verifies the datatype assigned in <code>Var</code> and applies the appropriate datatype to that column.</li>
<li>If the variable is numeric, applies the appropriate conversion factor.</li>
<li>If the variable is categorical, maps the original levels to the final levels. This mapping is defined in the attribute <code>Var.var_map</code>.</li>
</ol>
</li>
</ol>
<p>The result for each cohort should be the same: a DataFrame with the same structure and format stored in the <code>homogeneous_data</code> attribute. Currently, the structure of the database allows cohorts with different variables each.</p>
<p>For more details on the specific implementation of the <code>Cohort</code> class and its methods, refer to the section dedicated to the <a href="../modules_documentation/cohort_utils_doc/">cohort_utils</a>.</p>
<h2 id="cohort-quality-control">Cohort Quality Control</h2>
<p>After the creation and harmonization of each <code>cohort</code> object, a quality control check is performed and stored in an instance of <code>QCchecker</code> object. For more details on the specific implementation of the QC checks, refer to the section  <a href="../modules_documentation/qc_checks_utils_doc/">qc_checks_utils_doc</a>.</p>
<h2 id="cohort-merging">Cohort Merging</h2>
<p>Once all cohorts have been formatted, they are merged using the <code>merge_cohorts</code> function, defined in the <a href="../modules_documentation/cohort_utils_doc/"><code>cohort_utils</code></a> module. <code>merge_cohorts</code> takes a list of already instantiated cohorts (i.e., they contain the <code>homogeneous_data</code> attribute) and performs the following actions:</p>
<ol>
<li>Concatenates (vertically) the <code>homogeneous_data</code> attributes (DataFrames) from each cohort, ensuring that the resulting DataFrame combines all the data from the individual cohorts.</li>
<li>Instantiates a ConfigDataManager object, which allows to merge properly all the <code>VarData</code> objects from each cohort.</li>
<li>Instantiates a new <code>Cohort</code> object using the previously created merged elements. This cohort is named <code>liveraim</code> from now on.</li>
</ol>
<p>Once the new <code>cohort</code> object is created, the <code>set_id</code> method is called. This method generates a new column in the <code>homogeneous_data</code> dataframe attribute, populated with a unified ID for each patient. The structure of this new ID follows the format <code>LAxxxxx</code>, where <code>xxxxx</code> is a sequential number starting from 1 and increasing up to the total number of rows in the dataframe (i.e., the position of the last patient in the dataframe). The IDs are assigned in ascending order throughout the dataframe.</p>
<blockquote>
<p><strong>Warning</strong>: As a result, the correspondence between the original ID (referred to here as <code>cohort_id</code>) and the unified ID (referred to here as <code>liveraim_id</code>) may vary depending on the position of each patient’s data within the <code>homogeneous_data</code> dataframe. This implies that the mapping between <code>cohort_id</code> and <code>liveraim_id</code> can change between executions.</p>
</blockquote>
<p>For more details on the functionality of this process, refer to the section <a href="../modules_documentation/cohort_utils_doc/#merge_cohorts-function"><code>merge_cohorts</code> function</a>.</p>
<h2 id="biomarker-data-management">Biomarker data management</h2>
<p>To check the structure of the processed biomarkers data panel check the subsection <a href="../liveraim_data_warehouse_specifications/#table-biomarkers">Biomarkers table</a>. There are two different sources of biomarkers data:</p>
<ul>
<li>
<p><strong>Partners</strong>: third party partners (name Nordic, Roche and Hospital Clínic) are responsible for the analysis and transfer of the biomarkers data, that will be then processed and integrated into the data warehouse. This process is described in the following sections <a href="#data-reading-1">Data Reading</a> and <a href="#data-processing">Data Processing</a>.</p>
</li>
<li>
<p><strong>Data Warehouse</strong>: Some biomarkes are computed using the variables from the data warehouse, formatted and merged with the biomarkers data obtained from the partners. This process is briefly described in <a href="#calculation-of-computed-biomarkers">Calculation of computed Biomarkers</a>.</p>
</li>
</ul>
<h4 id="data-reading_1">Data Reading</h4>
<p>The processing of biomarker data is performed immediately after the processing of cohort data, as the cohort IDs are used to validate the IDs in the biomarker data.</p>
<p>Biomarker data is received from different providers, namely Nordic, Roche, and Hospital Clínic, in the current version. Since each provider may send the data split into different batches, and similarly to the cohort data case, the reading process for biomarker data is handled by the <code>BMKDataReader</code> class. For each provider, an instance of <code>BMKDataReader</code> is created, resulting in a dictionary (stored in the <code>bmk_data_dict</code> attribute) where the keys are the version dates (batches) and the values are the dataframes containing the data. </p>
<p>For more details on the organization of the biomarker data directory and the specifics of the reading process, please check the section <a href="../modules_documentation/biomarker_utils_doc/#class-bmkdatareader">class <code>DataReader</code></a>.</p>
<h4 id="data-processing">Data Processing</h4>
<p>Once read, an instance of <code>BMKDataProcessor</code> is created for each provider. This instance is responsible for harmonizing and processing the biomarker data so it can be merged with data from other biomarkers. To instantiate this object, two main parameters are required, in addition to the <code>biomarkers_data</code>: the name of the provider (which allows calling the specific configuration class to process its data) and the set of common Liveraim IDs from the cohort database. The following section describes the general transformations applied to the data from each provider:</p>
<ul>
<li>
<p>For each version/batch received from the provider, the following transformations are applied:</p>
<ul>
<li>
<p>The columns of the DataFrame are <strong>renamed</strong> to follow a common naming convention.</p>
</li>
<li>
<p><strong>Provider-specific transformations</strong> are applied via the <code>BMKHarmonizer</code> class. This will be covered in more detail below. The result of this transformation is a DataFrame with a common format and structure, both among batches from the same provider and between different providers.</p>
</li>
<li>
<p>The <strong>original IDs are stored</strong> to generate a mapping between these IDs and the harmonized ones. This is mainly for debugging and traceability purposes — it allows tracing back the original IDs in the biomarker data that do not match the IDs in the cohort database.</p>
</li>
<li>
<p><strong>ID harmonization</strong>: The IDs are transformed to fix inconsistencies between cohort data and biomarker data. The transformations applied are described in the section <a href="#id-transformations"><strong>ID transformations</strong></a>.</p>
</li>
<li>
<p><strong>Duplicate removal</strong>: If any duplicate records for ID and biomarker are found within a batch, they are dropped and removed from the DataFrame, retaining none of them.</p>
</li>
<li>
<p><strong>Blinding of biomarkers</strong>: The biomarker names (column <code>variable</code>) are mapped to anonymize them. In the current version, they are mapped to uppercase letters.</p>
</li>
<li>
<p>A <strong>quality control</strong> process is performed on the transformed data and stored as a DataFrame in a dictionary, whose key will be the version/batch name. These checks are detailed below.</p>
</li>
</ul>
</li>
<li>
<p>The DataFrames from each version are then concatenated (vertically) to obtain a single DataFrame (which may contain duplicates).</p>
</li>
<li>The dictionary mapping the original IDs to the transformed IDs is created and stored in the <code>ids_transformation_dict</code> attribute.</li>
<li>The appropriate data type is applied to each column.</li>
<li>
<p><strong>Consistency checks</strong>: Two main checks are performed at this point:</p>
<ul>
<li>
<p><strong>ID matching</strong>: The IDs in the biomarker data are matched with those provided from the cohort data. If an ID from the biomarker data does not match any of the cohort IDs, it is dropped from the DataFrame and ignored.</p>
</li>
<li>
<p><strong>Duplicate removal</strong>: If any duplicates are found for the same biomarker and ID, the duplicates are dropped (currently keeping only the entry with the latest <code>validation_date</code>). This is due to the fact that there may be patient overlap between batches from the same provider.</p>
</li>
</ul>
</li>
<li>
<p>A new column (<code>liveraim_id</code> by default) is generated by mapping the transformed IDs to the Liveraim IDs from the cohort database. At this point, some identifiers might not match any of the provided Liveraim IDs (and will be subsequently dropped from the final data).</p>
</li>
<li>
<p><strong>Missing values drop</strong>: Rows with missing numeric values for a given biomarker are excluded from the DataFrame.</p>
</li>
<li>
<p><strong>Scaling</strong>: The values for each biomarker (currently in the <code>value</code> column) are scaled into a specified range ([0, 1] by default). The current version uses a Min-Max scaling method:</p>
</li>
</ul>
<div class="arithmatex">\[
X'_i = \frac{X_i - X_{min}}{X_{max} - X_{min}}
\]</div>
<ul>
<li>
<p><strong>Normalization</strong>: The z-score of the numeric value (grouped by biomarker) is computed and added as a column. To do so, the following formula is used:</p>
<div class="arithmatex">\[
X'_i = \frac{X_i - \mu}{\sigma}
\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(\mu\)</span> is the mean of the values for the given biomarker.  </li>
<li><span class="arithmatex">\(\sigma\)</span> is the standard deviation of the values for the given biomarker.  </li>
</ul>
</li>
<li>
<p><strong>Quintiles</strong>: quintiles are computed (grouped by biomarker) and added as a column. Labelled from 1 to 5. </p>
</li>
</ul>
<p>As mentioned above, this process is performed separately for the data from each provider. Finally, all the DataFrames (with compatible structures) are merged into a single one, ready to be exported.</p>
<h4 id="provider-specific-transformations">Provider-specific transformations</h4>
<h5 id="hospital-clinic"><strong>Hospital Clínic</strong></h5>
<p>The raw data from this provider may include the symbols <code>&lt;</code> or <code>&gt;</code> in the numeric value field of some biomarkers. These symbols indicate whether the value is below or above the quantification limit. Values without any symbol are considered to fall within the detectable range. Accordingly, the main specific transformations applied are:</p>
<ul>
<li>
<p><strong>Comments column</strong>: A new column called <code>comments</code> is created based on the biomarker value column. If any of the above symbols are present, the corresponding comment is added to this column. If no symbol is found, the default value is an empty string.</p>
</li>
<li>
<p><strong>Symbols processing</strong>: The symbol (if any) is extracted from the biomarker value and stored in a new column named <code>limit_detect</code>, which contains only the symbol. If no symbol is present, the default value is <code>pd.NA</code>. Additionally, a new column called <code>numeric_value</code> is created, containing only the numeric part of the biomarker value (as <code>float</code> type).</p>
</li>
</ul>
<h5 id="nordic"><strong>Nordic</strong></h5>
<ul>
<li>
<p><strong>Biomarker value formatting</strong>: In the biomarker value column, values with the string "ND" (indicating a missing value) are replaced with an empty string. Then, the column is converted to <code>float</code>, assigning <code>np.nan</code> to empty strings.</p>
</li>
<li>
<p><strong>Addition of <code>limit_detect</code> column</strong>: Based on the <code>comments</code> column in the raw data provided by Nordic, the new column called <code>limit_detect</code> is created, following the convention described in <a href="#hospital-clínic">Hospital Clínic transformations</a>.</p>
</li>
<li>
<p><strong>Comments mapping</strong>: Comments in the <code>comments</code> column are mapped to numeric codes using dictionaries defined in the configuration module.</p>
</li>
<li>
<p><strong>Value clipping</strong>: The raw data from Nordic includes the value of the analysis (<code>numeric_value</code>), even when it falls outside the detection range. To ensure consistency and preserve the structure of the panel, values outside the detection limits (those bounds have been provided by Nordic) are replaced with the appropriate limit value:</p>
<ul>
<li>Values below the detection limit are replaced with the lower detection limit.</li>
<li>Values above the detection limit are replaced with the upper detection limit.</li>
</ul>
</li>
</ul>
<h5 id="roche"><strong>Roche</strong></h5>
<p><strong><em>No data from Roche has been received yet - Not yet implemented</em></strong></p>
<h2 id="calculation-of-computed-biomarkers">Calculation of computed Biomarkers</h2>
<p>As some biomarkers are analyzed and provided by external partners, certain biomarkers need to be computed using variables from the current data warehouse. The following section briefly describes the process of computing and formatting these biomarkers.</p>
<p>The class responsible for performing these computations is <code>BMKCalculator</code>, which is instantiated with the already processed data warehouse (in particular, with a <code>Cohort</code> object). For more details on the implementation of this class, refer to the section <a href="../modules_documentation/biomarker_utils_doc/#class-bmkcalculator"><code>BMKCalculator</code> class</a>.</p>
<p>For each biomarker that needs to be computed, a specific Python function has been implemented. This function returns a <code>pd.Series</code> object with the computed values. The following steps are performed to obtain a compatible DataFrame that can be merged with the one generated by <code>BMKDataProcessor</code>:</p>
<ul>
<li>
<p>A biomarker-specific function (already defined) is applied to compute the values using the harmonized data warehouse. The resulting values are stored in the <code>numeric_value</code> column.</p>
</li>
<li>
<p>The <code>variable</code> column, containing the name of the biomarker (in the "blinded" form), is added.</p>
</li>
<li>
<p>The required columns (<code>liveraim_id</code>, <code>inclusion_date</code>, <code>numeric_value</code>, and <code>variable</code>) are selected and assembled into a DataFrame.</p>
<blockquote>
<p><strong>Note</strong>: Date used as <code>validation_date</code> variable for computed biomarkers will be, at this version, the <code>inclusion_date</code> variable.</p>
</blockquote>
</li>
<li>
<p>The columns <code>scaled_value</code>, <code>quintiles</code>, and <code>z_score</code> are added as described in the <a href="#data-processing">Biomarker Data Processing</a> section.</p>
</li>
<li>
<p>The columns <code>comments</code> and <code>limit_detect</code> are added to match the common structure.</p>
<blockquote>
<p><strong>Warning</strong>: In the current version, no detection limits are available for the computed biomarkers. Accordingly, all records will contain default values in the <code>limit_detect</code> and <code>comments</code> columns (<code>pd.NA</code> and <code>""</code>, respectively). Furthermore, no clipping is applied to the biomarker values.</p>
</blockquote>
</li>
</ul>
<p>Each biomarker data results in an individual DataFrame. These DataFrames are then merged into a single, consistent DataFrame that can be seamlessly combined with those produced by <code>BMKDataProcessor</code>.</p>
<h2 id="exportation-to-files">Exportation to Files</h2>
<p>If the variable <code>EXPORT_FILES</code> defined in the <code>main_config</code> module is set to <code>True</code>, the data for each of the final panels will be exported as files. In the current version, the data is exported to <code>.csv</code> and <code>.feather</code> formats.</p>
<h2 id="exportation-to-mysql-database">Exportation to MySQL Database</h2>
<p>After exporting the database as files, the MySQL database is created. The module responsible for handling this export is <a href="../modules_documentation/sql_exporting_utils_doc/"><code>sql_exporting_utils</code></a>. Specifically, the <code>SQLExporter</code> class, defined in this module, centralizes the connection to the database, the creation of tables, and the export of data.</p>
<p>The <code>SQLExporter</code> class is initialized using the <code>panel_metadata</code> DataFrame dictionary (this object has not been modified at any point). This dictionary specifies the structure of each of the final panels in the database. For more information on the structure of the <code>panel_metadata</code> file (and the corresponding <code>panel_metadata</code> object), see the section <a href="../liveraim_data_warehouse_specifications/#panel_data-file"><code>panel_data</code> file</a>.</p>
<p><code>SQLExporter</code> performs the following actions sequentially:</p>
<ol>
<li><strong>Creates an <code>engine</code> object</strong>, which establishes the connection to the database based on the configuration specified in the <code>connection_config</code> module. For more information on the connection parameters, see the section <a href="../modules_documentation/configuration_module/#connection_config-module"><code>connection_config</code> module</a>.</li>
<li><strong>Creates the database structure</strong>: It defines the tables, the format of each table (which variables each contains, whether it's in long or wide format, etc.), and the relationships between them. To do this, it uses the configuration data  present in the <code>panel_metadata</code> object.</li>
<li>Establishes the connection to the database and generates the previously defined tables in the MySQL database.</li>
<li>Iterates over each of the DataFrames in <code>liveraim.final_data</code> and inserts the data into the corresponding table.</li>
</ol>
<blockquote>
<p><strong>Note</strong>: The <code>engine</code> object does not immediately create the connection, but dynamically manages connections as needed. Therefore, until the tables are explicitly created in the database (and then populated), it does not actually connect to the database. Similarly, step <em>2</em> is internal to the <code>SQLExporter</code> class, meaning that when the table and relationship structure is created, it is stored in an internal object within the class. It is in step <em>3</em> that this structure is executed when an explicit connection to the database is established.</p>
</blockquote></div>
        
        
    </div>

    
      <footer class="col-md-12 text-center">
          
          
            <hr>
            <p>
            <small>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</small>
            </p>
          

          
          
      </footer>
    
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="../js/bootstrap-3.0.3.min.js"></script>

    
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/highlight.min.js"></script>
        
    <script>hljs.initHighlightingOnLoad();</script>
    

    <script>var base_url = ".."</script>
    
    <script src="../js/base.js"></script>
    <script src="../javascripts/mathjax.js"></script>
    <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../search/main.js"></script>

    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal">
                    <span aria-hidden="true">&times;</span>
                    <span class="sr-only">Close</span>
                </button>
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>
    </body>

</html>
