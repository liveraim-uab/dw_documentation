{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Executive Summary The LIVERAIM warehouse is a high-quality, scalable, and interoperable data warehouse specifically designed for research on liver disease. Built to support clinical and biomedical research, the data warehouse integrates patient data from multiple cohorts and standardizes it into a unified, homogeneous format. It follows a ETL (Extract, Transform, Load) type process, ensuring data quality, consistency, and interoperability across multiple patient cohorts. The warehouse is composed of various tables, including patient demographics, medical history, blood test results, and physical examination details, which are stored in either wide or long formats. By using modern data processing tools and complying with security standards, LIVERAIM warehouse ensures that sensitive patient data is protected while providing a flexible and scalable research environment. The datasets can be exported in multiple formats and is compatible with major data analysis platforms, making it a valuable tool for the scientific and clinical communities involved in liver disease research. Table of Contents Overview A brief description of the data flow and key concepts. LIVERAIM Database Specifications Documentation about the LIVERAIM database structure. Modules Documentation File Reading utils - Utilities for reading files. Preprocessing utils - Functions and utilities for data preprocessing. Cohort Utils - Tools to create a Cohort object and format cohort data into a common structure. Biomarker Utils - Tools to process and check biomarkers data. Quality Control utils - Tools for data quality control. File Exporting utils - Modules for exporting files in various formats. SQL Exporting utils - Utilities for exporting data to SQL databases. Configuration module - Documentation for the configuration module. Quick Start Guide A guide to quickly get started with the documentation and modules.","title":"Home"},{"location":"#executive-summary","text":"The LIVERAIM warehouse is a high-quality, scalable, and interoperable data warehouse specifically designed for research on liver disease. Built to support clinical and biomedical research, the data warehouse integrates patient data from multiple cohorts and standardizes it into a unified, homogeneous format. It follows a ETL (Extract, Transform, Load) type process, ensuring data quality, consistency, and interoperability across multiple patient cohorts. The warehouse is composed of various tables, including patient demographics, medical history, blood test results, and physical examination details, which are stored in either wide or long formats. By using modern data processing tools and complying with security standards, LIVERAIM warehouse ensures that sensitive patient data is protected while providing a flexible and scalable research environment. The datasets can be exported in multiple formats and is compatible with major data analysis platforms, making it a valuable tool for the scientific and clinical communities involved in liver disease research.","title":"Executive Summary"},{"location":"#table-of-contents","text":"","title":"Table of Contents"},{"location":"#overview","text":"A brief description of the data flow and key concepts.","title":"Overview"},{"location":"#liveraim-database-specifications","text":"Documentation about the LIVERAIM database structure.","title":"LIVERAIM Database Specifications"},{"location":"#modules-documentation","text":"File Reading utils - Utilities for reading files. Preprocessing utils - Functions and utilities for data preprocessing. Cohort Utils - Tools to create a Cohort object and format cohort data into a common structure. Biomarker Utils - Tools to process and check biomarkers data. Quality Control utils - Tools for data quality control. File Exporting utils - Modules for exporting files in various formats. SQL Exporting utils - Utilities for exporting data to SQL databases. Configuration module - Documentation for the configuration module.","title":"Modules Documentation"},{"location":"#quick-start-guide","text":"A guide to quickly get started with the documentation and modules.","title":"Quick Start Guide"},{"location":"dataflow/","text":"Overview Dataflow This section describes the data flow and ETL process, from the data reading to data export in various formats. The data processing follows these steps, which are explained in detail in this document: Data Reading : Reading of all data required for the creation of the data warehouse. Specifically, the files described in initial_data_configuration are read. Data Preprocessing : An initial transformation of the data is performed, where some variables are added and combined. This process allows each cohort to be treated individually and specific changes to be applied to that cohort. Cohort Instantiation and Processing : For each cohort, a Cohort object is created. This centralizes the formatting and homogenization of the cohorts to produce a curated database with a consistent format across all cohorts. Cohort Quality Control : Quality control is performed for each cohort to validate the transformations carried out in the previous steps. Cohort Merging : Once the different cohorts with homogenized data have been created, they are merged into a new Cohort object (named liveraim ). Two important actions are performed in this cohort: The variable liveraim_id is added, a common format identifier. The different panels (in DataFrame format) that will later be exported are created. Biomarker data management : Exportation to Files : The created panels are exported as .feather and .csv files for further analysis. Exportation to MySQL Database : A connection is made to MySQL, where the various tables and relationships of the schema are created, and the data is exported to SQL format. The following image describes schematically the ETL process: Data Reading Data reading is handled through the DataReader class. This class is responsible for reading the following files for each cohort: var_data (.xlsx) : A .xlsx file that is loaded into the code as a DataFrame . level_data (.xlsx) : A .xlsx file that is loaded into the code as a DataFrame . comb_var_data (.json) : A .json file that is loaded into the code as a dictionary. databases (variable) : For each cohort, there will be one or more versions of the database. DataReader reads each version in its respective format ( .dta , .sav , ...) and loads them into the code as pandas DataFrames . These dataframes are stored in a dictionary where the key is the version name (i.e., the date when the DB version was received) and the value is the DataFrame containing the database. Additionally, it reads the data related to the final structure of the panels. This is stored in the panel_metadata (.xlsx) file. Since this is a .xlsx file with multiple sheets, it is loaded into the code as a dictionary where the key is the sheet name (i.e., the name of the panel) and the value is a DataFrame with the content of the sheet. These data are stored in the DataReader.all_data attribute, a dictionary with the following structure: all_data: <cohort_name_1>: # Name of the cohort data: <database_versions_dictionary> # Dictionary containing each database version var_data: <var_data_file> # Excel file containing var_data level_data: <level_data_file> # File containing level_data comb_var_data: <comb_var_data_file> # File containing comb_var_data <cohort_name_2>: # Name of the next cohort data: <database_versions_dictionary> # Dictionary containing each database version var_data: <var_data_file> # Excel file containing var_data level_data: <level_data_file> # File containing level_data comb_var_data: <comb_var_data_file> # File containing comb_var_data ... panel_data: <panel_data_file> # Dictionary with panel_data information This process is carried out for the data of each WP. In particular, each WP will contain not only the various data related to the cohort databases and configuration files but will also have its own panel configuration document, which may differ from those of other WPs. Therefore, when using the DataReader class, one must specify both the list of cohort names to be read and the WP they belong to (currently, only WP1 and WP2 can be specified). Note 1 : Currently, only data from the same WP1 can be read at once due to the structure of the code. In future versions, reading all cohorts at once will be possible (probably if no WP is provided). Note 2 : The data warehouse will be created using all versions of the cohort databases, up to the most recent one. This allows for more detailed tracking of patients who have entered and exited in each version. How the different versions are managed is explained in the next section, Data Preprocessing . These files described above are essential for the correct processing of the data. To see the structure of these files, refer to the section initial_data_configuration . The variables that store the filenames to be read, as well as the directories where these files are located, are defined in the module main_config . Refer to data_reading_utils for more details on the internal workings of DataReader and the data reading process. Data Preprocessing Once the initial data is loaded, an instance of the DataPreprocessor class is created for each cohort. This instance takes as parameters all the elements from all_data[cohort_name] , as well as the name of the respective cohort. This data is stored in the atributes cohort_databases , vara_data , level_data , var_comb_data and cohort_name respectively. DataPreprocessor performs the following actions: Creation of VarData object Merging database versions ID transformations Addition of new variables Combination of variables IDs dropping Cohort-specific transformations The aim of this class is to prepare the data from each cohort for the formatting and homogenization process, which requires a specific structure. This class is designed to handle each cohort individually and allows for specific adjustments to be made for each cohort, adapting to the particularities of each one. Creation of VarData object The VarData object is responsible for centralizing all the configuration data and metadata of the core variables (both the variables from the original cohort and those calculated during execution). It allows easy access to every variable property and facilitates the merging of different cohorts into one. For further information on the VarData object, its configuration, and functionality, please check the section VarData . This object is instantiated for each cohort using the data contained in the var_data and level_data dataframes, as well as the var_comb_dict dictionary. Once created, it will be used to perform the proper transformation of the raw data (i.e., the previously mentioned objects var_data , level_data , and var_comb_data will no longer be used). This object will be stored in the attribute DataPreprocessor.varData of the class to be used during data harmonization, quality control, and exportation. Merging database versions The dictionary containing the cohort database versions is stored in the attribute DataPreprocessor.cohort_databases . From the dataframes of each version, a single dataframe will be generated that contains, for each patient present in at least one of the versions, the latest available data (i.e., the data from the most recent version in which they appear). Additionally, the variable status is added. This categorical variable can take 3 values: finished : Patients who appear in the latest version of the cohort database, and the cohort is already closed. ongoing : Patients who appear in the latest version of the cohort database, and the cohort is still ongoing. withdrawn : Patients who do not appear in the latest version of the cohort database. The variable date_version (str) is also added, indicating the version from which each patient's data is extracted. Note : During this process, it is required that the variables in the databases (and their names) do not change between versions. Checks are performed to ensure compatibility between versions, and if they are not compatible, only the DataFrame form the latest version will be used, and the previous ones will be ignored. To obtain this single dataframe , the following steps are followed: The date_version column is assigned to each dataframe from each version. This column will contain the version to which that data corresponds (i.e., all columns in the same dataframe will have the same value). All versions of the database are concatenated (vertically). This requires the structure of the dataframes to be compatible. The rows are sorted according to the date_version variable. Rows with duplicate values in the id_var column (the patient identifier) are removed, keeping the latest date_version . The status variable is added by verifying if the date_version of each patient corresponds to the latest version. Since status will be one of the core variables in the data warehouse, the varData attribute is updated to include this variable. this new dataframe is stored in data attribute. ID transformations The format of patient identification variables can vary significantly between cohorts (even at the intra-cohort level), and the identifiers from laboratories may present (and do present) certain inconsistencies and errors compared to the cohort identifiers. To address this issue, the ID variables of all cohorts are transformed, ensuring that there is a correspondence between the IDs from the cohorts and those from the laboratory data. The following transformations are performed: Addition of 0's : A significant number of IDs contain a hyphen in the middle of the identifier ( e.g. , 1055-12 ). This leads to errors if, for example, the same patient is registered as 1055-0012 in another database. To prevent this, zeros are added after the hyphen to ensure that all identifiers have the same length (after the hyphen). In the current version, this length has been set to 5. Thus, after the transformation, both identifiers would be converted to 1055-00012 . Deletion of blank spaces : All blank spaces in the IDs are removed. Deletion of typos : Some symbols, such as \u00ba, $ , etc., are considered typos and are removed from the ID if present. Uppercasing : All letters in the identifiers are converted to uppercase (if they are not already). Thus, the identifiers such as glu1234 and GLU1234 would both be transformed into GLU1234 . IDs dropping Some patients must be removed from the database, often because they have duplicate entries for certain values. This may occur, for example, due to errors when entering IDs into the databases. The file ids_to_drop.txt , located in the data directory (see Structure of the Project Directory ), contains the list of IDs that must be removed from the final DW. During the preprocessing of each cohort, the system checks whether the identifiers listed in this file are present in the database, and if so, their entries are deleted. Addition of new variables Once a single dataframe with the database is created, the following variables are added to the data dataframe attribute: birth_date : Obtained by combining the patient's age with the date of entry into the cohort. exit_date : Date of exit from the cohort if the patient is not ongoing, or the current date at the time of code execution otherwise. cohort : The name of the cohort to which the data belongs. work_package : The work package which the cohort comes from. Note : Just as with the status variable, since these new variables will be core variables, the varData attribute will be updated accordingly (adding the metadata of the respective variable). Combination of variables If the var_comb_data parameter is passed during the initialization of DataPreprocessor , it will be used to combine the variables specified in that parameter. The primary goal here is to reduce the number of missing values by using variables that refer to the same magnitude but have different units. To achieve this, DataPreprocessor utilizes the VarCombiner class, which is instantiated with a dataframe (in this case, the data attribute) and a dictionary that defines the variable combinations (in this case the var_comb_dict attribute). It is recommended to consult the section comb_var_data file for details on the internal structure of the dictionary. Additionally, refer to the section Class VarCombiner for more information on how the class works. For each variable to be combined, VarCombiner sequentially checks whether a variable with a certain magnitude is missing. If it is not missing, the value of that variable is taken as the final value (applying the appropriate conversion factor). If the value is missing, it moves on to the next variable referring to the same magnitude (but with different units) and performs the same check. If all variables in the iteration are missing, NaN is assigned as the final value. To illustrate this, suppose the var_comb_data dictionary is as follows: { \"glc\": { \"glc\": 1, \"glc_mg_dl\": 0.055 }, \"crea_mg_dl\": { \"crea_mg_dl\": 1, \"creat\": 0.017 } } This indicates that two variables should be combined: glc (using glc and glc_mg_dl ) and crea_mg_dl (using creat and crea_mg_dl ). VarCombiner will first iterate over the pair \"glc\": {\"glc\": 1, \"glc_mg_dl\": 0.055} . The glc variable (the first key) will be calculated as described above: if glc (in this case, the key in the pair \"glc\": 1 ) is not missing, this value will be used for the glc variable. Otherwise, it will check if glc_mg_dl is missing. If it is not missing, the value of glc_mg_dl multiplied by 0.055 will be used as the final value (where 0.055 is the conversion factor to convert glc_mg_dl units to glc units). The processing for crea_mg_dl is entirely analogous. The final result is that in the data attribute, the glc and crea_mg_dl variables will be populated with values from other compatible variables. Cohort-specific transformations The following section describes the specific modifications applied to each cohort: Liverscreen Temporary : Currently, one of the database versions ( 20241223 ) includes the Ever smoker level in the smoke variable, which was not present in previous versions. This level is encoded with the value 1.5 . These values are mapped to the string \"Ever smoker\" , a notation that is more consistent with the rest of the versions. Decide Some patients in the Decide cohort are also present in the Liverscreen cohort (those whose ID starts with 1056 ). These patients are removed from the cohort to avoid duplicates. Alcofib Some heterogeneity in variables with percentage units can be found at the cohort level. This means that, for the same variable (with percentage units), some values are given as actual % , while others appear to be given per unit (i.e., as a proportion). To address this issue, the harmonize_percent function is called, which transforms all the values into percentages. To achieve this, the function multiplies by 100 all the values below a certain threshold (currently set to 1), assuming that any value lower than 1 is given as a proportion rather than a percentage. Warning : This \"two units\" situation is inferred from the nature of the data and the variables. This means that there has been no strict confirmation of this issue yet. However, as it is the best explanation for the inconsistency, the transformation is applied. Ideally, this should be verified with the individuals responsible for the cohorts where these transformations are performed. Glucofib Similarly to the Alcofib cohort, some heterogeneity in variables with percentage units can be found at the cohort level. This means that, for the same variable (with percentage units), some values are given as actual % , while others appear to be given per unit (i.e., as a proportion). To address this issue, the harmonize_percent function is called, which transforms all the values into percentages. To achieve this, the function multiplies by 100 all the values below a certain threshold (currently set to 1), assuming that any value lower than 1 is given as a proportion rather than a percentage. Warning : This \"two units\" situation is inferred from the nature of the data and the variables. This means that there has been no strict confirmation of this issue yet. However, as it is the best explanation for the inconsistency, the transformation is applied. Ideally, this should be verified with the individuals responsible for the cohorts where these transformations are performed. Marina1 No transformations specific to this cohort are currently performed. Metronord No transformations specific to this cohort are currently performed. Gala-ald No transformations specific to this cohort are currently performed. Cohort Instantiation and Processing Once the cohort data has been preprocessed, it is formatted so that all cohorts have a homogeneous structure and can be subsequently combined. The central element of this processing is the Cohort class, and the VarData object mentioned earlier is essential for its correct operation. For each cohort, a Cohort object is instantiated, which uses the (already preprocessed) attributes of DataPreprocessor : data (as the raw_data parameter), and VarData . It is important to note that the VarData object contains a dictionary of Var objects (for further information, check the section Var ). Each of these Var objects contains data from a particular variable that will be used during the harmonization process. It contains the following information (it is not an exhaustive list): Initial variable name in the cohort. Final variable name in the common database. Final variable datatype. Conversion factor (if needed). mapping dictionary for categorical variables ... Additionally, the cohort name (parameter cohort_name ), the ID variable name (parameter id_variable ), the inclusion date variable name (parameter date_variable ), and the cohort status (parameter status ) are introduced as parameters. Optionally, a dictionary with the structure of comb_var_data can also be provided. For more detailed information about these objects, refer to the section initial_data_configuration . The instantiation of the cohort concludes with a call to the Cohort.homogenize_data method. This method creates a copy of the raw_data attribute (which was initialized with the preprocessed cohort database, i.e., the raw_data parameter), formats it, and stores it in the homogeneous_data attribute. In general terms, the data formatting follows these steps: Selection of the subset of raw_data with the core variables. Translation of the variable names (columns of the DataFrame) to the final names defined in VarData . Formatting of the variables. It iterates over the VarData for each of the selected variables and: Verifies the datatype assigned in Var and applies the appropriate datatype to that column. If the variable is numeric, applies the appropriate conversion factor. If the variable is categorical, maps the original levels to the final levels. This mapping is defined in the attribute Var.var_map . The result for each cohort should be the same: a DataFrame with the same structure and format stored in the homogeneous_data attribute. Currently, the structure of the database allows cohorts with different variables each. For more details on the specific implementation of the Cohort class and its methods, refer to the section dedicated to the cohort_utils . Cohort Quality Control After the creation and harmonization of each cohort object, a quality control check is performed and stored in an instance of QCchecker object. For more details on the specific implementation of the QC checks, refer to the section qc_checks_utils_doc . Cohort Merging Once all cohorts have been formatted, they are merged using the merge_cohorts function, defined in the cohort_utils module. merge_cohorts takes a list of already instantiated cohorts (i.e., they contain the homogeneous_data attribute) and performs the following actions: Concatenates (vertically) the homogeneous_data attributes (DataFrames) from each cohort, ensuring that the resulting DataFrame combines all the data from the individual cohorts. Instantiates a ConfigDataManager object, which allows to merge properly all the VarData objects from each cohort. Instantiates a new Cohort object using the previously created merged elements. This cohort is named liveraim from now on. Once the new cohort object is created, the set_id method is called. This method generates a new column in the homogeneous_data dataframe attribute, populated with a unified ID for each patient. The structure of this new ID follows the format LAxxxxx , where xxxxx is a sequential number starting from 1 and increasing up to the total number of rows in the dataframe (i.e., the position of the last patient in the dataframe). The IDs are assigned in ascending order throughout the dataframe. Warning : As a result, the correspondence between the original ID (referred to here as cohort_id ) and the unified ID (referred to here as liveraim_id ) may vary depending on the position of each patient\u2019s data within the homogeneous_data dataframe. This implies that the mapping between cohort_id and liveraim_id can change between executions. For more details on the functionality of this process, refer to the section merge_cohorts function . Biomarker data management To check the structure of the processed biomarkers data panel check the subsection Biomarkers table . There are two different sources of biomarkers data: Partners : third party partners (name Nordic, Roche and Hospital Cl\u00ednic) are responsible for the analysis and transfer of the biomarkers data, that will be then processed and integrated into the data warehouse. This process is described in the following sections Data Reading and Data Processing . Data Warehouse : Some biomarkes are computed using the variables from the data warehouse, formatted and merged with the biomarkers data obtained from the partners. This process is briefly described in Calculation of computed Biomarkers . Data Reading The processing of biomarker data is performed immediately after the processing of cohort data, as the cohort IDs are used to validate the IDs in the biomarker data. Biomarker data is received from different providers, namely Nordic, Roche, and Hospital Cl\u00ednic, in the current version. Since each provider may send the data split into different batches, and similarly to the cohort data case, the reading process for biomarker data is handled by the BMKDataReader class. For each provider, an instance of BMKDataReader is created, resulting in a dictionary (stored in the bmk_data_dict attribute) where the keys are the version dates (batches) and the values are the dataframes containing the data. For more details on the organization of the biomarker data directory and the specifics of the reading process, please check the section class DataReader . Data Processing Once read, an instance of BMKDataProcessor is created for each provider. This instance is responsible for harmonizing and processing the biomarker data so it can be merged with data from other biomarkers. To instantiate this object, two main parameters are required, in addition to the biomarkers_data : the name of the provider (which allows calling the specific configuration class to process its data) and the set of common Liveraim IDs from the cohort database. The following section describes the general transformations applied to the data from each provider: For each version/batch received from the provider, the following transformations are applied: The columns of the DataFrame are renamed to follow a common naming convention. Provider-specific transformations are applied via the BMKHarmonizer class. This will be covered in more detail below. The result of this transformation is a DataFrame with a common format and structure, both among batches from the same provider and between different providers. The original IDs are stored to generate a mapping between these IDs and the harmonized ones. This is mainly for debugging and traceability purposes \u2014 it allows tracing back the original IDs in the biomarker data that do not match the IDs in the cohort database. ID harmonization : The IDs are transformed to fix inconsistencies between cohort data and biomarker data. The transformations applied are described in the section ID transformations . Duplicate removal : If any duplicate records for ID and biomarker are found within a batch, they are dropped and removed from the DataFrame, retaining none of them. Blinding of biomarkers : The biomarker names (column variable ) are mapped to anonymize them. In the current version, they are mapped to uppercase letters. A quality control process is performed on the transformed data and stored as a DataFrame in a dictionary, whose key will be the version/batch name. These checks are detailed below. The DataFrames from each version are then concatenated (vertically) to obtain a single DataFrame (which may contain duplicates). The dictionary mapping the original IDs to the transformed IDs is created and stored in the ids_transformation_dict attribute. The appropriate data type is applied to each column. Consistency checks : Two main checks are performed at this point: ID matching : The IDs in the biomarker data are matched with those provided from the cohort data. If an ID from the biomarker data does not match any of the cohort IDs, it is dropped from the DataFrame and ignored. Duplicate removal : If any duplicates are found for the same biomarker and ID, the duplicates are dropped (currently keeping only the entry with the latest validation_date ). This is due to the fact that there may be patient overlap between batches from the same provider. A new column ( liveraim_id by default) is generated by mapping the transformed IDs to the Liveraim IDs from the cohort database. At this point, some identifiers might not match any of the provided Liveraim IDs (and will be subsequently dropped from the final data). Missing values drop : Rows with missing numeric values for a given biomarker are excluded from the DataFrame. Scaling : The values for each biomarker (currently in the value column) are scaled into a specified range ([0, 1] by default). The current version uses a Min-Max scaling method: \\[ X'_i = \\frac{X_i - X_{min}}{X_{max} - X_{min}} \\] Normalization : The z-score of the numeric value (grouped by biomarker) is computed and added as a column. To do so, the following formula is used: \\[ X'_i = \\frac{X_i - \\mu}{\\sigma} \\] Where: \\(\\mu\\) is the mean of the values for the given biomarker. \\(\\sigma\\) is the standard deviation of the values for the given biomarker. Quintiles : quintiles are computed (grouped by biomarker) and added as a column. Labelled from 1 to 5. As mentioned above, this process is performed separately for the data from each provider. Finally, all the DataFrames (with compatible structures) are merged into a single one, ready to be exported. Provider-specific transformations Hospital Cl\u00ednic The raw data from this provider may include the symbols < or > in the numeric value field of some biomarkers. These symbols indicate whether the value is below or above the quantification limit. Values without any symbol are considered to fall within the detectable range. Accordingly, the main specific transformations applied are: Comments column : A new column called comments is created based on the biomarker value column. If any of the above symbols are present, the corresponding comment is added to this column. If no symbol is found, the default value is an empty string. Symbols processing : The symbol (if any) is extracted from the biomarker value and stored in a new column named limit_detect , which contains only the symbol. If no symbol is present, the default value is pd.NA . Additionally, a new column called numeric_value is created, containing only the numeric part of the biomarker value (as float type). Nordic Biomarker value formatting : In the biomarker value column, values with the string \"ND\" (indicating a missing value) are replaced with an empty string. Then, the column is converted to float , assigning np.nan to empty strings. Addition of limit_detect column : Based on the comments column in the raw data provided by Nordic, the new column called limit_detect is created, following the convention described in Hospital Cl\u00ednic transformations . Comments mapping : Comments in the comments column are mapped to numeric codes using dictionaries defined in the configuration module. Value clipping : The raw data from Nordic includes the value of the analysis ( numeric_value ), even when it falls outside the detection range. To ensure consistency and preserve the structure of the panel, values outside the detection limits (those bounds have been provided by Nordic) are replaced with the appropriate limit value: Values below the detection limit are replaced with the lower detection limit. Values above the detection limit are replaced with the upper detection limit. Roche No data from Roche has been received yet - Not yet implemented Calculation of computed Biomarkers As some biomarkers are analyzed and provided by external partners, certain biomarkers need to be computed using variables from the current data warehouse. The following section briefly describes the process of computing and formatting these biomarkers. The class responsible for performing these computations is BMKCalculator , which is instantiated with the already processed data warehouse (in particular, with a Cohort object). For more details on the implementation of this class, refer to the section BMKCalculator class . For each biomarker that needs to be computed, a specific Python function has been implemented. This function returns a pd.Series object with the computed values. The following steps are performed to obtain a compatible DataFrame that can be merged with the one generated by BMKDataProcessor : A biomarker-specific function (already defined) is applied to compute the values using the harmonized data warehouse. The resulting values are stored in the numeric_value column. The variable column, containing the name of the biomarker (in the \"blinded\" form), is added. The required columns ( liveraim_id , inclusion_date , numeric_value , and variable ) are selected and assembled into a DataFrame. Note : Date used as validation_date variable for computed biomarkers will be, at this version, the inclusion_date variable. The columns scaled_value , quintiles , and z_score are added as described in the Biomarker Data Processing section. The columns comments and limit_detect are added to match the common structure. Warning : In the current version, no detection limits are available for the computed biomarkers. Accordingly, all records will contain default values in the limit_detect and comments columns ( pd.NA and \"\" , respectively). Furthermore, no clipping is applied to the biomarker values. Each biomarker data results in an individual DataFrame. These DataFrames are then merged into a single, consistent DataFrame that can be seamlessly combined with those produced by BMKDataProcessor . Exportation to Files If the variable EXPORT_FILES defined in the main_config module is set to True , the data for each of the final panels will be exported as files. In the current version, the data is exported to .csv and .feather formats. Exportation to MySQL Database After exporting the database as files, the MySQL database is created. The module responsible for handling this export is sql_exporting_utils . Specifically, the SQLExporter class, defined in this module, centralizes the connection to the database, the creation of tables, and the export of data. The SQLExporter class is initialized using the panel_metadata DataFrame dictionary (this object has not been modified at any point). This dictionary specifies the structure of each of the final panels in the database. For more information on the structure of the panel_metadata file (and the corresponding panel_metadata object), see the section panel_data file . SQLExporter performs the following actions sequentially: Creates an engine object , which establishes the connection to the database based on the configuration specified in the connection_config module. For more information on the connection parameters, see the section connection_config module . Creates the database structure : It defines the tables, the format of each table (which variables each contains, whether it's in long or wide format, etc.), and the relationships between them. To do this, it uses the configuration data present in the panel_metadata object. Establishes the connection to the database and generates the previously defined tables in the MySQL database. Iterates over each of the DataFrames in liveraim.final_data and inserts the data into the corresponding table. Note : The engine object does not immediately create the connection, but dynamically manages connections as needed. Therefore, until the tables are explicitly created in the database (and then populated), it does not actually connect to the database. Similarly, step 2 is internal to the SQLExporter class, meaning that when the table and relationship structure is created, it is stored in an internal object within the class. It is in step 3 that this structure is executed when an explicit connection to the database is established.","title":"Overview"},{"location":"dataflow/#overview","text":"","title":"Overview"},{"location":"dataflow/#dataflow","text":"This section describes the data flow and ETL process, from the data reading to data export in various formats. The data processing follows these steps, which are explained in detail in this document: Data Reading : Reading of all data required for the creation of the data warehouse. Specifically, the files described in initial_data_configuration are read. Data Preprocessing : An initial transformation of the data is performed, where some variables are added and combined. This process allows each cohort to be treated individually and specific changes to be applied to that cohort. Cohort Instantiation and Processing : For each cohort, a Cohort object is created. This centralizes the formatting and homogenization of the cohorts to produce a curated database with a consistent format across all cohorts. Cohort Quality Control : Quality control is performed for each cohort to validate the transformations carried out in the previous steps. Cohort Merging : Once the different cohorts with homogenized data have been created, they are merged into a new Cohort object (named liveraim ). Two important actions are performed in this cohort: The variable liveraim_id is added, a common format identifier. The different panels (in DataFrame format) that will later be exported are created. Biomarker data management : Exportation to Files : The created panels are exported as .feather and .csv files for further analysis. Exportation to MySQL Database : A connection is made to MySQL, where the various tables and relationships of the schema are created, and the data is exported to SQL format. The following image describes schematically the ETL process:","title":"Dataflow"},{"location":"dataflow/#data-reading","text":"Data reading is handled through the DataReader class. This class is responsible for reading the following files for each cohort: var_data (.xlsx) : A .xlsx file that is loaded into the code as a DataFrame . level_data (.xlsx) : A .xlsx file that is loaded into the code as a DataFrame . comb_var_data (.json) : A .json file that is loaded into the code as a dictionary. databases (variable) : For each cohort, there will be one or more versions of the database. DataReader reads each version in its respective format ( .dta , .sav , ...) and loads them into the code as pandas DataFrames . These dataframes are stored in a dictionary where the key is the version name (i.e., the date when the DB version was received) and the value is the DataFrame containing the database. Additionally, it reads the data related to the final structure of the panels. This is stored in the panel_metadata (.xlsx) file. Since this is a .xlsx file with multiple sheets, it is loaded into the code as a dictionary where the key is the sheet name (i.e., the name of the panel) and the value is a DataFrame with the content of the sheet. These data are stored in the DataReader.all_data attribute, a dictionary with the following structure: all_data: <cohort_name_1>: # Name of the cohort data: <database_versions_dictionary> # Dictionary containing each database version var_data: <var_data_file> # Excel file containing var_data level_data: <level_data_file> # File containing level_data comb_var_data: <comb_var_data_file> # File containing comb_var_data <cohort_name_2>: # Name of the next cohort data: <database_versions_dictionary> # Dictionary containing each database version var_data: <var_data_file> # Excel file containing var_data level_data: <level_data_file> # File containing level_data comb_var_data: <comb_var_data_file> # File containing comb_var_data ... panel_data: <panel_data_file> # Dictionary with panel_data information This process is carried out for the data of each WP. In particular, each WP will contain not only the various data related to the cohort databases and configuration files but will also have its own panel configuration document, which may differ from those of other WPs. Therefore, when using the DataReader class, one must specify both the list of cohort names to be read and the WP they belong to (currently, only WP1 and WP2 can be specified). Note 1 : Currently, only data from the same WP1 can be read at once due to the structure of the code. In future versions, reading all cohorts at once will be possible (probably if no WP is provided). Note 2 : The data warehouse will be created using all versions of the cohort databases, up to the most recent one. This allows for more detailed tracking of patients who have entered and exited in each version. How the different versions are managed is explained in the next section, Data Preprocessing . These files described above are essential for the correct processing of the data. To see the structure of these files, refer to the section initial_data_configuration . The variables that store the filenames to be read, as well as the directories where these files are located, are defined in the module main_config . Refer to data_reading_utils for more details on the internal workings of DataReader and the data reading process.","title":"Data Reading"},{"location":"dataflow/#data-preprocessing","text":"Once the initial data is loaded, an instance of the DataPreprocessor class is created for each cohort. This instance takes as parameters all the elements from all_data[cohort_name] , as well as the name of the respective cohort. This data is stored in the atributes cohort_databases , vara_data , level_data , var_comb_data and cohort_name respectively. DataPreprocessor performs the following actions: Creation of VarData object Merging database versions ID transformations Addition of new variables Combination of variables IDs dropping Cohort-specific transformations The aim of this class is to prepare the data from each cohort for the formatting and homogenization process, which requires a specific structure. This class is designed to handle each cohort individually and allows for specific adjustments to be made for each cohort, adapting to the particularities of each one.","title":"Data Preprocessing"},{"location":"dataflow/#creation-of-vardata-object","text":"The VarData object is responsible for centralizing all the configuration data and metadata of the core variables (both the variables from the original cohort and those calculated during execution). It allows easy access to every variable property and facilitates the merging of different cohorts into one. For further information on the VarData object, its configuration, and functionality, please check the section VarData . This object is instantiated for each cohort using the data contained in the var_data and level_data dataframes, as well as the var_comb_dict dictionary. Once created, it will be used to perform the proper transformation of the raw data (i.e., the previously mentioned objects var_data , level_data , and var_comb_data will no longer be used). This object will be stored in the attribute DataPreprocessor.varData of the class to be used during data harmonization, quality control, and exportation.","title":"Creation of VarData object"},{"location":"dataflow/#merging-database-versions","text":"The dictionary containing the cohort database versions is stored in the attribute DataPreprocessor.cohort_databases . From the dataframes of each version, a single dataframe will be generated that contains, for each patient present in at least one of the versions, the latest available data (i.e., the data from the most recent version in which they appear). Additionally, the variable status is added. This categorical variable can take 3 values: finished : Patients who appear in the latest version of the cohort database, and the cohort is already closed. ongoing : Patients who appear in the latest version of the cohort database, and the cohort is still ongoing. withdrawn : Patients who do not appear in the latest version of the cohort database. The variable date_version (str) is also added, indicating the version from which each patient's data is extracted. Note : During this process, it is required that the variables in the databases (and their names) do not change between versions. Checks are performed to ensure compatibility between versions, and if they are not compatible, only the DataFrame form the latest version will be used, and the previous ones will be ignored. To obtain this single dataframe , the following steps are followed: The date_version column is assigned to each dataframe from each version. This column will contain the version to which that data corresponds (i.e., all columns in the same dataframe will have the same value). All versions of the database are concatenated (vertically). This requires the structure of the dataframes to be compatible. The rows are sorted according to the date_version variable. Rows with duplicate values in the id_var column (the patient identifier) are removed, keeping the latest date_version . The status variable is added by verifying if the date_version of each patient corresponds to the latest version. Since status will be one of the core variables in the data warehouse, the varData attribute is updated to include this variable. this new dataframe is stored in data attribute.","title":"Merging database versions"},{"location":"dataflow/#id-transformations","text":"The format of patient identification variables can vary significantly between cohorts (even at the intra-cohort level), and the identifiers from laboratories may present (and do present) certain inconsistencies and errors compared to the cohort identifiers. To address this issue, the ID variables of all cohorts are transformed, ensuring that there is a correspondence between the IDs from the cohorts and those from the laboratory data. The following transformations are performed: Addition of 0's : A significant number of IDs contain a hyphen in the middle of the identifier ( e.g. , 1055-12 ). This leads to errors if, for example, the same patient is registered as 1055-0012 in another database. To prevent this, zeros are added after the hyphen to ensure that all identifiers have the same length (after the hyphen). In the current version, this length has been set to 5. Thus, after the transformation, both identifiers would be converted to 1055-00012 . Deletion of blank spaces : All blank spaces in the IDs are removed. Deletion of typos : Some symbols, such as \u00ba, $ , etc., are considered typos and are removed from the ID if present. Uppercasing : All letters in the identifiers are converted to uppercase (if they are not already). Thus, the identifiers such as glu1234 and GLU1234 would both be transformed into GLU1234 .","title":"ID transformations"},{"location":"dataflow/#ids-dropping","text":"Some patients must be removed from the database, often because they have duplicate entries for certain values. This may occur, for example, due to errors when entering IDs into the databases. The file ids_to_drop.txt , located in the data directory (see Structure of the Project Directory ), contains the list of IDs that must be removed from the final DW. During the preprocessing of each cohort, the system checks whether the identifiers listed in this file are present in the database, and if so, their entries are deleted.","title":"IDs dropping"},{"location":"dataflow/#addition-of-new-variables","text":"Once a single dataframe with the database is created, the following variables are added to the data dataframe attribute: birth_date : Obtained by combining the patient's age with the date of entry into the cohort. exit_date : Date of exit from the cohort if the patient is not ongoing, or the current date at the time of code execution otherwise. cohort : The name of the cohort to which the data belongs. work_package : The work package which the cohort comes from. Note : Just as with the status variable, since these new variables will be core variables, the varData attribute will be updated accordingly (adding the metadata of the respective variable).","title":"Addition of new variables"},{"location":"dataflow/#combination-of-variables","text":"If the var_comb_data parameter is passed during the initialization of DataPreprocessor , it will be used to combine the variables specified in that parameter. The primary goal here is to reduce the number of missing values by using variables that refer to the same magnitude but have different units. To achieve this, DataPreprocessor utilizes the VarCombiner class, which is instantiated with a dataframe (in this case, the data attribute) and a dictionary that defines the variable combinations (in this case the var_comb_dict attribute). It is recommended to consult the section comb_var_data file for details on the internal structure of the dictionary. Additionally, refer to the section Class VarCombiner for more information on how the class works. For each variable to be combined, VarCombiner sequentially checks whether a variable with a certain magnitude is missing. If it is not missing, the value of that variable is taken as the final value (applying the appropriate conversion factor). If the value is missing, it moves on to the next variable referring to the same magnitude (but with different units) and performs the same check. If all variables in the iteration are missing, NaN is assigned as the final value. To illustrate this, suppose the var_comb_data dictionary is as follows: { \"glc\": { \"glc\": 1, \"glc_mg_dl\": 0.055 }, \"crea_mg_dl\": { \"crea_mg_dl\": 1, \"creat\": 0.017 } } This indicates that two variables should be combined: glc (using glc and glc_mg_dl ) and crea_mg_dl (using creat and crea_mg_dl ). VarCombiner will first iterate over the pair \"glc\": {\"glc\": 1, \"glc_mg_dl\": 0.055} . The glc variable (the first key) will be calculated as described above: if glc (in this case, the key in the pair \"glc\": 1 ) is not missing, this value will be used for the glc variable. Otherwise, it will check if glc_mg_dl is missing. If it is not missing, the value of glc_mg_dl multiplied by 0.055 will be used as the final value (where 0.055 is the conversion factor to convert glc_mg_dl units to glc units). The processing for crea_mg_dl is entirely analogous. The final result is that in the data attribute, the glc and crea_mg_dl variables will be populated with values from other compatible variables.","title":"Combination of variables"},{"location":"dataflow/#cohort-specific-transformations","text":"The following section describes the specific modifications applied to each cohort:","title":"Cohort-specific transformations"},{"location":"dataflow/#liverscreen","text":"Temporary : Currently, one of the database versions ( 20241223 ) includes the Ever smoker level in the smoke variable, which was not present in previous versions. This level is encoded with the value 1.5 . These values are mapped to the string \"Ever smoker\" , a notation that is more consistent with the rest of the versions.","title":"Liverscreen"},{"location":"dataflow/#decide","text":"Some patients in the Decide cohort are also present in the Liverscreen cohort (those whose ID starts with 1056 ). These patients are removed from the cohort to avoid duplicates.","title":"Decide"},{"location":"dataflow/#alcofib","text":"Some heterogeneity in variables with percentage units can be found at the cohort level. This means that, for the same variable (with percentage units), some values are given as actual % , while others appear to be given per unit (i.e., as a proportion). To address this issue, the harmonize_percent function is called, which transforms all the values into percentages. To achieve this, the function multiplies by 100 all the values below a certain threshold (currently set to 1), assuming that any value lower than 1 is given as a proportion rather than a percentage. Warning : This \"two units\" situation is inferred from the nature of the data and the variables. This means that there has been no strict confirmation of this issue yet. However, as it is the best explanation for the inconsistency, the transformation is applied. Ideally, this should be verified with the individuals responsible for the cohorts where these transformations are performed.","title":"Alcofib"},{"location":"dataflow/#glucofib","text":"Similarly to the Alcofib cohort, some heterogeneity in variables with percentage units can be found at the cohort level. This means that, for the same variable (with percentage units), some values are given as actual % , while others appear to be given per unit (i.e., as a proportion). To address this issue, the harmonize_percent function is called, which transforms all the values into percentages. To achieve this, the function multiplies by 100 all the values below a certain threshold (currently set to 1), assuming that any value lower than 1 is given as a proportion rather than a percentage. Warning : This \"two units\" situation is inferred from the nature of the data and the variables. This means that there has been no strict confirmation of this issue yet. However, as it is the best explanation for the inconsistency, the transformation is applied. Ideally, this should be verified with the individuals responsible for the cohorts where these transformations are performed.","title":"Glucofib"},{"location":"dataflow/#marina1","text":"No transformations specific to this cohort are currently performed.","title":"Marina1"},{"location":"dataflow/#metronord","text":"No transformations specific to this cohort are currently performed.","title":"Metronord"},{"location":"dataflow/#gala-ald","text":"No transformations specific to this cohort are currently performed.","title":"Gala-ald"},{"location":"dataflow/#cohort-instantiation-and-processing","text":"Once the cohort data has been preprocessed, it is formatted so that all cohorts have a homogeneous structure and can be subsequently combined. The central element of this processing is the Cohort class, and the VarData object mentioned earlier is essential for its correct operation. For each cohort, a Cohort object is instantiated, which uses the (already preprocessed) attributes of DataPreprocessor : data (as the raw_data parameter), and VarData . It is important to note that the VarData object contains a dictionary of Var objects (for further information, check the section Var ). Each of these Var objects contains data from a particular variable that will be used during the harmonization process. It contains the following information (it is not an exhaustive list): Initial variable name in the cohort. Final variable name in the common database. Final variable datatype. Conversion factor (if needed). mapping dictionary for categorical variables ... Additionally, the cohort name (parameter cohort_name ), the ID variable name (parameter id_variable ), the inclusion date variable name (parameter date_variable ), and the cohort status (parameter status ) are introduced as parameters. Optionally, a dictionary with the structure of comb_var_data can also be provided. For more detailed information about these objects, refer to the section initial_data_configuration . The instantiation of the cohort concludes with a call to the Cohort.homogenize_data method. This method creates a copy of the raw_data attribute (which was initialized with the preprocessed cohort database, i.e., the raw_data parameter), formats it, and stores it in the homogeneous_data attribute. In general terms, the data formatting follows these steps: Selection of the subset of raw_data with the core variables. Translation of the variable names (columns of the DataFrame) to the final names defined in VarData . Formatting of the variables. It iterates over the VarData for each of the selected variables and: Verifies the datatype assigned in Var and applies the appropriate datatype to that column. If the variable is numeric, applies the appropriate conversion factor. If the variable is categorical, maps the original levels to the final levels. This mapping is defined in the attribute Var.var_map . The result for each cohort should be the same: a DataFrame with the same structure and format stored in the homogeneous_data attribute. Currently, the structure of the database allows cohorts with different variables each. For more details on the specific implementation of the Cohort class and its methods, refer to the section dedicated to the cohort_utils .","title":"Cohort Instantiation and Processing"},{"location":"dataflow/#cohort-quality-control","text":"After the creation and harmonization of each cohort object, a quality control check is performed and stored in an instance of QCchecker object. For more details on the specific implementation of the QC checks, refer to the section qc_checks_utils_doc .","title":"Cohort Quality Control"},{"location":"dataflow/#cohort-merging","text":"Once all cohorts have been formatted, they are merged using the merge_cohorts function, defined in the cohort_utils module. merge_cohorts takes a list of already instantiated cohorts (i.e., they contain the homogeneous_data attribute) and performs the following actions: Concatenates (vertically) the homogeneous_data attributes (DataFrames) from each cohort, ensuring that the resulting DataFrame combines all the data from the individual cohorts. Instantiates a ConfigDataManager object, which allows to merge properly all the VarData objects from each cohort. Instantiates a new Cohort object using the previously created merged elements. This cohort is named liveraim from now on. Once the new cohort object is created, the set_id method is called. This method generates a new column in the homogeneous_data dataframe attribute, populated with a unified ID for each patient. The structure of this new ID follows the format LAxxxxx , where xxxxx is a sequential number starting from 1 and increasing up to the total number of rows in the dataframe (i.e., the position of the last patient in the dataframe). The IDs are assigned in ascending order throughout the dataframe. Warning : As a result, the correspondence between the original ID (referred to here as cohort_id ) and the unified ID (referred to here as liveraim_id ) may vary depending on the position of each patient\u2019s data within the homogeneous_data dataframe. This implies that the mapping between cohort_id and liveraim_id can change between executions. For more details on the functionality of this process, refer to the section merge_cohorts function .","title":"Cohort Merging"},{"location":"dataflow/#biomarker-data-management","text":"To check the structure of the processed biomarkers data panel check the subsection Biomarkers table . There are two different sources of biomarkers data: Partners : third party partners (name Nordic, Roche and Hospital Cl\u00ednic) are responsible for the analysis and transfer of the biomarkers data, that will be then processed and integrated into the data warehouse. This process is described in the following sections Data Reading and Data Processing . Data Warehouse : Some biomarkes are computed using the variables from the data warehouse, formatted and merged with the biomarkers data obtained from the partners. This process is briefly described in Calculation of computed Biomarkers .","title":"Biomarker data management"},{"location":"dataflow/#data-reading_1","text":"The processing of biomarker data is performed immediately after the processing of cohort data, as the cohort IDs are used to validate the IDs in the biomarker data. Biomarker data is received from different providers, namely Nordic, Roche, and Hospital Cl\u00ednic, in the current version. Since each provider may send the data split into different batches, and similarly to the cohort data case, the reading process for biomarker data is handled by the BMKDataReader class. For each provider, an instance of BMKDataReader is created, resulting in a dictionary (stored in the bmk_data_dict attribute) where the keys are the version dates (batches) and the values are the dataframes containing the data. For more details on the organization of the biomarker data directory and the specifics of the reading process, please check the section class DataReader .","title":"Data Reading"},{"location":"dataflow/#data-processing","text":"Once read, an instance of BMKDataProcessor is created for each provider. This instance is responsible for harmonizing and processing the biomarker data so it can be merged with data from other biomarkers. To instantiate this object, two main parameters are required, in addition to the biomarkers_data : the name of the provider (which allows calling the specific configuration class to process its data) and the set of common Liveraim IDs from the cohort database. The following section describes the general transformations applied to the data from each provider: For each version/batch received from the provider, the following transformations are applied: The columns of the DataFrame are renamed to follow a common naming convention. Provider-specific transformations are applied via the BMKHarmonizer class. This will be covered in more detail below. The result of this transformation is a DataFrame with a common format and structure, both among batches from the same provider and between different providers. The original IDs are stored to generate a mapping between these IDs and the harmonized ones. This is mainly for debugging and traceability purposes \u2014 it allows tracing back the original IDs in the biomarker data that do not match the IDs in the cohort database. ID harmonization : The IDs are transformed to fix inconsistencies between cohort data and biomarker data. The transformations applied are described in the section ID transformations . Duplicate removal : If any duplicate records for ID and biomarker are found within a batch, they are dropped and removed from the DataFrame, retaining none of them. Blinding of biomarkers : The biomarker names (column variable ) are mapped to anonymize them. In the current version, they are mapped to uppercase letters. A quality control process is performed on the transformed data and stored as a DataFrame in a dictionary, whose key will be the version/batch name. These checks are detailed below. The DataFrames from each version are then concatenated (vertically) to obtain a single DataFrame (which may contain duplicates). The dictionary mapping the original IDs to the transformed IDs is created and stored in the ids_transformation_dict attribute. The appropriate data type is applied to each column. Consistency checks : Two main checks are performed at this point: ID matching : The IDs in the biomarker data are matched with those provided from the cohort data. If an ID from the biomarker data does not match any of the cohort IDs, it is dropped from the DataFrame and ignored. Duplicate removal : If any duplicates are found for the same biomarker and ID, the duplicates are dropped (currently keeping only the entry with the latest validation_date ). This is due to the fact that there may be patient overlap between batches from the same provider. A new column ( liveraim_id by default) is generated by mapping the transformed IDs to the Liveraim IDs from the cohort database. At this point, some identifiers might not match any of the provided Liveraim IDs (and will be subsequently dropped from the final data). Missing values drop : Rows with missing numeric values for a given biomarker are excluded from the DataFrame. Scaling : The values for each biomarker (currently in the value column) are scaled into a specified range ([0, 1] by default). The current version uses a Min-Max scaling method: \\[ X'_i = \\frac{X_i - X_{min}}{X_{max} - X_{min}} \\] Normalization : The z-score of the numeric value (grouped by biomarker) is computed and added as a column. To do so, the following formula is used: \\[ X'_i = \\frac{X_i - \\mu}{\\sigma} \\] Where: \\(\\mu\\) is the mean of the values for the given biomarker. \\(\\sigma\\) is the standard deviation of the values for the given biomarker. Quintiles : quintiles are computed (grouped by biomarker) and added as a column. Labelled from 1 to 5. As mentioned above, this process is performed separately for the data from each provider. Finally, all the DataFrames (with compatible structures) are merged into a single one, ready to be exported.","title":"Data Processing"},{"location":"dataflow/#provider-specific-transformations","text":"","title":"Provider-specific transformations"},{"location":"dataflow/#hospital-clinic","text":"The raw data from this provider may include the symbols < or > in the numeric value field of some biomarkers. These symbols indicate whether the value is below or above the quantification limit. Values without any symbol are considered to fall within the detectable range. Accordingly, the main specific transformations applied are: Comments column : A new column called comments is created based on the biomarker value column. If any of the above symbols are present, the corresponding comment is added to this column. If no symbol is found, the default value is an empty string. Symbols processing : The symbol (if any) is extracted from the biomarker value and stored in a new column named limit_detect , which contains only the symbol. If no symbol is present, the default value is pd.NA . Additionally, a new column called numeric_value is created, containing only the numeric part of the biomarker value (as float type).","title":"Hospital Cl\u00ednic"},{"location":"dataflow/#nordic","text":"Biomarker value formatting : In the biomarker value column, values with the string \"ND\" (indicating a missing value) are replaced with an empty string. Then, the column is converted to float , assigning np.nan to empty strings. Addition of limit_detect column : Based on the comments column in the raw data provided by Nordic, the new column called limit_detect is created, following the convention described in Hospital Cl\u00ednic transformations . Comments mapping : Comments in the comments column are mapped to numeric codes using dictionaries defined in the configuration module. Value clipping : The raw data from Nordic includes the value of the analysis ( numeric_value ), even when it falls outside the detection range. To ensure consistency and preserve the structure of the panel, values outside the detection limits (those bounds have been provided by Nordic) are replaced with the appropriate limit value: Values below the detection limit are replaced with the lower detection limit. Values above the detection limit are replaced with the upper detection limit.","title":"Nordic"},{"location":"dataflow/#roche","text":"No data from Roche has been received yet - Not yet implemented","title":"Roche"},{"location":"dataflow/#calculation-of-computed-biomarkers","text":"As some biomarkers are analyzed and provided by external partners, certain biomarkers need to be computed using variables from the current data warehouse. The following section briefly describes the process of computing and formatting these biomarkers. The class responsible for performing these computations is BMKCalculator , which is instantiated with the already processed data warehouse (in particular, with a Cohort object). For more details on the implementation of this class, refer to the section BMKCalculator class . For each biomarker that needs to be computed, a specific Python function has been implemented. This function returns a pd.Series object with the computed values. The following steps are performed to obtain a compatible DataFrame that can be merged with the one generated by BMKDataProcessor : A biomarker-specific function (already defined) is applied to compute the values using the harmonized data warehouse. The resulting values are stored in the numeric_value column. The variable column, containing the name of the biomarker (in the \"blinded\" form), is added. The required columns ( liveraim_id , inclusion_date , numeric_value , and variable ) are selected and assembled into a DataFrame. Note : Date used as validation_date variable for computed biomarkers will be, at this version, the inclusion_date variable. The columns scaled_value , quintiles , and z_score are added as described in the Biomarker Data Processing section. The columns comments and limit_detect are added to match the common structure. Warning : In the current version, no detection limits are available for the computed biomarkers. Accordingly, all records will contain default values in the limit_detect and comments columns ( pd.NA and \"\" , respectively). Furthermore, no clipping is applied to the biomarker values. Each biomarker data results in an individual DataFrame. These DataFrames are then merged into a single, consistent DataFrame that can be seamlessly combined with those produced by BMKDataProcessor .","title":"Calculation of computed Biomarkers"},{"location":"dataflow/#exportation-to-files","text":"If the variable EXPORT_FILES defined in the main_config module is set to True , the data for each of the final panels will be exported as files. In the current version, the data is exported to .csv and .feather formats.","title":"Exportation to Files"},{"location":"dataflow/#exportation-to-mysql-database","text":"After exporting the database as files, the MySQL database is created. The module responsible for handling this export is sql_exporting_utils . Specifically, the SQLExporter class, defined in this module, centralizes the connection to the database, the creation of tables, and the export of data. The SQLExporter class is initialized using the panel_metadata DataFrame dictionary (this object has not been modified at any point). This dictionary specifies the structure of each of the final panels in the database. For more information on the structure of the panel_metadata file (and the corresponding panel_metadata object), see the section panel_data file . SQLExporter performs the following actions sequentially: Creates an engine object , which establishes the connection to the database based on the configuration specified in the connection_config module. For more information on the connection parameters, see the section connection_config module . Creates the database structure : It defines the tables, the format of each table (which variables each contains, whether it's in long or wide format, etc.), and the relationships between them. To do this, it uses the configuration data present in the panel_metadata object. Establishes the connection to the database and generates the previously defined tables in the MySQL database. Iterates over each of the DataFrames in liveraim.final_data and inserts the data into the corresponding table. Note : The engine object does not immediately create the connection, but dynamically manages connections as needed. Therefore, until the tables are explicitly created in the database (and then populated), it does not actually connect to the database. Similarly, step 2 is internal to the SQLExporter class, meaning that when the table and relationship structure is created, it is stored in an internal object within the class. It is in step 3 that this structure is executed when an explicit connection to the database is established.","title":"Exportation to MySQL Database"},{"location":"liveraim_data_warehouse_specifications/","text":"Overview This section describes the structure, tables, and relationships within the data warehouse. The data warehouse is composed of several tables that can be divided into two groups: the database itself, which we will simply call database , and the configuration data tables, where information about the variables, the structure of the database, mapping dictionaries, etc., is stored. The database, in its first version (prior to receiving biomarker data), consists of the following tables: population : Contains basic information about each patient, such as gender, status, cohort membership, etc. blood_test_categorical : Contains categorical variables extracted from the blood test (at dat_0 ). blood_test_numerical : Contains numerical variables extracted from the blood test (at dat_0 ). demographics : Contains demographic information about the patient, such as gender, ethnicity, etc. fibroscan : Contains information related to the fibroscan test results for each patient. physical_exam : Contains information obtained from the physical examination, including variables such as weight, height, etc. medical_history : Contains information about each patient's medical history, including relevant comorbidities, etc. We will refer to these tables as panels or tables interchangeably. The population panel acts as the core of the database: the other panels are linked to this primary one (see General Relationships in the Schema ). The following image schematically depicts the structure and relationships of the data warehouse: The data warehouse contains two types of tables in terms of format: long format tables and wide format tables: Wide : These are the \"usual\" tables. Each variable is stored as a separate column in the table. Each row corresponds to the information of a single patient. In these tables, there cannot be duplicates in the identifier columns: all information for a patient is stored in a single row. Long : These tables contain at least one patient identifier column (in this case, liveraim_id and cohort_id ) along with the variable and value columns. The variable column contains the names of the variables, while the value column contains the value of the variable specified in the variable column. Tables in long format will have repeated entries in the patient identifier columns. In addition to the columns mentioned, these tables may include other columns that provide additional context for each entry: annotations for variable values ( lower_bound , upper_bound ), the date of each observation ( date_0 ), and the units of the variable ( final_units ). Transforming a wide table into a long table is a straightforward process (if the necessary data is available) and is referred to in this documentation as melting . Similarly, it is possible to transform a long format table into a wide format table with the same ease. An example of this format is shown below: date_0 liveraim_id cohort_id variable value final_units lower_bound upper_bound 2023-01-19 00:00:00 LA--- ----- alb NaN g/L 2023-01-19 00:00:00 LA--- ----- alk 41 U/L 2023-01-19 00:00:00 LA--- ----- alt 21 UI/L 2023-01-19 00:00:00 LA--- ----- ast 20 U/L 2023-01-19 00:00:00 LA--- ----- bili 0.6 mg/dL 2023-01-19 00:00:00 LA--- ----- cb NaN mg/dL 2023-01-19 00:00:00 LA--- ----- chol 236 mg/dL 2023-01-19 00:00:00 LA--- ----- crea 1.1 mg/dL 0 10 2023-01-19 00:00:00 LA--- ----- crp NaN mg/dL 2023-01-19 00:00:00 LA--- ----- ferritin 146 ng/mL ... For long tables, since there can (and should) be repetitions in the ID columns (one repetition for each variable in long format at least), composite primary keys are used. In general, this database uses the columns liveraim_id , cohort_id , and variable as composite primary keys: the database does not allow any repetitions of the combination of these three variables. Note : Using both liveraim_id and cohort_id within the composite keys is redundant, as there is a bijective correspondence between these two identifiers. However, it has been configured this way to maintain identifier consistency. It is important to clarify the difference between cohort_id and liveraim_id . cohort_id is the patient-specific identifier within each cohort. The encoding of these identifiers is heterogeneous, varies between cohorts, and originates from the original data. liveraim_id is a new variable generated during the execution of main.py , which assigns a unique identifier to each patient across all cohorts at the data warehouse level, following a common format. There is a bijective correspondence between these two variables. Note : The variables in the variable column (the \"melted\" variables) of each long table must be of the same data type. This is because SQL does not allow values with different data types in the same column. This adds robustness to the database at the cost of having to create additional tables. For example, in the case of blood test data, two tables are needed: blood_test_numerical and blood_test_categorical . Initial data and configuration data This section describes the initial data recieved from the partners and teh configuration files that the program needs to build the database. For each cohort, there are four types of files, which are described below: database : contains the raw data of the cohort var_data : contains configuration data about the core variables needed to create the data warehouse level_data : contains configuration data about the levels of categorical core variables needed to create the data warehouse comb_var_data : For the variables that need to be combined, it contains the necessary data to perform this process. Finally, an other file, panel_data is needed for the proper creation of the data warehouse. In this file the structure of the final panels is described. Database files The files we refer to as database files contain the raw database of the cohort. These are the files that the partners responsible for each cohort have sent us with the patient data. The content of these files is never modified, neither manually nor through the code: all data processing is done after reading these files, which remain intact after each execution. The only modification they receive is a renaming necessary for their correct reading. For more details, see the section Structure of data/cohort_name/databases/ directory . The format of these files varies depending on the cohort. For more information on reading the data, see the section File reading utils . When this files are read, they are loaded into the code as pandas.DataFrame objects. Note : For obvious data privacy reasons, these files are not uploaded to GitHub. var_data files This file, along with level_data, is essential for the creation of the data warehouse. It is an Excel file (.xlsx) where each row corresponds to one of the selected/core variables. It contains the following columns: original_name (str) : Name of the core variable in the original database. It varies between cohorts. liveraim_name (str) : Name of the core variable in the common format, i.e., in the liveraim format. panel (str) : Panel where the variable will be stored. data_type (str) : Final datatype of the variable. initial_units (str) : Units of the variable in the original cohort. description (str) : Short text describing the variable. final_units (str) : Final units of the variable, i.e., units in the liveraim data warehouse. lower_bound (float) : Lower bound of the variable. For checking purposes. upper_bound (float) : Upper bound of the variable. For checking purposes. conversion_factor (float) : Conversion factor from initial_units to final_units. Set to 1 if not needed. Before running the code, you must verify that these files are in the correct directories, can be read correctly, have the structure defined above, and that the information they contain is correct. Any variation in structure/content may result in an error in execution or the creation of an erroneous data warehouse. The correct functioning of the program is highly dependent on these files. These files are tracked in GitHub like the rest of the scripts, so there is no need to create them from scratch, and they should work fine if the repository is cloned. When this file is read, it is loaded into the code as a pandas.DataFrame object. Throughout all the documentation, we will refer to this DataFrame also as var_data . Note 1 : There must be certain coherence between the var_data files from each cohort: the number of variables in each file (i.e., the number of rows) should be the same. In addition, the set of variables in liveraim_name should be the same between cohorts, and the final_data column should be consistent. Although this is checked in the code, making sure that there are no errors is recomended. Note 2 : If needed, addition of other columns is possible and should not create incompatibilities (but the management of those new columns should be implemented). level_data files This file, along with var_data , is essential for the creation of the data warehouse. These files describe the different levels of the categorical core variables. In this case, each row does not correspond to a variable, but to a level of the original variable. The file contains the following columns: original_name (str) : Name of the core variable in the original database. It varies between cohorts. original_level_value (Variable) : Original value of the level in the cohort. original_data_type (str) : Original datatype of the variable. liveraim_level_value (Variable) : Value of the level in the final database (i.e., in the LIVERAIM data warehouse). level_desc (str) : Description/label of the level. liveraim_name (str) : Name of the core variable in the common format, i.e., in the liveraim format. Before running the code, you must verify that these files are in the correct directories, can be read correctly, have the structure defined above, and that the information they contain is correct. Any variation in structure/content may result in an error in execution or the creation of an erroneous data warehouse. The correct functioning of the program is highly dependent on these files. These files are tracked in GitHub like the rest of the scripts, so there is no need to create them from scratch, and they should work fine if the repository is cloned. When this file is read, it is loaded into the code as a pandas.DataFrame object. Throughout all the documentation, we will refer to this DataFrame also as level_data . Note : There must be certain coherence between the level_data files from each cohort: In this case, the number of rows in each file does not need to match, as in some cohorts two or more different levels might coincide in the same final level. However, the original_name , liveraim_name , and liveraim_level_value columns should be consistent between files. Although this is checked in the code, it is recommended to ensure there are no errors. comb_var_data file This is a json file that will be loaded into the code as a dictionary. It contains the necessary information to obtain a final variable by combining original variables present in the database. For more information about this process, see the section data_processing_utils , subsection class VarCombiner . This section describes the class responsible for combining the variables listed in comb_var_data according to the specified configuration. The primary goal of this dictionary is to combine variables that refer to the same magnitude but use different units. Combining these variables helps reduce the number of missing values. For example, in the LIVERSCREEN cohort, for the magnitude blood glucose (in visit 1), there are the variables glc (expressed in mmol/L) and glc_mg_dl (expressed in mg/dL). Both will be combined to fill missing at least one of the variables have a proper value. Note : Currently, only the functionality described above has been implemented. However, it can be extended (by modifying this dictionary and the VarCombiner class) to combine categorical variables, generate secondary or calculated variables, and more. The structure of the comb_var_data dictionary is as follows: Primary Key : The name of the final variable resulting from the combination. This variable must appear in the var_data file described in this documentation. It usually matches the name of one of the variables to be used in the combination. Primary Value : A dictionary with the following structure: Secondary Key : Names of the variables (in the database) that will be used for the combination. Secondary Value : Conversion factor to transform the variable from its original units to the final units (i.e., the units of the variable named by the primary key). The structure of the dictionary would be: comb_var_data: <final_variable_name_1>: # Primary Key: Name of the final variable <original_variable_name_1>: <conversion_factor_1> # Secondary Key-Value: Original variable and its conversion factor <original_variable_name_2>: <conversion_factor_2> # Secondary Key-Value: Original variable and its conversion factor <final_variable_name_2>: # Primary Key: Name of another final variable <original_variable_name_3>: <conversion_factor_3> # Secondary Key-Value: Original variable and its conversion factor <original_variable_name_4>: <conversion_factor_4> # Secondary Key-Value: Original variable and its conversion factor ... And an example: { \"glc\": { \"glc\": 1, \"glc_mg_dl\": 0.055 }, \"crea_mg_dl\": { \"creat\": 0.017, \"crea_mg_dl\": 1 } } In this case, it indicates that the glc variable will be created by combining the glc itself and glc_mg_dl variables. Specifically, the value of glc will be used if it exists, and if not, the value of glc_mg_dl will be used, multiplied by the corresponding conversion factor (in this case, 0.055). A similar procedure will be aplied to crea_mg_dl . These files are tracked in GitHub like the rest of the scripts, so there is no need to create them from scratch, and they should work fine if the repository is cloned. panel_data file This file is not specific to any cohort and is used as a guide to create and structure the different final panels. The file panel_metadata.xlsx is an Excel file that contains a sheet for each final panel. Each of these panels includes the following columns: var_name (str) : Name of the variable (in the common liveraim format). melt (int) : Integer (acting as a boolean, so it is 0 or 1) indicating: 0 : The variable is not melted: it will appear in the panel as a column. 1 : The variable is melted: the variable name and value will be restructured in a long format. A column variable will contain the name of the variable, and the column value will contain its value. These files are tracked in GitHub like the rest of the scripts, so there is no need to create them from scratch, and they should work fine if the repository is cloned. When this file is read, since it is an .xlsx file with multiple sheets, it is loaded into the code as a dictionary of pandas.DataFrames . Each sheet name becomes a key, and the corresponding table becomes its value (a DataFrame ). Throughout the documentation, we will refer to this dictionary as panel_metadata . It is important to maintain the structure of this file (and the previously described). Changes can be easily made to modify the final structure of the data warehouse by adding new sheets to the file (i.e., adding new panels), adding variables to a panel, or changing their structure (long or wide format). For more information about the structure of the final data, check the next section LIVERAIM DATA WAREHOUSE STRUCTURE . Liveraim Data Warehouse Structure The LIVERAIM DATA WAREHOUSE v0.1 is composed by the following tables: a. Database population blood_test_categorical blood_test_numerical demographics fibroscan physical_exam medical_history b. Configuration Data panel_metadata var_data level_data comb_var_data dictionaries a. Database Tables Description Table sources: For each cohort, there is at least one raw database containing patient data. Each database corresponds to a single file, and for some cohorts, different versions of the same database are available. If multiple versions are available, the data from the latest version for each patient will be used. During the data processing phase, the databases are merged into a single table, which is later split and restructured into the various tables of the final database. From each raw database, a subset of variables is selected. These variables must be common across all cohort databases (although the variable names may differ), ensuring that patient data from different cohorts can be merged. Thus, all variables that appear in the final database are either secondary variables created during the code execution, or they originate from one of the four cohorts (depending on the patient's source) used to construct the final database. For more information about the dataflow go to dataflow section. A summary of the data sources is presented below: Cohort Number of versions used File type Reading method liverscreen 7 stata file ( .dta ) pd.read_stata alcofib 2 sav file ( .sav ) pyreadstat.read_sav glucofib 1 csv ( .csv ) pd.read_csv decide 1 stata file ( .dta ) pd.read_stata galaald 1 stata file ( .dta ) pd.read_stata marina 2 sav file ( .sav ) pyreadstat.read_sav Note 1 : Some of the information below may be redundant, as certain variables appear in many or all panels. However, redundancies have been intentionally kept for the sake of clarity and readability. Note 2 : As three of the four used cohorts have been previously managed isnod RedCap, many RedCap names in the final database have been kept as standard (sometimes with few changes). Table: population Description: Defines the population of patients within different cohorts. Fields: liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. cohort_id (VARCHAR(255)): Cohort-specific identifier for each patient in a cohort. Unique within the cohort. gender (VARCHAR(8)): Gender of the patient. birth_date (DATETIME): Date of birth of the patient. status (VARCHAR(8)): Current status of the patient (ongoing,, withdrawn, finished). date_0 (DATETIME): Date of patient inclusion in the cohort. exit_date (DATETIME): Date of study exit. Format : wide Primary Key: ( liveraim_id , cohort_id ) Relations: Central table that relates to all other tables using liveraim_id and cohort_id . Actions and specification tables Liveraim name Liverscreen name Alcofib name Glucofib name Decide name SQL data type data type Rule/Notes liveraim_id - - - - VARCHAR(255) str Created dynamically after merging cohorts. cohort_id id Codigo_deidentificacion id id VARCHAR(255) str gender gender Sexe gender gender FLOAT category Applied a mapping (to homogenize level codification) defined in level_data . birth_date - - - - DATETIME datetime Variable calculated from patient's age and date_0 (inclusion date). date_0 date_0 Fecha_V1 date_0 date_0 DATETIME datetime status - - - - VARCHAR(8) category Calculated variable. If cohort recruitment is ongoing, it is set to ongoing . If cohort recruitment is finished, it is set to finished . If the patient has left the cohort (in cases where more than one version is available), it is set to withdrawn . exit_date - - - - DATETIME datetime Calculated variable. If the patient's status is ongoing, it is set to the current date (execution date). If the status is finished, it is set to the date of the last version. If the status is withdrawn, it is set to the date of the last version in which the patient appears. Table: physical_exam Description: Stores physical exam results. Fields: liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. cohort_id (VARCHAR(255)): Cohort-specific identifier for each patient in a cohort. Unique within the cohort. date_0 (DATETIME): Date of patient inclusion in the cohort. variable (VARCHAR(255)): Name of the variable to which the corresponding value in the value column refers. value (FLOAT): Value of variable in variable column. final_units (VARCHAR(255)): Final units of the value. lower_bound (FLOAT): Lower acceptable limit for the value. upper_bound (FLOAT): Upper acceptable limit for the value. Format : Long. Variables in varible column : weight , height , bmi , hip . Primary Key: ( liveraim_id , cohort_id , variable ) Relations: Relates to the population table via liveraim_id and cohort_id . Actions and specification tables Liveraim name Liverscreen name Alcofib name Glucofib name Decide name Data type Rule/Notes liveraim_id - - - - VARCHAR(255) Created dynamically after merging cohorts. cohort_id id Codigo_deidentificacion id id VARCHAR(255) date_0 date_0 Fecha_V1 date_0 date_0 DATETIME variable - - - - VARCHAR(255) Obtained from melting the variables weight , height , bmi , hip value - - - - FLOAT Obtained from melting the variables weight , height , bmi , hip final_units - - - - VARCHAR(255) lower_bound - - - - FLOAT upper_bound - - - - FLOAT Melted variables metadata Liveraim name Liverscreen name Alcofib name Glucofib name Decide name Data type Rule/Notes weight weight Peso_V1 weight weight float Data type modification Apply conversion factor height height Talla_V1 height height float Data type modification Apply conversion factor bmi bmi BMI_V1 bmi bmi float Data type modification Apply conversion factor hip hip diametro_cintura hip hip float Data type modification Apply conversion factor Table: demographics Description: Contains demographic information of patients. Fields: liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. cohort_id (VARCHAR(255)): Cohort-specific identifier for each patient in a cohort. Unique within the cohort. age (FLOAT): Age of the patient. ethnicity (VARCHAR(255)): Ethnicity of the patient. gender (VARCHAR(255)): Gender of the patient. date_0 (DATETIME): Date of patient inclusion in the cohort. Format : wide. Primary Key: ( liveraim_id , cohort_id ) Relations: Relates to the population table via liveraim_id and cohort_id . Actions and specification tables Liveraim name Liverscreen name Alcofib name Glucofib name Decide name SQL data type data type Rule/Notes liveraim_id - - - - VARCHAR(255) str Created dynamically after merging cohorts. cohort_id id Codigo_deidentificacion id id VARCHAR(255) str date_0 date_0 Fecha_V1 date_0 date_0 DATETIME datetime ethnicity ethnicity Etnia ethnicity ethnicity VARCHAR(255) category Applied a mapping (to homogenize level codification) defined in level_data gender gender Sexe gender gender VARCHAR(255) category Applied a mapping (to homogenize level codification) defined in level_data Table: fibroscan Description: Records fibroscan exam results. Fields: liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. cohort_id (VARCHAR(255)): Cohort-specific identifier for each patient in a cohort. Unique within the cohort. date_0 (DATETIME): Date of patient inclusion in the cohort. variable (VARCHAR(255)): Name of the variable to which the corresponding value in the value column refers. value (FLOAT): Value of variable in variable column. final_units (VARCHAR(255)): Final units of the value. lower_bound (FLOAT): Lower acceptable limit for the value. upper_bound (FLOAT): Upper acceptable limit for the value. Format : long Variables in varible column : te , cap . Primary Key: ( liveraim_id , cohort_id , variable ) Relations: Relates to the population table via liveraim_id and cohort_id . Actions and specification tables Liveraim name Liverscreen name Alcofib name Glucofib name Decide name Data type Rule/Notes liveraim_id - - - - VARCHAR(255) Created dynamically after merging cohorts. cohort_id id Codigo_deidentificacion id id VARCHAR(255) date_0 date_0 Fecha_V1 date_0 date_0 DATETIME variable - - - - VARCHAR(255) Obtained from melting the variables te , cap . value - - - - FLOAT Obtained from melting the variables te , cap . final_units - - - - VARCHAR(255) lower_bound - - - - FLOAT upper_bound - - - - FLOAT Melted variables metadata Liveraim name Liverscreen name Alcofib name Glucofib name Decide name Data type Rule/Notes te te LS_V1 te te float Data type modification Apply conversion factor cap cap CAP_V1 cap cap float Data type modification Apply conversion factor Table: medical_history This table structure must be reviewed. Description: Stores medical history of patients. Fields: liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. cohort_id (VARCHAR(255)): Cohort-specific identifier for each patient in a cohort. Unique within the cohort. date_0 (DATETIME): Date of patient inclusion in the cohort. hypertension (VARCHAR(255)): Hypertension information. centralobesity (VARCHAR(255)): Central obesity information. hightrigly (VARCHAR(255)): High triglycerides information. COPD (VARCHAR(255)): Chronic obstructive pulmonary disease information. smoke (VARCHAR(255)): Smoking status of the patient. alcohol_treat (VARCHAR(255)): Information on alcohol treatment. Format : wide Primary Key: ( liveraim_id , cohort_id ) Relations: Relates to the population table via liveraim_id and cohort_id . Actions and specification tables Liveraim name Liverscreen name Alcofib name Glucofib name Decide name SQL data type Data type Rule/Notes liveraim_id - - - - VARCHAR(255) str Created dynamically after merging cohorts. cohort_id id Codigo_deidentificacion id id VARCHAR(255) str date_0 date_0 Fecha_V1 date_0 date_0 DATETIME datetime hypertension hypertension HTA hypertension hypertension hypertension category Applied a mapping (to homogenize level codification) defined in level_data . centralobesity centralobesity Obesidad_abdominal centralobesity centralobesity centralobesity category Applied a mapping (to homogenize level codification) defined in level_data . hightrigly hightrigly Hipertriglic hightrigly hightrigly hightrigly category Applied a mapping (to homogenize level codification) defined in level_data . COPD comorbid___2 EPOC comorbid___2 comorbid___2 comorbid___2 category Applied a mapping (to homogenize level codification) defined in level_data . smoke smoke Tabaquismo smoke smoke smoke category Applied a mapping (to homogenize level codification) defined in level_data . alcohol_treat med_type___12 Medicacion_abstinencia med_type___12 med_type___12 med_type___12 category Applied a mapping (to homogenize level codification) defined in level_data . Table: blood_test_numerical Description: Stores numerical blood test results. Fields: date_0 (DATETIME): Date of patient inclusion in the cohort. liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. cohort_id (VARCHAR(255)): Cohort-specific identifier for each patient in a cohort. Unique within the cohort. variable (VARCHAR(255)): Type of variable measured. value (FLOAT): Value of the blood test result. final_units (VARCHAR(255)): Final units of the value. lower_bound (FLOAT): Lower acceptable limit for the value. upper_bound (FLOAT): Upper acceptable limit for the value. Format : long. Variables in varible column : crea , crp , glc , ast , alt , ggt , bili , cb , alk , prot_tot , alb , hto , plates , ferritin , fesat , inr , chol , hdl , ldl , tg , ghb , wcc . Primary Key: ( liveraim_id , cohort_id , variable ) Relations: Relates to the population table via liveraim_id and cohort_id . Actions and specification tables Liveraim name Liverscreen name Alcofib name Glucofib name Decide name Data type Rule/Notes liveraim_id - - - - VARCHAR(255) Created dynamically after merging cohorts. cohort_id id Codigo_deidentificacion id id VARCHAR(255) date_0 date_0 Fecha_V1 date_0 date_0 DATETIME variable - - - - VARCHAR(255) Obtained from melting the variables crea , crp , glc , ast , alt , ggt , bili , cb , alk , prot_tot , alb , hto , plates , ferritin , fesat , inr , chol , hdl , ldl , tg , ghb , wcc value - - - - FLOAT Obtained from melting the variables crea , crp , glc , ast , alt , ggt , bili , cb , alk , prot_tot , alb , hto , plates , ferritin , fesat , inr , chol , hdl , ldl , tg , ghb , wcc final_units - - - - VARCHAR(255) lower_bound - - - - FLOAT upper_bound - - - - FLOAT Melted variables metadata Liveraim name Liverscreen name Alcofib name Glucofib name Decide name Data type Rule/Notes crea crea_mg_dl Crea_V1 crea_mg_dl crea_mg_dl float Data type modification Apply conversion factor crp crp CRP_V1 crp crp float Data type modification Apply conversion factor glc glc_mg_dl Glu_V1 glc_mg_dl glc_mg_dl float Data type modification Apply conversion factor ast ast ASAT_V1 ast ast float Data type modification Apply conversion factor alt alt ALAT_V1 alt alt float Data type modification Apply conversion factor ggt ggt GGT_V1 ggt ggt float Data type modification Apply conversion factor bili bili_mgdl Bili_V1 bili_mgdl bili_mgdl float Data type modification Apply conversion factor cb cb_mg_dl Bili_directa_v1 cb_mg_dl cb_mg_dl float Data type modification Apply conversion factor alk alk FA_V1 alk alk float Data type modification Apply conversion factor prot_tot prot_g_l Prot_Tot_V1 prot_g_l prot_g_l float Data type modification Apply conversion factor alb alb Albumina_V1 alb alb float Data type modification Apply conversion factor hto hto HTO_V1 hto hto float Data type modification Apply conversion factor plates plates Plaq_V1 plates plates float Data type modification Apply conversion factor ferritin ferritin Ferr_V1 ferritin ferritin float Data type modification Apply conversion factor fesat fesat IS_V1 fesat fesat float Data type modification Apply conversion factor inr inr INR_V1 inr inr float Data type modification Apply conversion factor chol chol_mg_dl CT_V1 chol_mg_dl chol_mg_dl float Data type modification Apply conversion factor hdl hdl_mg_dl HDL_V1 hdl_mg_dl hdl_mg_dl float Data type modification Apply conversion factor ldl ldl_mg_dl LDL_V1 ldl_mg_dl ldl_mg_dl float Data type modification Apply conversion factor tg tg TG_V1 tg tg float Data type modification Apply conversion factor ghb ghb HbA1c_V1 ghb ghb float Data type modification Apply conversion factor wcc wcc LEU_V1 wcc wcc float Data type modification Apply conversion factor Table: blood_test_categorical Description: Stores categorical blood test results. Fields: date_0 (DATETIME): Date of patient inclusion in the cohort. liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. cohort_id (VARCHAR(255)): Cohort-specific identifier for each patient in a cohort. Unique within the cohort. variable (VARCHAR(255)): Name of the variable to which the corresponding value in the value column refers. value (VARCHAR(255)): Categorical result of the blood test. final_units (VARCHAR(255)): Final units of the value. lower_bound (FLOAT): Lower acceptable limit for the value (if applicable). upper_bound (FLOAT): Upper acceptable limit for the value (if applicable). Format : long. Variables in varible column : hbs_ag , hcv . Primary Key: ( liveraim_id , cohort_id , variable ) Relations: Relates to the population table via liveraim_id and cohort_id . Actions and specification tables Liveraim name Liverscreen name Alcofib name Glucofib name Decide name SQL data type Data type Rule/Notes liveraim_id - - - - VARCHAR(255) str Created dynamically after merging cohorts. cohort_id id Codigo_deidentificacion id id VARCHAR(255) str date_0 date_0 Fecha_V1 date_0 date_0 DATETIME datetime variable - - - - VARCHAR(255) Obtained from melting the variables hbs_ag , hcv . value - - - - VARCHAR(255) category Obtained from melting the variables hbs_ag , hcv . final_units - - - - VARCHAR(255) str lower_bound - - - - FLOAT float upper_bound - - - - FLOAT float Melted variables metadata Liveraim name Liverscreen name Alcofib name Glucofib name Decide name SQL data type Data type Rule/Notes hbs_ag hbs_ag HBsAg_V1 hbs_ag hbs_ag VARCHAR(255) category applied a mapping (to homogenize level codification) defined in level_data hcv hcv HCVAb_V1 hcv hcv VARCHAR(255) category applied a mapping (to homogenize level codification) defined in level_data Table: biomarkers Description: Stores biomarkers results, both from external providers (Nordic, Roche and Hospital Cl\u00ednic) and calculated using the data warehouse data. liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. variable (VARCHAR(255)): Blinded using a map from biomarker names to upper case letters. Codified/blinded name of the biomarker numeric_value (FLOAT):value/result of the e of biomarker analysis or calculation (in variable column) scaled_value (FLOAT): numeric value scaled to [0, 1] range. z_score (FLOAT): z_score of numeric_value quintiles (FLOAT): quintiles of numeric_value (from 1 to 5) limit_detect (VARCHAR(255)): Character indicating if the value in numeric_value is below ( < ) or above ( > ). Missing if it is in range. validation_date (DATETIME): validation_date of numeric_value results. For calculated biomarkers the inclusion_date is used as validation date comments (FLOAT): comments (see table of comments) Format : long. Variables in varible column : biomarker blinded codes (from A to K ). Primary Key: ( liveraim_id , variable ) Relations: Relates to the population table via liveraim_id . Column name SQL data type Data type Description Rule/Notes liveraim_id VARCHAR(255) str Intra-cohort identification code. Created dynamically after merging cohorts. variable VARCHAR(255) category Codified/blinded name of the biomarker. Blinded using a map from biomarker names to uppercase letters. numeric_value FLOAT float Value of biomarker (in variable column). Different transformations depending on data origin. Values clipped using detection limits. scaled_value FLOAT float Numeric value scaled to [0, 1] range. Computed from numeric_value using the min-max scaling method. z_score FLOAT float Z-score of numeric_value . Computed from numeric_value . quintiles FLOAT float Quintiles of numeric_value (from 1 to 5). Computed from numeric_value . limit_detect VARCHAR(255) str String indicating if numeric_value is below ( < ) or above ( > ). Computed using detection ranges. Missing if value is within range. No limits for calculated biomarkers. validation_date DATETIME datetime Validation date of numeric_value results. Date when result was validated. For calculated biomarkers, inclusion_date is used. comments VARCHAR(255) str Comments (see table of comments). Mapped from their description to an integer. Concatenated if multiple per record. Comments description Comment Provider Map Code Note Muestra insuficiente Hospital Cl\u00ednic 6 The associated value is missing. No calculable Hospital Cl\u00ednic 8 The associated value is missing. Below Lower Limit of Quantification Nordic 1 Hemolysis Nordic 3 Insufficient sample amount Nordic 6 The associated value is missing. Lipemic Nordic 5 Missing sample Nordic 8 The associated value is missing. Severe hemolysis Nordic 4 The associated value is missing. Unreliable test result and not reported Nordic 7 The associated value is missing. Note : In the current version, missings in numeric_value are dropped. Accordingly, code messages 4, 6, 7, 8 should not be found in the database. Varibles description liveraim_name data_type Units/levels panels Description liveraim_id string - all panels Intra-cohort identification code. cohort_id string - population Identifier for each cohort. inclusion_date datetime64[ns] date all cohort-related tables Date of visit 0. status category 0: Finished 1: Ongoing 2: Withdrawn population Status of the patients in the cohort. Finished if recruitment is complete, ongoing if still recruiting, withdrawn otherwise. birth_date datetime64[ns] date population Date of birth of the patients (calculated using variables age and inclusion_date). cohort category - population Cohort from which the patient originates. exit_date datetime64[ns] date population Date when recruitment finished or last version date for withdrawn patients. age float64 years demographics, population Age at inclusion. ethnicity category 0: Caucasian 1: Latin-American 2: African 3: Asian 4: Mix 5: Other demographics Ethnicity or continent of origin. gender category 0: Female 1: Male demographics, population Gender of the patient. hypertension category 0: No 1: Yes medical_history High blood pressure status. centralobesity category 0: No 1: Yes medical_history Abdominal obesity status. hightrigly category 0: No 1: Yes medical_history High triglycerides status. COPD category 0: No 1: Yes medical_history COPD comorbidity. smoke category 1: Current smoker 2: Prior smoking 3: Non-smoker medical_history Smoking history of the patient. weight float64 kg physical_exam Weight of the patient. height float64 cm physical_exam Height of the patient. bmi float64 kg/m\u00b2 physical_exam Body mass index. hip float64 cm physical_exam Hip circumference. probe category 0: M 1: XL fibroscan Probe type used in FibroScan. te float64 kPa fibroscan Median liver stiffness (kPa). cap float64 dB/m fibroscan CAP median. crea float64 mg/dL blood_test Creatinine (mg/dL). crp float64 mg/dL blood_test C-reactive protein (CRP, mg/dL). glc float64 mg/dL blood_test Glucose (mg/dL). ast float64 U/L blood_test Aspartate aminotransferase (AST). alt float64 U/L blood_test Alanine aminotransferase (ALT). ggt float64 U/L blood_test Gamma-glutamyl transferase (GGT). bili float64 mg/dL blood_test Bilirubin (mg/dL). cb float64 mg/dL blood_test Conjugated bilirubin (mg/dL). alk float64 U/L blood_test Alkaline phosphatase (U/L). prot_tot float64 g/L blood_test Total proteins (g/L). alb float64 g/L blood_test Albumin (g/L). hto float64 % blood_test Hematocrit. plates float64 10^9/L blood_test Platelet count. ferritin float64 ng/mL blood_test Ferritin (ng/mL). fesat float64 % blood_test Transferrin saturation (%). inr float64 - blood_test INR (International Normalized Ratio). chol float64 mg/dL blood_test Total cholesterol (mg/dL). hdl float64 mg/dL blood_test HDL-cholesterol (mg/dL). ldl float64 mg/dL blood_test LDL-cholesterol (mg/dL). tg float64 mg/dL blood_test Triglycerides (mg/dL). hcv category 0: Negative 1: Positive blood_test HCV antibody status. hbs_ag category 0: Negative 1: Positive blood_test HBs antigen status. alcohol_treat category 0: No 1: Yes medical_history Medication for alcoholism treatment. ghb float64 % blood_test Glycated hemoglobin (%). wcc float64 10^9/L blood_test White cell count. variable (biomarker) category - biomarkers Biomarker identification code (blinded). value (biomarkers) float64 Unknown units biomarkers Biomarker analysis value, may include '<' or '>' for values outside detection range. register_date datetime64[ns] date biomarkers Date when the sample was registered in the laboratory. b. Configuration Tables Description For further information about the structure of this configuration data go to initial data configuration section. General Relationships in the Schema Central Tables: The population table acts as the central table in the schema, relating to all other tables through liveraim_id and cohort_id . Foreign Keys: Tables containing specific data (such as fiscal_exam , fibroscan , blood_test_numerical , and blood_test_categorical ) use liveraim_id and cohort_id as foreign keys pointing to the population table.","title":"LIVERAIM Data Warehouse Specifications"},{"location":"liveraim_data_warehouse_specifications/#overview","text":"This section describes the structure, tables, and relationships within the data warehouse. The data warehouse is composed of several tables that can be divided into two groups: the database itself, which we will simply call database , and the configuration data tables, where information about the variables, the structure of the database, mapping dictionaries, etc., is stored. The database, in its first version (prior to receiving biomarker data), consists of the following tables: population : Contains basic information about each patient, such as gender, status, cohort membership, etc. blood_test_categorical : Contains categorical variables extracted from the blood test (at dat_0 ). blood_test_numerical : Contains numerical variables extracted from the blood test (at dat_0 ). demographics : Contains demographic information about the patient, such as gender, ethnicity, etc. fibroscan : Contains information related to the fibroscan test results for each patient. physical_exam : Contains information obtained from the physical examination, including variables such as weight, height, etc. medical_history : Contains information about each patient's medical history, including relevant comorbidities, etc. We will refer to these tables as panels or tables interchangeably. The population panel acts as the core of the database: the other panels are linked to this primary one (see General Relationships in the Schema ). The following image schematically depicts the structure and relationships of the data warehouse: The data warehouse contains two types of tables in terms of format: long format tables and wide format tables: Wide : These are the \"usual\" tables. Each variable is stored as a separate column in the table. Each row corresponds to the information of a single patient. In these tables, there cannot be duplicates in the identifier columns: all information for a patient is stored in a single row. Long : These tables contain at least one patient identifier column (in this case, liveraim_id and cohort_id ) along with the variable and value columns. The variable column contains the names of the variables, while the value column contains the value of the variable specified in the variable column. Tables in long format will have repeated entries in the patient identifier columns. In addition to the columns mentioned, these tables may include other columns that provide additional context for each entry: annotations for variable values ( lower_bound , upper_bound ), the date of each observation ( date_0 ), and the units of the variable ( final_units ). Transforming a wide table into a long table is a straightforward process (if the necessary data is available) and is referred to in this documentation as melting . Similarly, it is possible to transform a long format table into a wide format table with the same ease. An example of this format is shown below: date_0 liveraim_id cohort_id variable value final_units lower_bound upper_bound 2023-01-19 00:00:00 LA--- ----- alb NaN g/L 2023-01-19 00:00:00 LA--- ----- alk 41 U/L 2023-01-19 00:00:00 LA--- ----- alt 21 UI/L 2023-01-19 00:00:00 LA--- ----- ast 20 U/L 2023-01-19 00:00:00 LA--- ----- bili 0.6 mg/dL 2023-01-19 00:00:00 LA--- ----- cb NaN mg/dL 2023-01-19 00:00:00 LA--- ----- chol 236 mg/dL 2023-01-19 00:00:00 LA--- ----- crea 1.1 mg/dL 0 10 2023-01-19 00:00:00 LA--- ----- crp NaN mg/dL 2023-01-19 00:00:00 LA--- ----- ferritin 146 ng/mL ... For long tables, since there can (and should) be repetitions in the ID columns (one repetition for each variable in long format at least), composite primary keys are used. In general, this database uses the columns liveraim_id , cohort_id , and variable as composite primary keys: the database does not allow any repetitions of the combination of these three variables. Note : Using both liveraim_id and cohort_id within the composite keys is redundant, as there is a bijective correspondence between these two identifiers. However, it has been configured this way to maintain identifier consistency. It is important to clarify the difference between cohort_id and liveraim_id . cohort_id is the patient-specific identifier within each cohort. The encoding of these identifiers is heterogeneous, varies between cohorts, and originates from the original data. liveraim_id is a new variable generated during the execution of main.py , which assigns a unique identifier to each patient across all cohorts at the data warehouse level, following a common format. There is a bijective correspondence between these two variables. Note : The variables in the variable column (the \"melted\" variables) of each long table must be of the same data type. This is because SQL does not allow values with different data types in the same column. This adds robustness to the database at the cost of having to create additional tables. For example, in the case of blood test data, two tables are needed: blood_test_numerical and blood_test_categorical .","title":"Overview"},{"location":"liveraim_data_warehouse_specifications/#initial-data-and-configuration-data","text":"This section describes the initial data recieved from the partners and teh configuration files that the program needs to build the database. For each cohort, there are four types of files, which are described below: database : contains the raw data of the cohort var_data : contains configuration data about the core variables needed to create the data warehouse level_data : contains configuration data about the levels of categorical core variables needed to create the data warehouse comb_var_data : For the variables that need to be combined, it contains the necessary data to perform this process. Finally, an other file, panel_data is needed for the proper creation of the data warehouse. In this file the structure of the final panels is described.","title":"Initial data and configuration data"},{"location":"liveraim_data_warehouse_specifications/#database-files","text":"The files we refer to as database files contain the raw database of the cohort. These are the files that the partners responsible for each cohort have sent us with the patient data. The content of these files is never modified, neither manually nor through the code: all data processing is done after reading these files, which remain intact after each execution. The only modification they receive is a renaming necessary for their correct reading. For more details, see the section Structure of data/cohort_name/databases/ directory . The format of these files varies depending on the cohort. For more information on reading the data, see the section File reading utils . When this files are read, they are loaded into the code as pandas.DataFrame objects. Note : For obvious data privacy reasons, these files are not uploaded to GitHub.","title":"Database files"},{"location":"liveraim_data_warehouse_specifications/#var_data-files","text":"This file, along with level_data, is essential for the creation of the data warehouse. It is an Excel file (.xlsx) where each row corresponds to one of the selected/core variables. It contains the following columns: original_name (str) : Name of the core variable in the original database. It varies between cohorts. liveraim_name (str) : Name of the core variable in the common format, i.e., in the liveraim format. panel (str) : Panel where the variable will be stored. data_type (str) : Final datatype of the variable. initial_units (str) : Units of the variable in the original cohort. description (str) : Short text describing the variable. final_units (str) : Final units of the variable, i.e., units in the liveraim data warehouse. lower_bound (float) : Lower bound of the variable. For checking purposes. upper_bound (float) : Upper bound of the variable. For checking purposes. conversion_factor (float) : Conversion factor from initial_units to final_units. Set to 1 if not needed. Before running the code, you must verify that these files are in the correct directories, can be read correctly, have the structure defined above, and that the information they contain is correct. Any variation in structure/content may result in an error in execution or the creation of an erroneous data warehouse. The correct functioning of the program is highly dependent on these files. These files are tracked in GitHub like the rest of the scripts, so there is no need to create them from scratch, and they should work fine if the repository is cloned. When this file is read, it is loaded into the code as a pandas.DataFrame object. Throughout all the documentation, we will refer to this DataFrame also as var_data . Note 1 : There must be certain coherence between the var_data files from each cohort: the number of variables in each file (i.e., the number of rows) should be the same. In addition, the set of variables in liveraim_name should be the same between cohorts, and the final_data column should be consistent. Although this is checked in the code, making sure that there are no errors is recomended. Note 2 : If needed, addition of other columns is possible and should not create incompatibilities (but the management of those new columns should be implemented).","title":"var_data files"},{"location":"liveraim_data_warehouse_specifications/#level_data-files","text":"This file, along with var_data , is essential for the creation of the data warehouse. These files describe the different levels of the categorical core variables. In this case, each row does not correspond to a variable, but to a level of the original variable. The file contains the following columns: original_name (str) : Name of the core variable in the original database. It varies between cohorts. original_level_value (Variable) : Original value of the level in the cohort. original_data_type (str) : Original datatype of the variable. liveraim_level_value (Variable) : Value of the level in the final database (i.e., in the LIVERAIM data warehouse). level_desc (str) : Description/label of the level. liveraim_name (str) : Name of the core variable in the common format, i.e., in the liveraim format. Before running the code, you must verify that these files are in the correct directories, can be read correctly, have the structure defined above, and that the information they contain is correct. Any variation in structure/content may result in an error in execution or the creation of an erroneous data warehouse. The correct functioning of the program is highly dependent on these files. These files are tracked in GitHub like the rest of the scripts, so there is no need to create them from scratch, and they should work fine if the repository is cloned. When this file is read, it is loaded into the code as a pandas.DataFrame object. Throughout all the documentation, we will refer to this DataFrame also as level_data . Note : There must be certain coherence between the level_data files from each cohort: In this case, the number of rows in each file does not need to match, as in some cohorts two or more different levels might coincide in the same final level. However, the original_name , liveraim_name , and liveraim_level_value columns should be consistent between files. Although this is checked in the code, it is recommended to ensure there are no errors.","title":"level_data files"},{"location":"liveraim_data_warehouse_specifications/#comb_var_data-file","text":"This is a json file that will be loaded into the code as a dictionary. It contains the necessary information to obtain a final variable by combining original variables present in the database. For more information about this process, see the section data_processing_utils , subsection class VarCombiner . This section describes the class responsible for combining the variables listed in comb_var_data according to the specified configuration. The primary goal of this dictionary is to combine variables that refer to the same magnitude but use different units. Combining these variables helps reduce the number of missing values. For example, in the LIVERSCREEN cohort, for the magnitude blood glucose (in visit 1), there are the variables glc (expressed in mmol/L) and glc_mg_dl (expressed in mg/dL). Both will be combined to fill missing at least one of the variables have a proper value. Note : Currently, only the functionality described above has been implemented. However, it can be extended (by modifying this dictionary and the VarCombiner class) to combine categorical variables, generate secondary or calculated variables, and more. The structure of the comb_var_data dictionary is as follows: Primary Key : The name of the final variable resulting from the combination. This variable must appear in the var_data file described in this documentation. It usually matches the name of one of the variables to be used in the combination. Primary Value : A dictionary with the following structure: Secondary Key : Names of the variables (in the database) that will be used for the combination. Secondary Value : Conversion factor to transform the variable from its original units to the final units (i.e., the units of the variable named by the primary key). The structure of the dictionary would be: comb_var_data: <final_variable_name_1>: # Primary Key: Name of the final variable <original_variable_name_1>: <conversion_factor_1> # Secondary Key-Value: Original variable and its conversion factor <original_variable_name_2>: <conversion_factor_2> # Secondary Key-Value: Original variable and its conversion factor <final_variable_name_2>: # Primary Key: Name of another final variable <original_variable_name_3>: <conversion_factor_3> # Secondary Key-Value: Original variable and its conversion factor <original_variable_name_4>: <conversion_factor_4> # Secondary Key-Value: Original variable and its conversion factor ... And an example: { \"glc\": { \"glc\": 1, \"glc_mg_dl\": 0.055 }, \"crea_mg_dl\": { \"creat\": 0.017, \"crea_mg_dl\": 1 } } In this case, it indicates that the glc variable will be created by combining the glc itself and glc_mg_dl variables. Specifically, the value of glc will be used if it exists, and if not, the value of glc_mg_dl will be used, multiplied by the corresponding conversion factor (in this case, 0.055). A similar procedure will be aplied to crea_mg_dl . These files are tracked in GitHub like the rest of the scripts, so there is no need to create them from scratch, and they should work fine if the repository is cloned.","title":"comb_var_data file"},{"location":"liveraim_data_warehouse_specifications/#panel_data-file","text":"This file is not specific to any cohort and is used as a guide to create and structure the different final panels. The file panel_metadata.xlsx is an Excel file that contains a sheet for each final panel. Each of these panels includes the following columns: var_name (str) : Name of the variable (in the common liveraim format). melt (int) : Integer (acting as a boolean, so it is 0 or 1) indicating: 0 : The variable is not melted: it will appear in the panel as a column. 1 : The variable is melted: the variable name and value will be restructured in a long format. A column variable will contain the name of the variable, and the column value will contain its value. These files are tracked in GitHub like the rest of the scripts, so there is no need to create them from scratch, and they should work fine if the repository is cloned. When this file is read, since it is an .xlsx file with multiple sheets, it is loaded into the code as a dictionary of pandas.DataFrames . Each sheet name becomes a key, and the corresponding table becomes its value (a DataFrame ). Throughout the documentation, we will refer to this dictionary as panel_metadata . It is important to maintain the structure of this file (and the previously described). Changes can be easily made to modify the final structure of the data warehouse by adding new sheets to the file (i.e., adding new panels), adding variables to a panel, or changing their structure (long or wide format). For more information about the structure of the final data, check the next section LIVERAIM DATA WAREHOUSE STRUCTURE .","title":"panel_data file"},{"location":"liveraim_data_warehouse_specifications/#liveraim-data-warehouse-structure","text":"The LIVERAIM DATA WAREHOUSE v0.1 is composed by the following tables: a. Database population blood_test_categorical blood_test_numerical demographics fibroscan physical_exam medical_history b. Configuration Data panel_metadata var_data level_data comb_var_data dictionaries","title":"Liveraim Data Warehouse Structure"},{"location":"liveraim_data_warehouse_specifications/#a-database-tables-description","text":"","title":"a. Database Tables Description"},{"location":"liveraim_data_warehouse_specifications/#table-sources","text":"For each cohort, there is at least one raw database containing patient data. Each database corresponds to a single file, and for some cohorts, different versions of the same database are available. If multiple versions are available, the data from the latest version for each patient will be used. During the data processing phase, the databases are merged into a single table, which is later split and restructured into the various tables of the final database. From each raw database, a subset of variables is selected. These variables must be common across all cohort databases (although the variable names may differ), ensuring that patient data from different cohorts can be merged. Thus, all variables that appear in the final database are either secondary variables created during the code execution, or they originate from one of the four cohorts (depending on the patient's source) used to construct the final database. For more information about the dataflow go to dataflow section. A summary of the data sources is presented below: Cohort Number of versions used File type Reading method liverscreen 7 stata file ( .dta ) pd.read_stata alcofib 2 sav file ( .sav ) pyreadstat.read_sav glucofib 1 csv ( .csv ) pd.read_csv decide 1 stata file ( .dta ) pd.read_stata galaald 1 stata file ( .dta ) pd.read_stata marina 2 sav file ( .sav ) pyreadstat.read_sav Note 1 : Some of the information below may be redundant, as certain variables appear in many or all panels. However, redundancies have been intentionally kept for the sake of clarity and readability. Note 2 : As three of the four used cohorts have been previously managed isnod RedCap, many RedCap names in the final database have been kept as standard (sometimes with few changes).","title":"Table sources:"},{"location":"liveraim_data_warehouse_specifications/#table-population","text":"Description: Defines the population of patients within different cohorts. Fields: liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. cohort_id (VARCHAR(255)): Cohort-specific identifier for each patient in a cohort. Unique within the cohort. gender (VARCHAR(8)): Gender of the patient. birth_date (DATETIME): Date of birth of the patient. status (VARCHAR(8)): Current status of the patient (ongoing,, withdrawn, finished). date_0 (DATETIME): Date of patient inclusion in the cohort. exit_date (DATETIME): Date of study exit. Format : wide Primary Key: ( liveraim_id , cohort_id ) Relations: Central table that relates to all other tables using liveraim_id and cohort_id .","title":"Table: population"},{"location":"liveraim_data_warehouse_specifications/#actions-and-specification-tables","text":"Liveraim name Liverscreen name Alcofib name Glucofib name Decide name SQL data type data type Rule/Notes liveraim_id - - - - VARCHAR(255) str Created dynamically after merging cohorts. cohort_id id Codigo_deidentificacion id id VARCHAR(255) str gender gender Sexe gender gender FLOAT category Applied a mapping (to homogenize level codification) defined in level_data . birth_date - - - - DATETIME datetime Variable calculated from patient's age and date_0 (inclusion date). date_0 date_0 Fecha_V1 date_0 date_0 DATETIME datetime status - - - - VARCHAR(8) category Calculated variable. If cohort recruitment is ongoing, it is set to ongoing . If cohort recruitment is finished, it is set to finished . If the patient has left the cohort (in cases where more than one version is available), it is set to withdrawn . exit_date - - - - DATETIME datetime Calculated variable. If the patient's status is ongoing, it is set to the current date (execution date). If the status is finished, it is set to the date of the last version. If the status is withdrawn, it is set to the date of the last version in which the patient appears.","title":"Actions and specification tables"},{"location":"liveraim_data_warehouse_specifications/#table-physical_exam","text":"Description: Stores physical exam results. Fields: liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. cohort_id (VARCHAR(255)): Cohort-specific identifier for each patient in a cohort. Unique within the cohort. date_0 (DATETIME): Date of patient inclusion in the cohort. variable (VARCHAR(255)): Name of the variable to which the corresponding value in the value column refers. value (FLOAT): Value of variable in variable column. final_units (VARCHAR(255)): Final units of the value. lower_bound (FLOAT): Lower acceptable limit for the value. upper_bound (FLOAT): Upper acceptable limit for the value. Format : Long. Variables in varible column : weight , height , bmi , hip . Primary Key: ( liveraim_id , cohort_id , variable ) Relations: Relates to the population table via liveraim_id and cohort_id .","title":"Table: physical_exam"},{"location":"liveraim_data_warehouse_specifications/#actions-and-specification-tables_1","text":"Liveraim name Liverscreen name Alcofib name Glucofib name Decide name Data type Rule/Notes liveraim_id - - - - VARCHAR(255) Created dynamically after merging cohorts. cohort_id id Codigo_deidentificacion id id VARCHAR(255) date_0 date_0 Fecha_V1 date_0 date_0 DATETIME variable - - - - VARCHAR(255) Obtained from melting the variables weight , height , bmi , hip value - - - - FLOAT Obtained from melting the variables weight , height , bmi , hip final_units - - - - VARCHAR(255) lower_bound - - - - FLOAT upper_bound - - - - FLOAT","title":"Actions and specification tables"},{"location":"liveraim_data_warehouse_specifications/#melted-variables-metadata","text":"Liveraim name Liverscreen name Alcofib name Glucofib name Decide name Data type Rule/Notes weight weight Peso_V1 weight weight float Data type modification Apply conversion factor height height Talla_V1 height height float Data type modification Apply conversion factor bmi bmi BMI_V1 bmi bmi float Data type modification Apply conversion factor hip hip diametro_cintura hip hip float Data type modification Apply conversion factor","title":"Melted variables metadata"},{"location":"liveraim_data_warehouse_specifications/#table-demographics","text":"Description: Contains demographic information of patients. Fields: liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. cohort_id (VARCHAR(255)): Cohort-specific identifier for each patient in a cohort. Unique within the cohort. age (FLOAT): Age of the patient. ethnicity (VARCHAR(255)): Ethnicity of the patient. gender (VARCHAR(255)): Gender of the patient. date_0 (DATETIME): Date of patient inclusion in the cohort. Format : wide. Primary Key: ( liveraim_id , cohort_id ) Relations: Relates to the population table via liveraim_id and cohort_id .","title":"Table: demographics"},{"location":"liveraim_data_warehouse_specifications/#actions-and-specification-tables_2","text":"Liveraim name Liverscreen name Alcofib name Glucofib name Decide name SQL data type data type Rule/Notes liveraim_id - - - - VARCHAR(255) str Created dynamically after merging cohorts. cohort_id id Codigo_deidentificacion id id VARCHAR(255) str date_0 date_0 Fecha_V1 date_0 date_0 DATETIME datetime ethnicity ethnicity Etnia ethnicity ethnicity VARCHAR(255) category Applied a mapping (to homogenize level codification) defined in level_data gender gender Sexe gender gender VARCHAR(255) category Applied a mapping (to homogenize level codification) defined in level_data","title":"Actions and specification tables"},{"location":"liveraim_data_warehouse_specifications/#table-fibroscan","text":"Description: Records fibroscan exam results. Fields: liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. cohort_id (VARCHAR(255)): Cohort-specific identifier for each patient in a cohort. Unique within the cohort. date_0 (DATETIME): Date of patient inclusion in the cohort. variable (VARCHAR(255)): Name of the variable to which the corresponding value in the value column refers. value (FLOAT): Value of variable in variable column. final_units (VARCHAR(255)): Final units of the value. lower_bound (FLOAT): Lower acceptable limit for the value. upper_bound (FLOAT): Upper acceptable limit for the value. Format : long Variables in varible column : te , cap . Primary Key: ( liveraim_id , cohort_id , variable ) Relations: Relates to the population table via liveraim_id and cohort_id .","title":"Table: fibroscan"},{"location":"liveraim_data_warehouse_specifications/#actions-and-specification-tables_3","text":"Liveraim name Liverscreen name Alcofib name Glucofib name Decide name Data type Rule/Notes liveraim_id - - - - VARCHAR(255) Created dynamically after merging cohorts. cohort_id id Codigo_deidentificacion id id VARCHAR(255) date_0 date_0 Fecha_V1 date_0 date_0 DATETIME variable - - - - VARCHAR(255) Obtained from melting the variables te , cap . value - - - - FLOAT Obtained from melting the variables te , cap . final_units - - - - VARCHAR(255) lower_bound - - - - FLOAT upper_bound - - - - FLOAT","title":"Actions and specification tables"},{"location":"liveraim_data_warehouse_specifications/#melted-variables-metadata_1","text":"Liveraim name Liverscreen name Alcofib name Glucofib name Decide name Data type Rule/Notes te te LS_V1 te te float Data type modification Apply conversion factor cap cap CAP_V1 cap cap float Data type modification Apply conversion factor","title":"Melted variables metadata"},{"location":"liveraim_data_warehouse_specifications/#table-medical_history","text":"This table structure must be reviewed. Description: Stores medical history of patients. Fields: liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. cohort_id (VARCHAR(255)): Cohort-specific identifier for each patient in a cohort. Unique within the cohort. date_0 (DATETIME): Date of patient inclusion in the cohort. hypertension (VARCHAR(255)): Hypertension information. centralobesity (VARCHAR(255)): Central obesity information. hightrigly (VARCHAR(255)): High triglycerides information. COPD (VARCHAR(255)): Chronic obstructive pulmonary disease information. smoke (VARCHAR(255)): Smoking status of the patient. alcohol_treat (VARCHAR(255)): Information on alcohol treatment. Format : wide Primary Key: ( liveraim_id , cohort_id ) Relations: Relates to the population table via liveraim_id and cohort_id .","title":"Table: medical_history"},{"location":"liveraim_data_warehouse_specifications/#actions-and-specification-tables_4","text":"Liveraim name Liverscreen name Alcofib name Glucofib name Decide name SQL data type Data type Rule/Notes liveraim_id - - - - VARCHAR(255) str Created dynamically after merging cohorts. cohort_id id Codigo_deidentificacion id id VARCHAR(255) str date_0 date_0 Fecha_V1 date_0 date_0 DATETIME datetime hypertension hypertension HTA hypertension hypertension hypertension category Applied a mapping (to homogenize level codification) defined in level_data . centralobesity centralobesity Obesidad_abdominal centralobesity centralobesity centralobesity category Applied a mapping (to homogenize level codification) defined in level_data . hightrigly hightrigly Hipertriglic hightrigly hightrigly hightrigly category Applied a mapping (to homogenize level codification) defined in level_data . COPD comorbid___2 EPOC comorbid___2 comorbid___2 comorbid___2 category Applied a mapping (to homogenize level codification) defined in level_data . smoke smoke Tabaquismo smoke smoke smoke category Applied a mapping (to homogenize level codification) defined in level_data . alcohol_treat med_type___12 Medicacion_abstinencia med_type___12 med_type___12 med_type___12 category Applied a mapping (to homogenize level codification) defined in level_data .","title":"Actions and specification tables"},{"location":"liveraim_data_warehouse_specifications/#table-blood_test_numerical","text":"Description: Stores numerical blood test results. Fields: date_0 (DATETIME): Date of patient inclusion in the cohort. liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. cohort_id (VARCHAR(255)): Cohort-specific identifier for each patient in a cohort. Unique within the cohort. variable (VARCHAR(255)): Type of variable measured. value (FLOAT): Value of the blood test result. final_units (VARCHAR(255)): Final units of the value. lower_bound (FLOAT): Lower acceptable limit for the value. upper_bound (FLOAT): Upper acceptable limit for the value. Format : long. Variables in varible column : crea , crp , glc , ast , alt , ggt , bili , cb , alk , prot_tot , alb , hto , plates , ferritin , fesat , inr , chol , hdl , ldl , tg , ghb , wcc . Primary Key: ( liveraim_id , cohort_id , variable ) Relations: Relates to the population table via liveraim_id and cohort_id .","title":"Table: blood_test_numerical"},{"location":"liveraim_data_warehouse_specifications/#actions-and-specification-tables_5","text":"Liveraim name Liverscreen name Alcofib name Glucofib name Decide name Data type Rule/Notes liveraim_id - - - - VARCHAR(255) Created dynamically after merging cohorts. cohort_id id Codigo_deidentificacion id id VARCHAR(255) date_0 date_0 Fecha_V1 date_0 date_0 DATETIME variable - - - - VARCHAR(255) Obtained from melting the variables crea , crp , glc , ast , alt , ggt , bili , cb , alk , prot_tot , alb , hto , plates , ferritin , fesat , inr , chol , hdl , ldl , tg , ghb , wcc value - - - - FLOAT Obtained from melting the variables crea , crp , glc , ast , alt , ggt , bili , cb , alk , prot_tot , alb , hto , plates , ferritin , fesat , inr , chol , hdl , ldl , tg , ghb , wcc final_units - - - - VARCHAR(255) lower_bound - - - - FLOAT upper_bound - - - - FLOAT","title":"Actions and specification tables"},{"location":"liveraim_data_warehouse_specifications/#melted-variables-metadata_2","text":"Liveraim name Liverscreen name Alcofib name Glucofib name Decide name Data type Rule/Notes crea crea_mg_dl Crea_V1 crea_mg_dl crea_mg_dl float Data type modification Apply conversion factor crp crp CRP_V1 crp crp float Data type modification Apply conversion factor glc glc_mg_dl Glu_V1 glc_mg_dl glc_mg_dl float Data type modification Apply conversion factor ast ast ASAT_V1 ast ast float Data type modification Apply conversion factor alt alt ALAT_V1 alt alt float Data type modification Apply conversion factor ggt ggt GGT_V1 ggt ggt float Data type modification Apply conversion factor bili bili_mgdl Bili_V1 bili_mgdl bili_mgdl float Data type modification Apply conversion factor cb cb_mg_dl Bili_directa_v1 cb_mg_dl cb_mg_dl float Data type modification Apply conversion factor alk alk FA_V1 alk alk float Data type modification Apply conversion factor prot_tot prot_g_l Prot_Tot_V1 prot_g_l prot_g_l float Data type modification Apply conversion factor alb alb Albumina_V1 alb alb float Data type modification Apply conversion factor hto hto HTO_V1 hto hto float Data type modification Apply conversion factor plates plates Plaq_V1 plates plates float Data type modification Apply conversion factor ferritin ferritin Ferr_V1 ferritin ferritin float Data type modification Apply conversion factor fesat fesat IS_V1 fesat fesat float Data type modification Apply conversion factor inr inr INR_V1 inr inr float Data type modification Apply conversion factor chol chol_mg_dl CT_V1 chol_mg_dl chol_mg_dl float Data type modification Apply conversion factor hdl hdl_mg_dl HDL_V1 hdl_mg_dl hdl_mg_dl float Data type modification Apply conversion factor ldl ldl_mg_dl LDL_V1 ldl_mg_dl ldl_mg_dl float Data type modification Apply conversion factor tg tg TG_V1 tg tg float Data type modification Apply conversion factor ghb ghb HbA1c_V1 ghb ghb float Data type modification Apply conversion factor wcc wcc LEU_V1 wcc wcc float Data type modification Apply conversion factor","title":"Melted variables metadata"},{"location":"liveraim_data_warehouse_specifications/#table-blood_test_categorical","text":"Description: Stores categorical blood test results. Fields: date_0 (DATETIME): Date of patient inclusion in the cohort. liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. cohort_id (VARCHAR(255)): Cohort-specific identifier for each patient in a cohort. Unique within the cohort. variable (VARCHAR(255)): Name of the variable to which the corresponding value in the value column refers. value (VARCHAR(255)): Categorical result of the blood test. final_units (VARCHAR(255)): Final units of the value. lower_bound (FLOAT): Lower acceptable limit for the value (if applicable). upper_bound (FLOAT): Upper acceptable limit for the value (if applicable). Format : long. Variables in varible column : hbs_ag , hcv . Primary Key: ( liveraim_id , cohort_id , variable ) Relations: Relates to the population table via liveraim_id and cohort_id .","title":"Table: blood_test_categorical"},{"location":"liveraim_data_warehouse_specifications/#actions-and-specification-tables_6","text":"Liveraim name Liverscreen name Alcofib name Glucofib name Decide name SQL data type Data type Rule/Notes liveraim_id - - - - VARCHAR(255) str Created dynamically after merging cohorts. cohort_id id Codigo_deidentificacion id id VARCHAR(255) str date_0 date_0 Fecha_V1 date_0 date_0 DATETIME datetime variable - - - - VARCHAR(255) Obtained from melting the variables hbs_ag , hcv . value - - - - VARCHAR(255) category Obtained from melting the variables hbs_ag , hcv . final_units - - - - VARCHAR(255) str lower_bound - - - - FLOAT float upper_bound - - - - FLOAT float","title":"Actions and specification tables"},{"location":"liveraim_data_warehouse_specifications/#melted-variables-metadata_3","text":"Liveraim name Liverscreen name Alcofib name Glucofib name Decide name SQL data type Data type Rule/Notes hbs_ag hbs_ag HBsAg_V1 hbs_ag hbs_ag VARCHAR(255) category applied a mapping (to homogenize level codification) defined in level_data hcv hcv HCVAb_V1 hcv hcv VARCHAR(255) category applied a mapping (to homogenize level codification) defined in level_data","title":"Melted variables metadata"},{"location":"liveraim_data_warehouse_specifications/#table-biomarkers","text":"Description: Stores biomarkers results, both from external providers (Nordic, Roche and Hospital Cl\u00ednic) and calculated using the data warehouse data. liveraim_id (VARCHAR(255)): Common liveraim identifier for the patient. Unique within the data warehouse. variable (VARCHAR(255)): Blinded using a map from biomarker names to upper case letters. Codified/blinded name of the biomarker numeric_value (FLOAT):value/result of the e of biomarker analysis or calculation (in variable column) scaled_value (FLOAT): numeric value scaled to [0, 1] range. z_score (FLOAT): z_score of numeric_value quintiles (FLOAT): quintiles of numeric_value (from 1 to 5) limit_detect (VARCHAR(255)): Character indicating if the value in numeric_value is below ( < ) or above ( > ). Missing if it is in range. validation_date (DATETIME): validation_date of numeric_value results. For calculated biomarkers the inclusion_date is used as validation date comments (FLOAT): comments (see table of comments) Format : long. Variables in varible column : biomarker blinded codes (from A to K ). Primary Key: ( liveraim_id , variable ) Relations: Relates to the population table via liveraim_id . Column name SQL data type Data type Description Rule/Notes liveraim_id VARCHAR(255) str Intra-cohort identification code. Created dynamically after merging cohorts. variable VARCHAR(255) category Codified/blinded name of the biomarker. Blinded using a map from biomarker names to uppercase letters. numeric_value FLOAT float Value of biomarker (in variable column). Different transformations depending on data origin. Values clipped using detection limits. scaled_value FLOAT float Numeric value scaled to [0, 1] range. Computed from numeric_value using the min-max scaling method. z_score FLOAT float Z-score of numeric_value . Computed from numeric_value . quintiles FLOAT float Quintiles of numeric_value (from 1 to 5). Computed from numeric_value . limit_detect VARCHAR(255) str String indicating if numeric_value is below ( < ) or above ( > ). Computed using detection ranges. Missing if value is within range. No limits for calculated biomarkers. validation_date DATETIME datetime Validation date of numeric_value results. Date when result was validated. For calculated biomarkers, inclusion_date is used. comments VARCHAR(255) str Comments (see table of comments). Mapped from their description to an integer. Concatenated if multiple per record.","title":"Table: biomarkers"},{"location":"liveraim_data_warehouse_specifications/#comments-description","text":"Comment Provider Map Code Note Muestra insuficiente Hospital Cl\u00ednic 6 The associated value is missing. No calculable Hospital Cl\u00ednic 8 The associated value is missing. Below Lower Limit of Quantification Nordic 1 Hemolysis Nordic 3 Insufficient sample amount Nordic 6 The associated value is missing. Lipemic Nordic 5 Missing sample Nordic 8 The associated value is missing. Severe hemolysis Nordic 4 The associated value is missing. Unreliable test result and not reported Nordic 7 The associated value is missing. Note : In the current version, missings in numeric_value are dropped. Accordingly, code messages 4, 6, 7, 8 should not be found in the database.","title":"Comments description"},{"location":"liveraim_data_warehouse_specifications/#varibles-description","text":"liveraim_name data_type Units/levels panels Description liveraim_id string - all panels Intra-cohort identification code. cohort_id string - population Identifier for each cohort. inclusion_date datetime64[ns] date all cohort-related tables Date of visit 0. status category 0: Finished 1: Ongoing 2: Withdrawn population Status of the patients in the cohort. Finished if recruitment is complete, ongoing if still recruiting, withdrawn otherwise. birth_date datetime64[ns] date population Date of birth of the patients (calculated using variables age and inclusion_date). cohort category - population Cohort from which the patient originates. exit_date datetime64[ns] date population Date when recruitment finished or last version date for withdrawn patients. age float64 years demographics, population Age at inclusion. ethnicity category 0: Caucasian 1: Latin-American 2: African 3: Asian 4: Mix 5: Other demographics Ethnicity or continent of origin. gender category 0: Female 1: Male demographics, population Gender of the patient. hypertension category 0: No 1: Yes medical_history High blood pressure status. centralobesity category 0: No 1: Yes medical_history Abdominal obesity status. hightrigly category 0: No 1: Yes medical_history High triglycerides status. COPD category 0: No 1: Yes medical_history COPD comorbidity. smoke category 1: Current smoker 2: Prior smoking 3: Non-smoker medical_history Smoking history of the patient. weight float64 kg physical_exam Weight of the patient. height float64 cm physical_exam Height of the patient. bmi float64 kg/m\u00b2 physical_exam Body mass index. hip float64 cm physical_exam Hip circumference. probe category 0: M 1: XL fibroscan Probe type used in FibroScan. te float64 kPa fibroscan Median liver stiffness (kPa). cap float64 dB/m fibroscan CAP median. crea float64 mg/dL blood_test Creatinine (mg/dL). crp float64 mg/dL blood_test C-reactive protein (CRP, mg/dL). glc float64 mg/dL blood_test Glucose (mg/dL). ast float64 U/L blood_test Aspartate aminotransferase (AST). alt float64 U/L blood_test Alanine aminotransferase (ALT). ggt float64 U/L blood_test Gamma-glutamyl transferase (GGT). bili float64 mg/dL blood_test Bilirubin (mg/dL). cb float64 mg/dL blood_test Conjugated bilirubin (mg/dL). alk float64 U/L blood_test Alkaline phosphatase (U/L). prot_tot float64 g/L blood_test Total proteins (g/L). alb float64 g/L blood_test Albumin (g/L). hto float64 % blood_test Hematocrit. plates float64 10^9/L blood_test Platelet count. ferritin float64 ng/mL blood_test Ferritin (ng/mL). fesat float64 % blood_test Transferrin saturation (%). inr float64 - blood_test INR (International Normalized Ratio). chol float64 mg/dL blood_test Total cholesterol (mg/dL). hdl float64 mg/dL blood_test HDL-cholesterol (mg/dL). ldl float64 mg/dL blood_test LDL-cholesterol (mg/dL). tg float64 mg/dL blood_test Triglycerides (mg/dL). hcv category 0: Negative 1: Positive blood_test HCV antibody status. hbs_ag category 0: Negative 1: Positive blood_test HBs antigen status. alcohol_treat category 0: No 1: Yes medical_history Medication for alcoholism treatment. ghb float64 % blood_test Glycated hemoglobin (%). wcc float64 10^9/L blood_test White cell count. variable (biomarker) category - biomarkers Biomarker identification code (blinded). value (biomarkers) float64 Unknown units biomarkers Biomarker analysis value, may include '<' or '>' for values outside detection range. register_date datetime64[ns] date biomarkers Date when the sample was registered in the laboratory.","title":"Varibles description"},{"location":"liveraim_data_warehouse_specifications/#b-configuration-tables-description","text":"For further information about the structure of this configuration data go to initial data configuration section.","title":"b. Configuration Tables Description"},{"location":"liveraim_data_warehouse_specifications/#general-relationships-in-the-schema","text":"Central Tables: The population table acts as the central table in the schema, relating to all other tables through liveraim_id and cohort_id . Foreign Keys: Tables containing specific data (such as fiscal_exam , fibroscan , blood_test_numerical , and blood_test_categorical ) use liveraim_id and cohort_id as foreign keys pointing to the population table.","title":"General Relationships in the Schema"},{"location":"quick_start_guide/","text":"Fast Configuration This section describes the necessary configuration to run the code and generate the data warehouse (DW). For a comprehensive description of all configuration parameters, refer to the section Configuration Module . Important Notice : Before configuring the project, ensure that the prerequisites described in the Environment Setup section are met. Note : If you have downloaded the repository from GitHub, the structure of the entire repository should be correct. In this case, and if you want to run the code with the default configuration, only the following modifications would be needed: Configure the /data/cohort_name/databases directory for each cohort. See the section Structure of the data/cohort_name/databases/ Directory . Configure the MySQL DB connection parameters. See the section MySQL Connection Configuration . Structure of the Project Directory While the configuration allows some flexibility in the project directory structure, it is recommended to maintain the current structure to avoid errors. The project directory currently (as of 08/07/2024) has the following structure: DATA_WAREHOUSE/ \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 biomarkers_data \u2502 \u2502 \u251c\u2500\u2500 nordic_biomarkers_data \u2502 \u2502 \u2514\u2500\u2500 hclinic_biomarkers_data \u2502 \u251c\u2500\u2500 WP1 \u2502 \u2502 \u251c\u2500\u2500 Liverscreen \u2502 \u2502 \u251c\u2500\u2500 Alcofib \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u251c\u2500\u2500 WP2 \u2502 \u2502 \u2514\u2500\u2500 Galaald \u2502 \u2514\u2500\u2500 numeric_bounds.json \u2514\u2500\u2500 scr \u251c\u2500\u2500 config/ \u2502 \u251c\u2500\u2500 cohort_config.py \u2502 \u251c\u2500\u2500 connection_config.py \u2502 \u251c\u2500\u2500 main_config.py \u2502 \u251c\u2500\u2500 new_levels_config.py \u2502 \u251c\u2500\u2500 new_var_config.py \u2502 \u2514\u2500\u2500 reference_names.py \u251c\u2500\u2500 logs/ \u2502 ... \u251c\u2500\u2500 qc_report/ \u2502 ... \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 main.py ... other python modules Structure of data/cohort_name/ Directory Each subdirectory within data/ contains the databases and files related to each cohort or the biomarker data. These subdirectories should have the following structure: data/ \u251c\u2500\u2500 WP1 \u251c\u2500\u2500 <cohort_name>/ \u2502 \u251c\u2500\u2500 databases/ \u2502 \u2502 \u251c\u2500\u2500 <cohort_name_database_version_date>.extension \u2502 \u2502 ... \u2502 \u251c\u2500\u2500 <cohort_name>_level_data.xlsx \u2502 \u2514\u2500\u2500 <cohort_name>_var_data.xlsx \u2514\u2500\u2500 panel_data.xlsx \u251c\u2500\u2500 WP2 \u251c\u2500\u2500 ... ... \u2514\u2500\u2500 biomarkers_data \u251c\u2500\u2500 nordic_biomarkers_data \u251c\u2500\u2500 <name_database_version_date>.extension ... \u2514\u2500\u2500 hclinic_biomarkers_data \u251c\u2500\u2500 <name_database_version_date>.extension ... ... The cohort_name_level_data.xlsx and cohort_name_var_data.xlsx files are described in the section Initial Data and Configuration data . If you downloaded the repository from GitHub, you should not need to modify these files. Structure of data/cohort_name/databases/ Directory The /data/<cohort_name>/databases directory should store the files containing the raw data for the databases of each cohort. Each file in the directory corresponds to one of the received database versions. For more information on how different versions are handled, see the section Managing Raw Data Versions . It is very important (for this particular case) that the file names follow this structure: <cohort_name>_database_<version_date>.extension For example, the raw data file for the Liverscreen cohort received on 29/07/2024 in .dta format would be named: liverscreen_database_20240729.dta Note that the name is in lowercase and the date is in the yyyymmdd format. Note : This folder may contain only one file. It is recommended that this corresponds to the latest version of the data. Similarly, the directory /data/biomarkers_data/<provider>_biomarkers_data should store the files containing the raw data from the biomarkers transfered by this particular provider. This folder may contain different versions or batches of data. Those different files are, again, read and merged during the execution of the database. For further information on the merging of this data, see section Biomerkers data Management . Biomarker data files must also follow the structure: <provider>_biomarkers_<batch_date>.extension For example, data tranfered from Nordic recieved on 29/07/2024 in .csv format would be named: nordic_biomarkers_20240729.csv Since these files are not uploaded to GitHub, it is necessary to manually place them in the correct directory and format. Once this is done, the directory should be correctly configured. MySQL Connection Configuration If you want to export the data warehouse to the MySQL DB (presumably yes), the MySQL connection parameters must be configured. Therefore, it is necessary to have MySQL installed (if you want to generate the DB locally) and to have a username and password. [A section explaining how to download MySQL and create a user is missing]. Once these requirements are met, you need to configure the MySQL DB connection parameters. These are defined in the config.connection_config module, but for a quick configuration, you do not need to modify this file. Instead, you should create the following environment variables in your execution environment: USER (string): Username (must have permission to access the database). PASSWORD (string): Database access password. DATABASE_NAME (string): Name of the database you want to access. PORT (string): Connection port. HOST (string): Database host name. If running the code locally on a computer, the host is probably 'localhost'. If running on a server, the host may be the server name or its IP address. For information on setting environment variables, see the section Creating Environment Variables . For more information on MySQL connection configuration, see the section Connection Config Module . Once this is done, you should be able to run the code without any problems. In the Outputs Configuration section, you can find some useful parameters to determine which outputs you want to generate. [A section explaining how to download MySQL and create a user is missing] Outputs Configuration There are some parameters in the config.main_config module that may be useful for users to quickly customize the outputs of the code execution: EXPORT_QC_RECORD (bool) : With a value of True , it exports the data generated during quality control. For more details, see the section Quality Control Utils . With a value of False , quality control is still performed, but the generated data is not saved after execution (except for those printed in the log). EXPORT_FILES (bool) : With a value of True , it exports the final data (the data warehouse, with the panel structure described in ??? ) in .csv and .feather format files. With a value of False , these files are not generated. For more details, see the section File Exporting Utils Module . CREATE_SQL_DB (bool) : With a value of True , it exports the data to a MySQL database (i.e., the data warehouse is created). With a value of False , the data is not exported to MySQL. Access the config.main_config file and modify these variables according to your needs. Note: This list is not exhaustive. For more information, see the Configuration Module section. Creating Environment Variables Pending Enviroment setup In this section, we explain how to set up the virtual environment to run the source code (and generate the data warehouse). Important Notice: It is recommended to work in an active virtual environment when installing the requirements and running the code. This isolates the project and its configuration, ensuring that it does not interfere with other projects. Make sure a Python virtual environment is set up and active before continuing with the setup . If you haven't done so yet, refer to the section Set Up and Activate a Virtual Environment . Basic Requirements To run the code, you need to have the following installed: python version 3.12.2 or higher. pip version 24.0 or higher. To check if you have pip installed, open your terminal and run the command: pip --version > pip 24.0 from C:**\\enviroment\\Lib\\site-packages\\pip (python 3.12) To check your version of python , open your terminal and run the command: python --version > Python 3.12.2 If you do not have a compatible version or are missing any of the listed requirements, you can see how to install them in the section Installation of Requirements . Note : It is possible that the code may work with earlier versions of Python, although this has not been tested. It is strongly recommended to update Python to a compatible version. Package Installation All the packages required to run the code (and their respective versions) are listed in the requirements.txt file. When running the main code ( main.py ), it checks if these requirements are met. If any are not installed or have an earlier version than required, they will be installed and/or updated to the latest version. The setup.py module handles this process. For more details, see setup_utils . However, it may be useful for the user to install the necessary packages beforehand. Run the following command to see the packages installed in the active environment: pip list The output should look something like this: Package Version ------- ------- pip 24.0 Note : This result only contains pip as the installed module because the command was run in a newly created virtual environment. To install the packages listed in requirements.txt , run the command: pip install -r requirements.txt Alternatively, you can run the setup_utils module individually. This will check if the necessary requirements are met in the same way as during the execution of main.py. To do this, run the command: python setup_utils.py If you now check the installed packages again with the pip list command, you should get something like this: Package Version ---------------------- ----------- et-xmlfile 1.1.0 feather-format 0.4.1 greenlet 3.0.3 mysql-connector-python 9.0.0 numpy 2.0.1 openpyxl 3.1.5 pandas 2.2.2 pip 24.0 pyarrow 17.0.0 pyreadstat 1.2.7 ... pytz 2024.1 six 1.16.0 SQLAlchemy 2.0.32 typing_extensions 4.12.2 tzdata 2024.1 This is a way to verify that the packages have been installed correctly. Installing requirements If you don't have python (or pip ), you can use the following resources: Installing Python : You can download Python from the official Python website . To install it correctly, follow the instructions in the Python Installation Guide . Installing pip : If you have installed Python correctly from python.org, you should already have pip . To ensure it is installed or to fix any errors, you can check the guide Installing Packages . Set Up and Activate a Virtual Environment To create and set up a Python virtual environment, we will use the venv module, which is included by default in Python starting from version 3.3. Other tools (e.g., the virtualenv module or the conda package manager) also allow you to create virtual environments, but they will not be covered in this documentation. The steps are as follows: Install Python : Ensure that you have correctly installed Python beforehand. You can verify the installation by checking the current Python version with the command python --version . Navigate to the Project : Open a terminal and navigate to the project directory where you want to create the virtual environment. Once there, run the command: python -m venv environment_name This command should create a virtual environment named environment_name . A folder with the name of the new environment should appear in the current directory. This folder should have a structure similar to the following: environment_name/ \u251c\u2500\u2500 Include/ \u251c\u2500\u2500 Lib/ \u251c\u2500\u2500 Scripts/ \u2514\u2500\u2500 pyvenv.cfg Activate the Virtual Environment : Once the virtual environment is created, and each time you want to work from it, you will need to activate it. Depending on the operating system you are using, you will need to use a different command. Make sure you are in the directory where the environment you want to activate is located and run one of the following commands: On Windows : From the terminal, run the command: enviroment_name\\Scripts\\activate On Linux : From the terminal, run the command: source enviroment_name/bin/activate On macOS : From the terminal, run the command: source enviroment_name/bin/activate If the virtual environment is set up correctly, you should see the environment name in parentheses before the command line in your terminal. This indicates that the environment is active. Your terminal should look something like this: (environment_name) C:/Users/user_name/path/to/project Now all the packages you install will be isolated in this virtual environment and will not interfere with other projects. Additionally, if necessary, you can configure which version of Python you want to use in each virtual environment. To deactivate the virtual environment and return to the global environment or system environment , use the deactivate command in the terminal. You should see the environment name in parentheses disappear from the terminal. Cloning GitHub repository The GitHub repository that contains the code and configuration files to create the Data Warehouse is private. To clone it, your GitHub account must have access permissions to the repository. To clone the repository, open your terminal and navigate to the directory where you want to clone the repository. You can clone it using either an HTTPS or SSH connection: HTTPS : run the following command in the terminal: git clone https://github.com/ecaromolina/LiverAim_WP1.git SSH : run the following command in the terminal: git clone git@github.com:ecaromolina/LiverAim_WP1.git After cloning, you can navigate to the newly created directory named after the repository. For example: cd LiverAim_WP1 These commands will only copy the branch set as the 'Default Branch'. In general, this will correspond to the code of the latest version of the database. To check all the available branches in the remote repository, you can run the following command: git branch -r To switch to a specific branch after cloning, use: git checkout branch-name","title":"Quick Start Guide"},{"location":"quick_start_guide/#fast-configuration","text":"This section describes the necessary configuration to run the code and generate the data warehouse (DW). For a comprehensive description of all configuration parameters, refer to the section Configuration Module . Important Notice : Before configuring the project, ensure that the prerequisites described in the Environment Setup section are met. Note : If you have downloaded the repository from GitHub, the structure of the entire repository should be correct. In this case, and if you want to run the code with the default configuration, only the following modifications would be needed: Configure the /data/cohort_name/databases directory for each cohort. See the section Structure of the data/cohort_name/databases/ Directory . Configure the MySQL DB connection parameters. See the section MySQL Connection Configuration .","title":"Fast Configuration"},{"location":"quick_start_guide/#structure-of-the-project-directory","text":"While the configuration allows some flexibility in the project directory structure, it is recommended to maintain the current structure to avoid errors. The project directory currently (as of 08/07/2024) has the following structure: DATA_WAREHOUSE/ \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 biomarkers_data \u2502 \u2502 \u251c\u2500\u2500 nordic_biomarkers_data \u2502 \u2502 \u2514\u2500\u2500 hclinic_biomarkers_data \u2502 \u251c\u2500\u2500 WP1 \u2502 \u2502 \u251c\u2500\u2500 Liverscreen \u2502 \u2502 \u251c\u2500\u2500 Alcofib \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u251c\u2500\u2500 WP2 \u2502 \u2502 \u2514\u2500\u2500 Galaald \u2502 \u2514\u2500\u2500 numeric_bounds.json \u2514\u2500\u2500 scr \u251c\u2500\u2500 config/ \u2502 \u251c\u2500\u2500 cohort_config.py \u2502 \u251c\u2500\u2500 connection_config.py \u2502 \u251c\u2500\u2500 main_config.py \u2502 \u251c\u2500\u2500 new_levels_config.py \u2502 \u251c\u2500\u2500 new_var_config.py \u2502 \u2514\u2500\u2500 reference_names.py \u251c\u2500\u2500 logs/ \u2502 ... \u251c\u2500\u2500 qc_report/ \u2502 ... \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 main.py ... other python modules","title":"Structure of the Project Directory"},{"location":"quick_start_guide/#structure-of-datacohort_name-directory","text":"Each subdirectory within data/ contains the databases and files related to each cohort or the biomarker data. These subdirectories should have the following structure: data/ \u251c\u2500\u2500 WP1 \u251c\u2500\u2500 <cohort_name>/ \u2502 \u251c\u2500\u2500 databases/ \u2502 \u2502 \u251c\u2500\u2500 <cohort_name_database_version_date>.extension \u2502 \u2502 ... \u2502 \u251c\u2500\u2500 <cohort_name>_level_data.xlsx \u2502 \u2514\u2500\u2500 <cohort_name>_var_data.xlsx \u2514\u2500\u2500 panel_data.xlsx \u251c\u2500\u2500 WP2 \u251c\u2500\u2500 ... ... \u2514\u2500\u2500 biomarkers_data \u251c\u2500\u2500 nordic_biomarkers_data \u251c\u2500\u2500 <name_database_version_date>.extension ... \u2514\u2500\u2500 hclinic_biomarkers_data \u251c\u2500\u2500 <name_database_version_date>.extension ... ... The cohort_name_level_data.xlsx and cohort_name_var_data.xlsx files are described in the section Initial Data and Configuration data . If you downloaded the repository from GitHub, you should not need to modify these files.","title":"Structure of data/cohort_name/ Directory"},{"location":"quick_start_guide/#structure-of-datacohort_namedatabases-directory","text":"The /data/<cohort_name>/databases directory should store the files containing the raw data for the databases of each cohort. Each file in the directory corresponds to one of the received database versions. For more information on how different versions are handled, see the section Managing Raw Data Versions . It is very important (for this particular case) that the file names follow this structure: <cohort_name>_database_<version_date>.extension For example, the raw data file for the Liverscreen cohort received on 29/07/2024 in .dta format would be named: liverscreen_database_20240729.dta Note that the name is in lowercase and the date is in the yyyymmdd format. Note : This folder may contain only one file. It is recommended that this corresponds to the latest version of the data. Similarly, the directory /data/biomarkers_data/<provider>_biomarkers_data should store the files containing the raw data from the biomarkers transfered by this particular provider. This folder may contain different versions or batches of data. Those different files are, again, read and merged during the execution of the database. For further information on the merging of this data, see section Biomerkers data Management . Biomarker data files must also follow the structure: <provider>_biomarkers_<batch_date>.extension For example, data tranfered from Nordic recieved on 29/07/2024 in .csv format would be named: nordic_biomarkers_20240729.csv Since these files are not uploaded to GitHub, it is necessary to manually place them in the correct directory and format. Once this is done, the directory should be correctly configured.","title":"Structure of data/cohort_name/databases/ Directory"},{"location":"quick_start_guide/#mysql-connection-configuration","text":"If you want to export the data warehouse to the MySQL DB (presumably yes), the MySQL connection parameters must be configured. Therefore, it is necessary to have MySQL installed (if you want to generate the DB locally) and to have a username and password. [A section explaining how to download MySQL and create a user is missing]. Once these requirements are met, you need to configure the MySQL DB connection parameters. These are defined in the config.connection_config module, but for a quick configuration, you do not need to modify this file. Instead, you should create the following environment variables in your execution environment: USER (string): Username (must have permission to access the database). PASSWORD (string): Database access password. DATABASE_NAME (string): Name of the database you want to access. PORT (string): Connection port. HOST (string): Database host name. If running the code locally on a computer, the host is probably 'localhost'. If running on a server, the host may be the server name or its IP address. For information on setting environment variables, see the section Creating Environment Variables . For more information on MySQL connection configuration, see the section Connection Config Module . Once this is done, you should be able to run the code without any problems. In the Outputs Configuration section, you can find some useful parameters to determine which outputs you want to generate. [A section explaining how to download MySQL and create a user is missing]","title":"MySQL Connection Configuration"},{"location":"quick_start_guide/#outputs-configuration","text":"There are some parameters in the config.main_config module that may be useful for users to quickly customize the outputs of the code execution: EXPORT_QC_RECORD (bool) : With a value of True , it exports the data generated during quality control. For more details, see the section Quality Control Utils . With a value of False , quality control is still performed, but the generated data is not saved after execution (except for those printed in the log). EXPORT_FILES (bool) : With a value of True , it exports the final data (the data warehouse, with the panel structure described in ??? ) in .csv and .feather format files. With a value of False , these files are not generated. For more details, see the section File Exporting Utils Module . CREATE_SQL_DB (bool) : With a value of True , it exports the data to a MySQL database (i.e., the data warehouse is created). With a value of False , the data is not exported to MySQL. Access the config.main_config file and modify these variables according to your needs. Note: This list is not exhaustive. For more information, see the Configuration Module section.","title":"Outputs Configuration"},{"location":"quick_start_guide/#creating-environment-variables","text":"Pending","title":"Creating Environment Variables"},{"location":"quick_start_guide/#enviroment-setup","text":"In this section, we explain how to set up the virtual environment to run the source code (and generate the data warehouse). Important Notice: It is recommended to work in an active virtual environment when installing the requirements and running the code. This isolates the project and its configuration, ensuring that it does not interfere with other projects. Make sure a Python virtual environment is set up and active before continuing with the setup . If you haven't done so yet, refer to the section Set Up and Activate a Virtual Environment .","title":"Enviroment setup"},{"location":"quick_start_guide/#basic-requirements","text":"To run the code, you need to have the following installed: python version 3.12.2 or higher. pip version 24.0 or higher. To check if you have pip installed, open your terminal and run the command: pip --version > pip 24.0 from C:**\\enviroment\\Lib\\site-packages\\pip (python 3.12) To check your version of python , open your terminal and run the command: python --version > Python 3.12.2 If you do not have a compatible version or are missing any of the listed requirements, you can see how to install them in the section Installation of Requirements . Note : It is possible that the code may work with earlier versions of Python, although this has not been tested. It is strongly recommended to update Python to a compatible version.","title":"Basic Requirements"},{"location":"quick_start_guide/#package-installation","text":"All the packages required to run the code (and their respective versions) are listed in the requirements.txt file. When running the main code ( main.py ), it checks if these requirements are met. If any are not installed or have an earlier version than required, they will be installed and/or updated to the latest version. The setup.py module handles this process. For more details, see setup_utils . However, it may be useful for the user to install the necessary packages beforehand. Run the following command to see the packages installed in the active environment: pip list The output should look something like this: Package Version ------- ------- pip 24.0 Note : This result only contains pip as the installed module because the command was run in a newly created virtual environment. To install the packages listed in requirements.txt , run the command: pip install -r requirements.txt Alternatively, you can run the setup_utils module individually. This will check if the necessary requirements are met in the same way as during the execution of main.py. To do this, run the command: python setup_utils.py If you now check the installed packages again with the pip list command, you should get something like this: Package Version ---------------------- ----------- et-xmlfile 1.1.0 feather-format 0.4.1 greenlet 3.0.3 mysql-connector-python 9.0.0 numpy 2.0.1 openpyxl 3.1.5 pandas 2.2.2 pip 24.0 pyarrow 17.0.0 pyreadstat 1.2.7 ... pytz 2024.1 six 1.16.0 SQLAlchemy 2.0.32 typing_extensions 4.12.2 tzdata 2024.1 This is a way to verify that the packages have been installed correctly.","title":"Package Installation"},{"location":"quick_start_guide/#installing-requirements","text":"If you don't have python (or pip ), you can use the following resources: Installing Python : You can download Python from the official Python website . To install it correctly, follow the instructions in the Python Installation Guide . Installing pip : If you have installed Python correctly from python.org, you should already have pip . To ensure it is installed or to fix any errors, you can check the guide Installing Packages .","title":"Installing requirements"},{"location":"quick_start_guide/#set-up-and-activate-a-virtual-environment","text":"To create and set up a Python virtual environment, we will use the venv module, which is included by default in Python starting from version 3.3. Other tools (e.g., the virtualenv module or the conda package manager) also allow you to create virtual environments, but they will not be covered in this documentation. The steps are as follows: Install Python : Ensure that you have correctly installed Python beforehand. You can verify the installation by checking the current Python version with the command python --version . Navigate to the Project : Open a terminal and navigate to the project directory where you want to create the virtual environment. Once there, run the command: python -m venv environment_name This command should create a virtual environment named environment_name . A folder with the name of the new environment should appear in the current directory. This folder should have a structure similar to the following: environment_name/ \u251c\u2500\u2500 Include/ \u251c\u2500\u2500 Lib/ \u251c\u2500\u2500 Scripts/ \u2514\u2500\u2500 pyvenv.cfg Activate the Virtual Environment : Once the virtual environment is created, and each time you want to work from it, you will need to activate it. Depending on the operating system you are using, you will need to use a different command. Make sure you are in the directory where the environment you want to activate is located and run one of the following commands: On Windows : From the terminal, run the command: enviroment_name\\Scripts\\activate On Linux : From the terminal, run the command: source enviroment_name/bin/activate On macOS : From the terminal, run the command: source enviroment_name/bin/activate If the virtual environment is set up correctly, you should see the environment name in parentheses before the command line in your terminal. This indicates that the environment is active. Your terminal should look something like this: (environment_name) C:/Users/user_name/path/to/project Now all the packages you install will be isolated in this virtual environment and will not interfere with other projects. Additionally, if necessary, you can configure which version of Python you want to use in each virtual environment. To deactivate the virtual environment and return to the global environment or system environment , use the deactivate command in the terminal. You should see the environment name in parentheses disappear from the terminal.","title":"Set Up and Activate a Virtual Environment"},{"location":"quick_start_guide/#cloning-github-repository","text":"The GitHub repository that contains the code and configuration files to create the Data Warehouse is private. To clone it, your GitHub account must have access permissions to the repository. To clone the repository, open your terminal and navigate to the directory where you want to clone the repository. You can clone it using either an HTTPS or SSH connection: HTTPS : run the following command in the terminal: git clone https://github.com/ecaromolina/LiverAim_WP1.git SSH : run the following command in the terminal: git clone git@github.com:ecaromolina/LiverAim_WP1.git After cloning, you can navigate to the newly created directory named after the repository. For example: cd LiverAim_WP1 These commands will only copy the branch set as the 'Default Branch'. In general, this will correspond to the code of the latest version of the database. To check all the available branches in the remote repository, you can run the following command: git branch -r To switch to a specific branch after cloning, use: git checkout branch-name","title":"Cloning GitHub repository"},{"location":"modules_documentation/biomarker_utils_doc/","text":"biomarker_utils Documentation General Sturcture This module defines the following classes: BMKDataReader : Responsible for centralizing the process of reading biomarker data. BMKDataProcessor : Handles the processing, formatting, and validation of biomarker data. The resulting data is ready for export in .csv or feather format as well as to a MySQL database . Functions and Classes In this sections are described the classes and functions defined in the module: Functions test_BMKDataReader() -> None If executed, it creates an instance of BMKDataReader and reads the biomarker data using the parameters set in biomarkers_config . If an error occurs, it prints the error. Otherwise, it prints the names of the files read and their hash. Logs If an error occurs during the instantiation of the BMKDataReader class. If an error occurs while reading any of the files listed in the bmk_data_directory attribute of the BMKDataReader instance. If no errors occur, the files read and their hash are printed. Classes Class BMKDataReader Description Reads all the data related to the biomarkers. It is responsible for reading and and organizing the files in a data dictionary, by version. This class is file-name dependant, so the files in the folder bmk-data_directory must meet the nomenclature (see read_bmk_data). Attributes bmk_data_directory (str) : directory where de bmk raw data is stored. Variable exported from the bmk configuration file. read_files (str): list of the files that has been read (the file name and its hash). initialized as None. bmk_data_dict (str): dict whit all the read bmk data. Keys are each bmk version (date it was recieved) and values are the data read in DataFrame format. Initialized as None. Methods read_bmk_data(self) -> 'BMKDataReader': Reads all files in the folder self.bmk_data_directory . The correct operation of this method depends on the proper naming of the files in the folder. BMK data files must follow the naming convention: bmk_database_<version_date> , where <version_date> is formatted as yyyymmdd . An example of a valid filename would be: bmk_database_20241011 . The method stores the read data in a dictionary, where keys are the different versions and values are DataFrames containing the BMK data. Returns : BMKDataReader : The updated instance of BMKDataReader with the attribute bmk_data_dict filled with the BMK data. Class BMKDataProcessor Processes the biomarker data to meet the proper format and merges all the versions (data batches). It also includes a QC checks and manages the exportation of the QC results. Attributes cohort_ids_dict (dict) : dictionary to map the cohort ids to the common liveraim ids. cohort_ids (set) : set of original ids of all patients in liveraim database. bmk_qc_dir (str) : directory where qc reports will be dumped. raw_bmk_data (pd.DataFrame) : formatted_bmk_data (pd.DataFrame) : df containing all bmk data ready to export. unmatching_ids (set) : ids in bmk data that do not match with any id in liveraim db. inconsistancies (pd.Series) : bmk_summary_dict (dict[str, pd.DataFrame]) : dict of dataframes with the qc reports of the bmk data. Methods check_bmk_structure(self, df: pd.DataFrame) -> None: Checks if the DataFrame provided as a parameter contains the columns defined in the biomarkers_config module. It also verifies if there are any duplicates for the combination of ID and variable (biomarker/Prestacion). Arguments: df (pd.DataFrame): DataFrame to be checked. Logs : Logs an error if the columns in biomarkers databases do not match the ones described in biomarkers_config . Logs an error if finds any duplicates of the pair ID(Paciente) - Variable(Prestacion) process(self) -> 'BMKDataProcessor': Centralizes the formatting of bmk_data . For each version (data batch), applies the following transformations: process_symbol : Extracts symbols for \"out-of-range\" values and creates two additional columns (see process_symbol method). column_typing : Assigns the appropriate data type to each column. Generates a summary of the processing results for quality control (QC). Renames the columns in biomarkers data. Add the liveraim_id column, the common identifier, to the patients that appear in the biomarker data and in the Liveraim database. After processing, the method merges all DataFrames and checks for any inconsistencies in the data. Finally, it assigns the merged DataFrame to the attribute formatted_bmk_data and stores the QC data in bmk_summary_dict . Returns : BMKDataProcessor : The updated instance of BMKDataProcessor with formatted_bmk_data and bmk_summary_dict attributes populated. log_bmk_summary(self, df: pd.DataFrame) -> None: Logs a brief summary of the current biomarkers data. Arguments : df ( pd.DataFrame ): The DataFrame containing biomarker data to be summarized. Logs : Logs the count of missing numeric values in the biomarkers data. Logs the count of \"off limits\" values found in biomarkers data, specifically values marked with symbols < or > . Logs the total number of unique patients and unique biomarker labels in the data. extract_symbol(self, value: str) -> float: Extracts the symbols '<' or '>' from a string (if present) and returns the numeric portion as a float. If the input contains \"No calculable,\" it returns a NaN value. This method is intended to be used within a DataFrame's apply function (see process_symbol ). Arguments : value (str): A string containing a numeric value and, in some cases, the symbol '<' or '>'. Returns : float : The numeric portion of the input value, or NaN if \"No calculable\" is found. process_symbol(self, df: pd.DataFrame, regex: str = r'([<>])') -> None: Processes the symbols '<' or '>' in a specified column of the DataFrame and extracts the numeric values. This method creates two additional columns: one for storing the detected symbols (if any) and another for the numeric portion of the values. It has the following effects: Adds a new column to df named bmk_config.LIMIT_DETECT_COLUMN , containing any extracted '<' or '>' symbols. Adds another column named bmk_config.NUMERIC_VALUE_COLUMN with the numeric portion of each value, processed by the extract_symbol method to handle strings with symbols. Arguments : df (pd.DataFrame): The DataFrame containing the data to process. This DataFrame should include the column defined by bmk_config.VALUE_COLUMN , which may contain numerical values with '<' or '>' symbols. regex (str, optional): The regular expression used to identify and extract symbols from the VALUE_COLUMN . Defaults to r'([<>])' , which targets the symbols '<' and '>'. column_typing(self, df: pd.DataFrame) -> None: Assigns the appropriate data type to each column in the provided DataFrame based on the types specified in bmk_config.python_column_types . If a column's type is not found in this configuration, a warning is logged, and the column remains unmodified. Modifies df in place by changing the data types of its columns. Argumentss : df (pd.DataFrame): The DataFrame containing the data to be typed. Each column's data type will be set according to bmk_config.python_column_types . Logs : Logs a warning if a column is not found in python_column_types . Logs an error if a column cannot be converted to the specified data type, detailing the column and error message. column_typing(self, df: pd.DataFrame) -> None: Maps and assigns liveraim_id values to the DataFrame based on a provided ID mapping dictionary. If no mapping is provided, it defaults to self.cohort_ids_dict . Arguments : df (pd.DataFrame): The DataFrame containing the data to which liveraim_id values will be assigned. id_map (dict, optional): A dictionary mapping original IDs (in bmk_config.ID_COLUMN ) to liveraim_id values. If not provided, self.cohort_ids_dict is used. Returns : pd.DataFrame : The updated DataFrame with a new column, bmk_config.LIVERAIM_ID_COLUMN , containing the mapped liveraim_id values. check_ids_coherence(self, df: pd.DataFrame, cohort_ids: set) -> pd.DataFrame: Checks for coherence between IDs in the provided DataFrame and a set of cohort IDs. Identifies and removes any unmatched IDs from the DataFrame to prevent potential errors during exportation to MySQL database. The effects are: Sets self.unmatching_ids with the unmatched IDs found. Modifies df by removing rows with unmatched IDs. Arguments : df (pd.DataFrame): The DataFrame containing the data with IDs to be checked. cohort_ids (set): A set of valid cohort IDs for comparison against the IDs in the DataFrame column defined by bmk_config.ID_COLUMN . Returns : pd.DataFrame : The updated DataFrame with unmatched IDs removed. Logs : - Logs a warning if unmatched IDs are found, detailing the count and indicating that these IDs will be removed. check_inconsistancies(self, df: pd.DataFrame) -> pd.DataFrame: Checks for inconsistencies in the provided DataFrame, including duplicate records and value mismatches across duplicates, ensuring data compatibility for export to MySQL. This method identifies and handles: - Duplicate records based on key columns, retaining only the most recent record (based on validation date). - Inconsistent values across duplicate entries, logging any discrepancies found. Arguments : df (pd.DataFrame): The DataFrame containing biomarker data to be checked for both duplicate records and value inconsistencies. Returns : pd.DataFrame : The DataFrame with duplicate records removed, where applicable. Logs : Logs the number of duplicates and any value inconsistencies found across duplicate entries. export_qc_report(self) -> None: Exports the quality control (QC) report to an Excel file if the configuration allows it. The report includes data summaries, such as version summaries and biomarker-specific information, saved in an Excel workbook with separate sheets. This method uses the config.EXPORT_QC_RECORD setting to determine if the export should proceed. If enabled, the report is saved in a directory defined by config.QC_RECORD_EXPORT_FOLDER , organized by execution time. Creates a directory at the specified export path if it does not already exist and exports the QC report to an Excel file at the location {QC_RECORD_EXPORT_FOLDER}/qc_execution_<execution_time>/bmk_inconsistancies.xlsx . bmk_summary(self, df: pd.DataFrame, cohort_ids: set) -> dict: Generates a summary of key metrics for the given biomarkers DataFrame, including the number of unique patients and biomarkers, counts of missing values, duplicate records, unmatched IDs, and values marked as out-of-range. Arguments : df (pd.DataFrame): The DataFrame containing biomarker data to summarize. cohort_ids (set): A set of cohort IDs to compare against the IDs in the DataFrame, used to count unmatched IDs. Returns : dict : A dictionary containing the following summary metrics: \"number_patients\": Count of unique patients in the data. \"number_biomarkers\": Count of unique biomarkers in the data. \"missings\": Total count of missing values in NUMERIC_VALUE_COLUMN . \"duplicates\": Number of duplicate entries based on patient and biomarker ID. \"unmatching_ids\": Count of IDs in ID_COLUMN that do not match any cohort IDs. \"off_limits\": Count of values marked as out-of-range (with '<' or '>'). data_by_bmk(self, df: pd.DataFrame) -> pd.DataFrame: Aggregates biomarker data by calculating the count of missing values and the number of unique patients for each biomarker in the provided DataFrame. Arguments : df (pd.DataFrame): The DataFrame containing biomarker data to be aggregated. It should include columns specified in bmk_config.NUMERIC_VALUE_COLUMN and bmk_config.ID_COLUMN . Returns : pd.DataFrame : A DataFrame with one row per biomarker, containing the following columns: bmk_config.VARIABLE_COLUMN : The biomarker identifier. \"missings\": Count of missing values for each biomarker. \"patients_count\": Number of unique patients associated with each biomarker. bmk_summary(self, df: pd.DataFrame, cohort_ids: set) -> dict: Centralizes the quality control (QC) checks for the biomarker data. This method is intended to receive a DataFrame containing the formatted biomarker data, which will be checked for inconsistencies. If inconsistencies are found, they will be resolved by removing the necessary rows to maintain data integrity. Calls various internal methods to perform specific QC checks, such as boundary checks, data type validation, ID coherence, and inconsistency resolution. Arguments : df (pd.DataFrame): The DataFrame containing formatted biomarker data to be checked. Returns : pd.DataFrame : The updated DataFrame with any identified inconsistencies resolved by removing affected rows. compute_bmk_bounds(self) -> None: Method that creates the bmk_bounds attribute from the previously formatted biomarker data. For each biomarker, it extracts the upper and lower bounds (if present in the data) and stores them in a dictionary. Once created, the dictionary is assigned to the bmk_bounds attribute. The dictionary structure is as follows: { <bmk_label>: { \"<\": <lower_bound>, \">\": <upper_bound>}, ... } Logs : If the biomarker data has not yet been processed, a warning is generated indicating that the data must be formatted before creating bmk_bounds . If multiple bounds of the same type are found for the same biomarker, a warning about possible inconsistency in the bounds is issued. check_bmk_bounds(self) -> None: Method responsible for checking the bounds of formatted biomarker data using the previously created bmk_bounds attribute. Specifically, it verifies if any values exceed or fall below the established bounds. Logs : If the bmk_bounds attribute has not yet been created, a warning is issued indicating that this attribute must be created first, then the method returns. If any inconsistencies are found, i.e., values outside the bmk_bounds limits, a message with relevant error information is logged. Class BMKCalculator General Operation This module centralizes the reading, processing, and quality control of biomarker data (unlike other modules, where these functionalities are separate). The biomarker data reading is managed by the BMKDataReader class (and not by the DataReader class). Once the class is instantiated, biomarker data can be read by calling the read_bmk_data method (it is important to verify the directory structure and file naming conventions before reading the data). Once read, the data is stored in the bmk_data_dict attribute (to ensure data is read correctly, see the section Biomarker Data Reading Checks ). To obtain processed biomarker data ready for export, the BMKDataProcessor class is required. Instantiating the class requires: bmk_data_dict (dict): A dictionary containing biomarker data. Keys represent the date of each version, while values are DataFrames containing the biomarker data. cohort_ids_dict (dict): A dictionary mapping cohort-specific identifiers (keys) to the common liveraim identifiers (values). Once instantiated, the process() method centralizes the processing, transformation, and verification of biomarker data. The merged and formatted biomarker data is stored as a pd.DataFrame in the formatted_bmk_data attribute. This DataFrame is ready to be exported to .csv , .feather , or to a MySQL database. The method export_qc_report is resposible for exporting the data to the specified file types. SQLExporter is respondible of exporting data to MySQL database. Biomarker Data Reading Checks To verify that the biomarker data reading configuration is correct, the test_BMKDataReader() function can be called. If any errors occur during reading, it will log them. If there are no errors, it will print the names of the files read (presumably all batches of biomarker data) and their respective hashes. Alternatively, you can execute the biomarker_utils module directly from the terminal to achieve the same effect: python biomarkers_utils.py This will produce the following output: Instance of BMKDataReader successfully created. Dir. path: data/biomarkers_data Data successfully read: {'file_name': 'biomarker_data_20241016.xls', 'hash': 'e79002b53b2214e72643158dc033affe9ca43b443c7b912c03179ed215ef6b6c'} Usage in main execution ... # Reading of cohorts data # Reading biomarkers data bmk_data: dict = BMKDataReader().read_bmk_data().bmk_data_dict ... # processing of cohorts data # Processing of biomarkers data bmk_processor = BMKDataProcessor(bmk_data, cohort_ids).process() bmk_processed_data = bmk_processor.formatted_bmk_data # Exportation of bmk data to csv and feather files bmk_processor.export_qc_report() ...# instantitations of SQLExporter # Exportation to MySQL DB of bmk data sql_exporter.export_df_to_sql(bmk_processed_data, \"biomarkers\")","title":"Biomkarker utils"},{"location":"modules_documentation/biomarker_utils_doc/#biomarker_utils-documentation","text":"","title":"biomarker_utils Documentation"},{"location":"modules_documentation/biomarker_utils_doc/#general-sturcture","text":"This module defines the following classes: BMKDataReader : Responsible for centralizing the process of reading biomarker data. BMKDataProcessor : Handles the processing, formatting, and validation of biomarker data. The resulting data is ready for export in .csv or feather format as well as to a MySQL database .","title":"General Sturcture"},{"location":"modules_documentation/biomarker_utils_doc/#functions-and-classes","text":"In this sections are described the classes and functions defined in the module:","title":"Functions and Classes"},{"location":"modules_documentation/biomarker_utils_doc/#functions","text":"test_BMKDataReader() -> None If executed, it creates an instance of BMKDataReader and reads the biomarker data using the parameters set in biomarkers_config . If an error occurs, it prints the error. Otherwise, it prints the names of the files read and their hash. Logs If an error occurs during the instantiation of the BMKDataReader class. If an error occurs while reading any of the files listed in the bmk_data_directory attribute of the BMKDataReader instance. If no errors occur, the files read and their hash are printed.","title":"Functions"},{"location":"modules_documentation/biomarker_utils_doc/#classes","text":"","title":"Classes"},{"location":"modules_documentation/biomarker_utils_doc/#class-bmkdatareader","text":"","title":"Class BMKDataReader"},{"location":"modules_documentation/biomarker_utils_doc/#description","text":"Reads all the data related to the biomarkers. It is responsible for reading and and organizing the files in a data dictionary, by version. This class is file-name dependant, so the files in the folder bmk-data_directory must meet the nomenclature (see read_bmk_data).","title":"Description"},{"location":"modules_documentation/biomarker_utils_doc/#attributes","text":"bmk_data_directory (str) : directory where de bmk raw data is stored. Variable exported from the bmk configuration file. read_files (str): list of the files that has been read (the file name and its hash). initialized as None. bmk_data_dict (str): dict whit all the read bmk data. Keys are each bmk version (date it was recieved) and values are the data read in DataFrame format. Initialized as None.","title":"Attributes"},{"location":"modules_documentation/biomarker_utils_doc/#methods","text":"read_bmk_data(self) -> 'BMKDataReader': Reads all files in the folder self.bmk_data_directory . The correct operation of this method depends on the proper naming of the files in the folder. BMK data files must follow the naming convention: bmk_database_<version_date> , where <version_date> is formatted as yyyymmdd . An example of a valid filename would be: bmk_database_20241011 . The method stores the read data in a dictionary, where keys are the different versions and values are DataFrames containing the BMK data. Returns : BMKDataReader : The updated instance of BMKDataReader with the attribute bmk_data_dict filled with the BMK data.","title":"Methods"},{"location":"modules_documentation/biomarker_utils_doc/#class-bmkdataprocessor","text":"Processes the biomarker data to meet the proper format and merges all the versions (data batches). It also includes a QC checks and manages the exportation of the QC results.","title":"Class BMKDataProcessor"},{"location":"modules_documentation/biomarker_utils_doc/#attributes_1","text":"cohort_ids_dict (dict) : dictionary to map the cohort ids to the common liveraim ids. cohort_ids (set) : set of original ids of all patients in liveraim database. bmk_qc_dir (str) : directory where qc reports will be dumped. raw_bmk_data (pd.DataFrame) : formatted_bmk_data (pd.DataFrame) : df containing all bmk data ready to export. unmatching_ids (set) : ids in bmk data that do not match with any id in liveraim db. inconsistancies (pd.Series) : bmk_summary_dict (dict[str, pd.DataFrame]) : dict of dataframes with the qc reports of the bmk data.","title":"Attributes"},{"location":"modules_documentation/biomarker_utils_doc/#methods_1","text":"check_bmk_structure(self, df: pd.DataFrame) -> None: Checks if the DataFrame provided as a parameter contains the columns defined in the biomarkers_config module. It also verifies if there are any duplicates for the combination of ID and variable (biomarker/Prestacion). Arguments: df (pd.DataFrame): DataFrame to be checked. Logs : Logs an error if the columns in biomarkers databases do not match the ones described in biomarkers_config . Logs an error if finds any duplicates of the pair ID(Paciente) - Variable(Prestacion) process(self) -> 'BMKDataProcessor': Centralizes the formatting of bmk_data . For each version (data batch), applies the following transformations: process_symbol : Extracts symbols for \"out-of-range\" values and creates two additional columns (see process_symbol method). column_typing : Assigns the appropriate data type to each column. Generates a summary of the processing results for quality control (QC). Renames the columns in biomarkers data. Add the liveraim_id column, the common identifier, to the patients that appear in the biomarker data and in the Liveraim database. After processing, the method merges all DataFrames and checks for any inconsistencies in the data. Finally, it assigns the merged DataFrame to the attribute formatted_bmk_data and stores the QC data in bmk_summary_dict . Returns : BMKDataProcessor : The updated instance of BMKDataProcessor with formatted_bmk_data and bmk_summary_dict attributes populated. log_bmk_summary(self, df: pd.DataFrame) -> None: Logs a brief summary of the current biomarkers data. Arguments : df ( pd.DataFrame ): The DataFrame containing biomarker data to be summarized. Logs : Logs the count of missing numeric values in the biomarkers data. Logs the count of \"off limits\" values found in biomarkers data, specifically values marked with symbols < or > . Logs the total number of unique patients and unique biomarker labels in the data. extract_symbol(self, value: str) -> float: Extracts the symbols '<' or '>' from a string (if present) and returns the numeric portion as a float. If the input contains \"No calculable,\" it returns a NaN value. This method is intended to be used within a DataFrame's apply function (see process_symbol ). Arguments : value (str): A string containing a numeric value and, in some cases, the symbol '<' or '>'. Returns : float : The numeric portion of the input value, or NaN if \"No calculable\" is found. process_symbol(self, df: pd.DataFrame, regex: str = r'([<>])') -> None: Processes the symbols '<' or '>' in a specified column of the DataFrame and extracts the numeric values. This method creates two additional columns: one for storing the detected symbols (if any) and another for the numeric portion of the values. It has the following effects: Adds a new column to df named bmk_config.LIMIT_DETECT_COLUMN , containing any extracted '<' or '>' symbols. Adds another column named bmk_config.NUMERIC_VALUE_COLUMN with the numeric portion of each value, processed by the extract_symbol method to handle strings with symbols. Arguments : df (pd.DataFrame): The DataFrame containing the data to process. This DataFrame should include the column defined by bmk_config.VALUE_COLUMN , which may contain numerical values with '<' or '>' symbols. regex (str, optional): The regular expression used to identify and extract symbols from the VALUE_COLUMN . Defaults to r'([<>])' , which targets the symbols '<' and '>'. column_typing(self, df: pd.DataFrame) -> None: Assigns the appropriate data type to each column in the provided DataFrame based on the types specified in bmk_config.python_column_types . If a column's type is not found in this configuration, a warning is logged, and the column remains unmodified. Modifies df in place by changing the data types of its columns. Argumentss : df (pd.DataFrame): The DataFrame containing the data to be typed. Each column's data type will be set according to bmk_config.python_column_types . Logs : Logs a warning if a column is not found in python_column_types . Logs an error if a column cannot be converted to the specified data type, detailing the column and error message. column_typing(self, df: pd.DataFrame) -> None: Maps and assigns liveraim_id values to the DataFrame based on a provided ID mapping dictionary. If no mapping is provided, it defaults to self.cohort_ids_dict . Arguments : df (pd.DataFrame): The DataFrame containing the data to which liveraim_id values will be assigned. id_map (dict, optional): A dictionary mapping original IDs (in bmk_config.ID_COLUMN ) to liveraim_id values. If not provided, self.cohort_ids_dict is used. Returns : pd.DataFrame : The updated DataFrame with a new column, bmk_config.LIVERAIM_ID_COLUMN , containing the mapped liveraim_id values. check_ids_coherence(self, df: pd.DataFrame, cohort_ids: set) -> pd.DataFrame: Checks for coherence between IDs in the provided DataFrame and a set of cohort IDs. Identifies and removes any unmatched IDs from the DataFrame to prevent potential errors during exportation to MySQL database. The effects are: Sets self.unmatching_ids with the unmatched IDs found. Modifies df by removing rows with unmatched IDs. Arguments : df (pd.DataFrame): The DataFrame containing the data with IDs to be checked. cohort_ids (set): A set of valid cohort IDs for comparison against the IDs in the DataFrame column defined by bmk_config.ID_COLUMN . Returns : pd.DataFrame : The updated DataFrame with unmatched IDs removed. Logs : - Logs a warning if unmatched IDs are found, detailing the count and indicating that these IDs will be removed. check_inconsistancies(self, df: pd.DataFrame) -> pd.DataFrame: Checks for inconsistencies in the provided DataFrame, including duplicate records and value mismatches across duplicates, ensuring data compatibility for export to MySQL. This method identifies and handles: - Duplicate records based on key columns, retaining only the most recent record (based on validation date). - Inconsistent values across duplicate entries, logging any discrepancies found. Arguments : df (pd.DataFrame): The DataFrame containing biomarker data to be checked for both duplicate records and value inconsistencies. Returns : pd.DataFrame : The DataFrame with duplicate records removed, where applicable. Logs : Logs the number of duplicates and any value inconsistencies found across duplicate entries. export_qc_report(self) -> None: Exports the quality control (QC) report to an Excel file if the configuration allows it. The report includes data summaries, such as version summaries and biomarker-specific information, saved in an Excel workbook with separate sheets. This method uses the config.EXPORT_QC_RECORD setting to determine if the export should proceed. If enabled, the report is saved in a directory defined by config.QC_RECORD_EXPORT_FOLDER , organized by execution time. Creates a directory at the specified export path if it does not already exist and exports the QC report to an Excel file at the location {QC_RECORD_EXPORT_FOLDER}/qc_execution_<execution_time>/bmk_inconsistancies.xlsx . bmk_summary(self, df: pd.DataFrame, cohort_ids: set) -> dict: Generates a summary of key metrics for the given biomarkers DataFrame, including the number of unique patients and biomarkers, counts of missing values, duplicate records, unmatched IDs, and values marked as out-of-range. Arguments : df (pd.DataFrame): The DataFrame containing biomarker data to summarize. cohort_ids (set): A set of cohort IDs to compare against the IDs in the DataFrame, used to count unmatched IDs. Returns : dict : A dictionary containing the following summary metrics: \"number_patients\": Count of unique patients in the data. \"number_biomarkers\": Count of unique biomarkers in the data. \"missings\": Total count of missing values in NUMERIC_VALUE_COLUMN . \"duplicates\": Number of duplicate entries based on patient and biomarker ID. \"unmatching_ids\": Count of IDs in ID_COLUMN that do not match any cohort IDs. \"off_limits\": Count of values marked as out-of-range (with '<' or '>'). data_by_bmk(self, df: pd.DataFrame) -> pd.DataFrame: Aggregates biomarker data by calculating the count of missing values and the number of unique patients for each biomarker in the provided DataFrame. Arguments : df (pd.DataFrame): The DataFrame containing biomarker data to be aggregated. It should include columns specified in bmk_config.NUMERIC_VALUE_COLUMN and bmk_config.ID_COLUMN . Returns : pd.DataFrame : A DataFrame with one row per biomarker, containing the following columns: bmk_config.VARIABLE_COLUMN : The biomarker identifier. \"missings\": Count of missing values for each biomarker. \"patients_count\": Number of unique patients associated with each biomarker. bmk_summary(self, df: pd.DataFrame, cohort_ids: set) -> dict: Centralizes the quality control (QC) checks for the biomarker data. This method is intended to receive a DataFrame containing the formatted biomarker data, which will be checked for inconsistencies. If inconsistencies are found, they will be resolved by removing the necessary rows to maintain data integrity. Calls various internal methods to perform specific QC checks, such as boundary checks, data type validation, ID coherence, and inconsistency resolution. Arguments : df (pd.DataFrame): The DataFrame containing formatted biomarker data to be checked. Returns : pd.DataFrame : The updated DataFrame with any identified inconsistencies resolved by removing affected rows. compute_bmk_bounds(self) -> None: Method that creates the bmk_bounds attribute from the previously formatted biomarker data. For each biomarker, it extracts the upper and lower bounds (if present in the data) and stores them in a dictionary. Once created, the dictionary is assigned to the bmk_bounds attribute. The dictionary structure is as follows: { <bmk_label>: { \"<\": <lower_bound>, \">\": <upper_bound>}, ... } Logs : If the biomarker data has not yet been processed, a warning is generated indicating that the data must be formatted before creating bmk_bounds . If multiple bounds of the same type are found for the same biomarker, a warning about possible inconsistency in the bounds is issued. check_bmk_bounds(self) -> None: Method responsible for checking the bounds of formatted biomarker data using the previously created bmk_bounds attribute. Specifically, it verifies if any values exceed or fall below the established bounds. Logs : If the bmk_bounds attribute has not yet been created, a warning is issued indicating that this attribute must be created first, then the method returns. If any inconsistencies are found, i.e., values outside the bmk_bounds limits, a message with relevant error information is logged.","title":"Methods"},{"location":"modules_documentation/biomarker_utils_doc/#class-bmkcalculator","text":"","title":"Class BMKCalculator"},{"location":"modules_documentation/biomarker_utils_doc/#general-operation","text":"This module centralizes the reading, processing, and quality control of biomarker data (unlike other modules, where these functionalities are separate). The biomarker data reading is managed by the BMKDataReader class (and not by the DataReader class). Once the class is instantiated, biomarker data can be read by calling the read_bmk_data method (it is important to verify the directory structure and file naming conventions before reading the data). Once read, the data is stored in the bmk_data_dict attribute (to ensure data is read correctly, see the section Biomarker Data Reading Checks ). To obtain processed biomarker data ready for export, the BMKDataProcessor class is required. Instantiating the class requires: bmk_data_dict (dict): A dictionary containing biomarker data. Keys represent the date of each version, while values are DataFrames containing the biomarker data. cohort_ids_dict (dict): A dictionary mapping cohort-specific identifiers (keys) to the common liveraim identifiers (values). Once instantiated, the process() method centralizes the processing, transformation, and verification of biomarker data. The merged and formatted biomarker data is stored as a pd.DataFrame in the formatted_bmk_data attribute. This DataFrame is ready to be exported to .csv , .feather , or to a MySQL database. The method export_qc_report is resposible for exporting the data to the specified file types. SQLExporter is respondible of exporting data to MySQL database.","title":"General Operation"},{"location":"modules_documentation/biomarker_utils_doc/#biomarker-data-reading-checks","text":"To verify that the biomarker data reading configuration is correct, the test_BMKDataReader() function can be called. If any errors occur during reading, it will log them. If there are no errors, it will print the names of the files read (presumably all batches of biomarker data) and their respective hashes. Alternatively, you can execute the biomarker_utils module directly from the terminal to achieve the same effect: python biomarkers_utils.py This will produce the following output: Instance of BMKDataReader successfully created. Dir. path: data/biomarkers_data Data successfully read: {'file_name': 'biomarker_data_20241016.xls', 'hash': 'e79002b53b2214e72643158dc033affe9ca43b443c7b912c03179ed215ef6b6c'}","title":"Biomarker Data Reading Checks"},{"location":"modules_documentation/biomarker_utils_doc/#usage-in-main-execution","text":"... # Reading of cohorts data # Reading biomarkers data bmk_data: dict = BMKDataReader().read_bmk_data().bmk_data_dict ... # processing of cohorts data # Processing of biomarkers data bmk_processor = BMKDataProcessor(bmk_data, cohort_ids).process() bmk_processed_data = bmk_processor.formatted_bmk_data # Exportation of bmk data to csv and feather files bmk_processor.export_qc_report() ...# instantitations of SQLExporter # Exportation to MySQL DB of bmk data sql_exporter.export_df_to_sql(bmk_processed_data, \"biomarkers\")","title":"Usage in main execution"},{"location":"modules_documentation/cohort_utils_doc/","text":"cohorts_utils Documentation Overview This module defines the Cohort class, which centralizes the process of homogenizing data from different cohorts into a common format, determined by the configuration data. Additionally, it is responsible for distributing the selected variables into different panels and formatting them appropriately (as specified in panel_data ) to allow for export in various formats. General Sturcture The module consists of the following key components: Functions responsible for the transformation and homogenization of the cohort databases (in pd.DataFrame format). Cohort class : This class centralizes cohort data (both the raw database, configuration data, etc.) as well as the process of data homogenization and, finally, the distribution of variables into each panel. It is important to note that the arguments required to initialize Cohort are the result of preprocessing carried out by DataPreprocessor . Auxiliary functions used by Cohort to initialize attributes. Functions and Classes In this sections are described the classes and functions defined in the module: Functions manage_string_type(df: pd.DataFrame, var: str) -> None Converts the specified column var in the DataFrame df to string type. Arguments : df (pd.DataFrame): The DataFrame containing the data. var (str): The name of the column to convert to string. Returns : None : This function does not return any value. It modifies the DataFrame in place. Logs If a ValueError occurs during conversion (e.g., invalid data), an error is logged. If a KeyError occurs (i.e., the column does not exist in the DataFrame), an error is logged. manage_category_type(...) -> None Converts the specified column var in the DataFrame df to the category data type, using a mapping provided in level_data_json . Arguments : var (str): The name of the column to convert to category. df (pd.DataFrame): The DataFrame containing the data. level_data_json (dict): A dictionary that contains the mapping ( map ) for the conversion of the column to category. Returns : None : This function does not return any value. It modifies the DataFrame in place. Logs If a ValueError occurs during conversion (e.g., invalid data), an error is logged. If a KeyError occurs (e.g., the column does not exist in the DataFrame), an error is logged. If any other unexpected Exception occurs, an error is logged. manage_numerical_type(var: str, data_type: Literal['int64', 'float64'], df: pd.DataFrame, var_data_json: dict) -> None Converts the specified column var in the DataFrame df to a numerical data type (int or float), and applies a conversion factor if available. Arguments : var (str): The name of the column to convert. data_type (Literal['int64', 'float64']): The target data type to convert the column to ( int64 or float64 ). df (pd.DataFrame): The DataFrame containing the data. var_data_json (dict): A dictionary containing additional information for the conversion, specifically the conversion factor. Returns : None : This function does not return any value. It modifies the DataFrame in place. Logs : If a ValueError occurs during conversion (e.g., invalid data), an error is logged. If a KeyError occurs (e.g., the column does not exist in the DataFrame), an error is logged. If any other unexpected Exception occurs, an error is logged. manage_datetime_type(var: str, data_type: str, df: pd.DataFrame) -> None Converts the specified column var in the DataFrame df to a datetime format, normalizing the date (removing time components). Arguments : var (str): The name of the column to convert to datetime. data_type (str): The target data type (used in logging) to convert the column to. df (pd.DataFrame): The DataFrame containing the data. Returns : None : This function does not return any value. It modifies the DataFrame in place. Logs : If a ValueError occurs during conversion (e.g., invalid datetime format), an error is logged. If a KeyError occurs (e.g., the column does not exist in the DataFrame), an error is logged. If any other unexpected Exception occurs, an error is logged. Notes The function uses pd.to_datetime to convert the column to datetime format and normalizes the date, setting the time to midnight (00:00:00). In case of errors, detailed log messages are recorded with information about the variable and the type of error that occurred. set_liveraim_format(df: pd.DataFrame, var_data_json: dict, level_data_json: dict) -> None Converts the columns (variables) in the DataFrame df to the proper format for liveraim. The format (data types, categories codification, etc.) is configured in var_data_json and level_data_json dictionaries. Uses the functions defined above to transform each column according to its data type defined in var_data_json . Arguments : df (pd.DataFrame): The DataFrame containing the data to be transformed. var_data_json (dict): Contains relevant information about the variables, used to transform the data into a common format. level_data_json (dict): Contains information about categorical variables, their levels, and the description codification of each level. Returns : None : This function does not return any value. It modifies the DataFrame in place. Logs Logs an info message when the data formatting process begins. Logs a warning if the variables in the DataFrame are not a subset of the variables described in var_data_json or level_data_json . type_converter(value, data_type: str, dictionary: dict) -> any Converts a value to a specified data type using a provided converter map. Arguments : value (any): The value to convert. data_type (str): The target data type (e.g., 'int', 'float', 'str'). dictionary (dict): A dictionary mapping data type strings to conversion functions. Returns : any : The converted value. Logs If an Exception occurs during the conversion process, an error is logged with details about the value, target data type, and the error itself. The program will then exit. levels_data_to_json(...) -> dict Converts a DataFrame containing configuration data about categorical variable levels into a dictionary and saves it as a JSON file if a path is provided. This dictionary will be used as a lighter object to format the variables. Arguments : var_column (str): Column name for the variable name. original_var_column (str): Column name for the original variable name. original_code_column (str): Column name for the original code value. common_code_column (str): Column name for the common code value. description_column (str): Column name for the description. original_data_type_column (str): Column name for the original data type. xlsx_file (str, optional): Path to the Excel file to read the DataFrame from. Default is None . df (pd.DataFrame, optional): DataFrame containing the data. If None , the xlsx_file is used. Default is None . final_path (str, optional): Path to save the JSON file. Default is None . Returns : dict : Dictionary containing the converted data. Logs Logs an error if the Excel file is not found ( FileNotFoundError ), if there's an issue with the format ( ValueError ), or if an unexpected exception occurs while reading the Excel file. During the dictionary creation process, no specific logs are recorded, but the function handles type conversion using the type_converter function and logs errors accordingly. If a JSON file is saved, a confirmation message is printed after saving the file. var_data_to_json(var_data: pd.DataFrame) -> dict Converts a DataFrame containing configuration data about core variables into a dictionary. This dictionary will be used as a lighter object to convert cohort databases into a homogeneous format. Arguments : var_data (pd.DataFrame): DataFrame containing the metadata about the core variables needed to set a common format between cohorts. Returns : dict : A dictionary containing the variables metadata stored in the DataFrame var_data . panel_data_to_json(panel_data: pd.DataFrame, var_data_json: dict) -> dict Converts a DataFrame containing information about the distribution of variables in panels into a JSON dictionary. It adds information from var_data_json to allow the building of the panels. Arguments : panel_data (pd.DataFrame): DataFrame containing the distribution of the variables in the chosen panels. var_data_json (dict): Dictionary containing relevant information about the variables needed to build the panels, such as bounds for numerical variables and units. Returns : dict : A dictionary containing the information from panel_data with additional data needed to build the panels, such as bounds and units for numerical variables. Logs Logs an error if panel_data is empty and will not proceed with the conversion. var_data_merge(cohort_list: list[Cohort]) -> tuple[pd.DataFrame, dict, bool] Merges the var_data DataFrames of each cohort and creates a new common var_data DataFrame to be used in the instantiation of the merged cohort (Liveraim). It selects a subset of columns in the var_data DataFrames from the different cohorts, using the var_data from the first cohort as a reference to check for discrepancies. Only the selected columns will be taken into account for merging. It also checks for any unmatched variables across cohorts and reports discrepancies. Arguments : cohort_list (list[Cohort]): A list of cohorts whose var_data DataFrames will be merged. Returns : pd.DataFrame : The merged var_data DataFrame containing the selected columns from all cohorts. dict[str, tuples] : A dictionary ( merge_meta ) containing rows (in tuple format) from each cohort\u2019s var_data that have at least one discrepant value in a selected column (i.e., this row does not exist in the reference var_data DataFrame). bool : True if the merging process was successful without discrepancies. False if there were discrepancies or if the input format was incorrect. Logs Logs an error if there is an issue with the cohort_list , such as being empty or not containing a list of valid Cohort objects. No specific logs are recorded during the comparison of var_data rows, but discrepancies are recorded in merge_meta . Additional Information : The function defines an internal compare_tuples function to handle tuple comparison, especially in cases where NaN values are present. If any cohort has rows in var_data that do not match the reference cohort, correct_merge is set to False . merge_cohorts(cohorts_list: list[Cohort], cohort_name: str) -> Cohort Merges the cohorts in cohorts_list . First, it merges the raw_data of each cohort, the var_data DataFrame (using the var_data_merge method), and the level_data DataFrame. It then processes the merged data to add the appropriate columns, drop duplicates, and finally creates a new instance of a Cohort using the merged data. Arguments : cohorts_list (list[Cohort]): A list of cohorts to be merged into a new cohort. cohort_name (str): The name of the new merged cohort. Returns : Cohort : The new merged_cohort initialized with the merged data from the cohorts in cohorts_list . Logs Logs an info message indicating the start of the cohort merging process. Logs an error if there is a problem during the merging of var_data , including information about discrepancies recorded in merge_meta . Additional Information : The function concatenates the raw_data , var_data , and level_data from each cohort. After merging, it removes duplicates in the merged var_data and level_data . The level_data is processed to ensure the correct mapping between original and final values, and the description column is dropped. The resulting merged cohort is initialized with the merged data, using 'id' as the ID variable and 'inclusion_date' as the date variable. Classes Class Cohort Description This class is the main object for data processing and export in the LiverAim project. It takes raw_data (presumably preprocessed) as input, along with configuration data about the selected/core variables ( var_data ), configuration data about the levels of the selected/core categorical variables ( level_data ), and other relevant parameters that define each cohort. It then centralizes the process of homogenization and formatting of the cohort, creating the attribute homogenized_data , which has a common format across cohorts. It also includes a method to distribute the variables in the final panels according to the configuration data in panel_metadata (see initial data and configuration data section for more specifications about this object). Attributes cohort_name (str): The name of the cohort being processed. status (str): The status of the cohort. Set to \"ongoing\" by default. raw_data (pd.DataFrame): Preprocessed data extracted from the DataPreprocessor class. This contains all the variables in the cohort in their original format, along with additional variables added in the preprocessor class. var_data (pd.DataFrame): Preprocessed variable configuration data extracted from the DataPreprocessor class. This DataFrame contains the original variable information along with additional metadata added during preprocessing. var_data_json (dict): A dictionary containing the same information as var_data , in a JSON-like format. level_data (pd.DataFrame): Preprocessed level configuration data extracted from the DataPreprocessor class. This DataFrame contains original categorical variable information along with additional configuration data. level_data_json (dict): A dictionary containing the same information as level_data , in a JSON-like format. id_variable (str): The name of the variable used for patient IDs. date_variable (str): The name of the variable representing the inclusion date for patients. selected_variables (list): A list of the selected/core variables of the cohort. These are the original variable names from the cohort data. panel_data (pd.DataFrame, optional): Configuration data about panels and the distribution of variables in each panel. panel_data_json (dict, optional): A JSON-like dictionary containing the same information as panel_data . var_translate_dict (dict): A dictionary used to translate specific cohort variable names to the common format names. homogeneous_data (pd.DataFrame, optional): A DataFrame containing the selected variables, processed to fit the common LiverAim format. final_data (dict[str, pd.DataFrame], optional): A dictionary where each key is a panel name, and the corresponding value is a DataFrame with the panel's data. Methods __init__(self, cohort_name: str, raw_data: pd.DataFrame, var_data: pd.DataFrame, level_data: pd.DataFrame, id_variable: str, date_variable: str, status: str = \"ongoing\") : Initializes the Cohort class with cohort data, variable and level configuration data, and relevant parameters such as ID and inclusion date variables. In particular it calls the setup method, responsible of creating the main attributes. Then, it initializes the attributes _homogeneous_data and fina_data as None , so the class can check if they have been instantiatied or not. Arguments : cohort_name (str): The name of the cohort. raw_data (pd.DataFrame): The preprocessed cohort data. var_data (pd.DataFrame): The preprocessed variable metadata. level_data (pd.DataFrame): The preprocessed level metadata. id_variable (str): The patient ID variable. date_variable (str): The inclusion date variable. status (str, optional): The status of the cohort, defaults to \"ongoing\" . setup(self, cohort_name: str, raw_data: pd.DataFrame, var_data: pd.DataFrame, level_data: pd.DataFrame, id_variable: str, date_variable: str, status: str = \"ongoing\") : Sets up the main attributes of the cohort. This method is called within __init__ but can also be used for updating data and configuration data. Arguments : Same as __init__ . Returns : None . homogenize_data(self) -> Cohort : Processes the raw data, selecting core variables, translating them to the final common name (using the var_translate_dict attribute) and formatting them according to the common LiverAim structure. It calls the function set_liveraim_format described in the section Functions . Sets the homogeneous_data attribute. Returns : Cohort : Returns the Cohort instance with the homogeneous_data attribute set. set_final_data(self, panel_data: dict[str, pd.DataFrame]) -> Cohort : Creates and sets the final dataset structure based on panel data. The panel_data provides information on which variables belong to each panel and which should be \"melted\". Arguments : panel_data (dict[str, pd.DataFrame]): A dictionary with panel names as keys and DataFrames describing the panel structure as values. Returns : Cohort : Returns the updated Cohort instance. set_var_translate_dict(self) -> None : Sets the var_translate_dict attribute using the data in var_data to translate variable names to the common LiverAim format. Returns : None . set_selected_variables(self) -> None : Sets the selected_variables attribute, which is a list of core variables based on the configuration data in var_data . Returns : None . set_panel_data(self, panel_data: pd.DataFrame) -> None : Sets the panel_data and panel_data_json attributes using the provided panel data. Returns : None . set_id(self, id_name: str = \"liveraim_id\") -> Cohort : Sets a new liveraim_id variable, a commmon-format identifier for each patient, and updates the cohort data with the new variable. Arguments : id_name (str, optional): The name of the new ID variable, defaults to \"liveraim_id\" . Returns : Cohort : Returns the updated Cohort instance. Logs In __init__ : Logs the initialization process and any errors related to setting up the cohort attributes. In homogenize_data : Logs the start and completion of the data homogenization process. In set_final_data : Logs the start and completion of the final data structuring process. In set_id : Logs the start and completion of the process to set the common ID ( liveraim_id ). General Operation The instantiation of a Cohort requires three fundamental elements (in addition to other less critical parameters) that come from preprocessing by the DataPreprocessor class (see Data Processing utils ): raw_data : The cohort database, which is the result of merging different versions of the database for the same cohort. var_data : A DataFrame containing configuration data for the selected variables, along with those added during preprocessing. level_data : A DataFrame containing configuration data for the levels of selected categorical variables, as well as those added during preprocessing. During instantiation, the attributes described in the class are created. Specifically, for the var_data and level_data DataFrames, an attribute with the same data is created but in dictionary format. This facilitates access to the information and subtly improves performance. Variables used as the ID and inclusion date are also defined, as they are particularly important. Additionally, a list of selected variables for the final dataset and a dictionary for translating their respective names into the common format are created. Once the class is instantiated, the homogenize_data method can be called. This method uses raw_data to select, translate, and transform variables, creating a new DataFrame. This new DataFrame is stored in the homogenized_data attribute. These transformations (e.g., data type assignment, unit conversion, and level mapping for categorical variables) are centralized in the set_liveraim_format function. After the data for all cohorts is homogenized, the merge_cohorts function takes a list of cohorts and merges them, returning a Cohort object in the common format. It does so by concatenating the homogenized_data DataFrames from each cohort and using them as the raw_data parameter to instantiate a new cohort. Similarly, the level_data and var_data files from each cohort are correctly \"merged\" for use as parameters in this new cohort instantiation. As a result, a new Cohort object is created, containing as raw_data the homogenized data from all the cohorts. The set_id method allows the creation of a new identifier variable in the raw_data attribute while updating the configuration data. This is useful for creating a common identifier across all cohorts once they have been merged (after homogenization). Although the raw_data produced by merge_cohorts has the correct format, merging DataFrames can sometimes alter some of the data types. To prevent this, it is recommended to call the homogenize_data method again, which ensures that the data types are correct. If you wish to create the final tables or panels (presumably after homogenizing, merging cohorts, and creating a new common ID variable), the set_final_data method generates a dictionary containing the different DataFrames, each representing a panel. It requires the panel_data dictionary as a parameter, which contains the configuration data for the panels. This method selects the variables that should appear in each panel and transforms the DataFrames to long format if necessary. The dictionary with the final DataFrames is stored in the final_data attribute. Usage in main execution ... # Create a list with the names of the cohorts to be used. cohort_name_list: list[str] = [cc.LIVERSCREEN_NAME, cc.GLUCOFIB_NAME, cc.ALCOFIB_NAME] # Create an empty list to store all the cohort objects. cohort_list = [] # Iterates throgh all the cohort names to creat a cohort object for each cohort. for cohort_name in cohort_name_list: ... # Preprocess the configuration data and the databases with an instance of DataPreprocessor, #in this case cohort_preprocessor data, var_data, level_data = cohort_preprocessor.get_data() # Intantiate a Cohort object using the config data and call the homogenize_data method. cohort = Cohort (cohort_name=cohort_name, raw_data=data, var_data=var_data, level_data=level_data, id_variable ='id', date_variable='date_0').homogenize_data() cohort_list.append(cohort) # Creates a cohort by merging all cohorts in cohort list liverscreen_cohort = merge_cohorts(cohort_list).set_id().homogenize_data() # Sets the attribute final_data, with all the final panels. liverscreen_cohort.set_final_data(panel_data)","title":"Cohort utils"},{"location":"modules_documentation/cohort_utils_doc/#cohorts_utils-documentation","text":"","title":"cohorts_utils Documentation"},{"location":"modules_documentation/cohort_utils_doc/#overview","text":"This module defines the Cohort class, which centralizes the process of homogenizing data from different cohorts into a common format, determined by the configuration data. Additionally, it is responsible for distributing the selected variables into different panels and formatting them appropriately (as specified in panel_data ) to allow for export in various formats.","title":"Overview"},{"location":"modules_documentation/cohort_utils_doc/#general-sturcture","text":"The module consists of the following key components: Functions responsible for the transformation and homogenization of the cohort databases (in pd.DataFrame format). Cohort class : This class centralizes cohort data (both the raw database, configuration data, etc.) as well as the process of data homogenization and, finally, the distribution of variables into each panel. It is important to note that the arguments required to initialize Cohort are the result of preprocessing carried out by DataPreprocessor . Auxiliary functions used by Cohort to initialize attributes.","title":"General Sturcture"},{"location":"modules_documentation/cohort_utils_doc/#functions-and-classes","text":"In this sections are described the classes and functions defined in the module:","title":"Functions and Classes"},{"location":"modules_documentation/cohort_utils_doc/#functions","text":"manage_string_type(df: pd.DataFrame, var: str) -> None Converts the specified column var in the DataFrame df to string type. Arguments : df (pd.DataFrame): The DataFrame containing the data. var (str): The name of the column to convert to string. Returns : None : This function does not return any value. It modifies the DataFrame in place. Logs If a ValueError occurs during conversion (e.g., invalid data), an error is logged. If a KeyError occurs (i.e., the column does not exist in the DataFrame), an error is logged. manage_category_type(...) -> None Converts the specified column var in the DataFrame df to the category data type, using a mapping provided in level_data_json . Arguments : var (str): The name of the column to convert to category. df (pd.DataFrame): The DataFrame containing the data. level_data_json (dict): A dictionary that contains the mapping ( map ) for the conversion of the column to category. Returns : None : This function does not return any value. It modifies the DataFrame in place. Logs If a ValueError occurs during conversion (e.g., invalid data), an error is logged. If a KeyError occurs (e.g., the column does not exist in the DataFrame), an error is logged. If any other unexpected Exception occurs, an error is logged. manage_numerical_type(var: str, data_type: Literal['int64', 'float64'], df: pd.DataFrame, var_data_json: dict) -> None Converts the specified column var in the DataFrame df to a numerical data type (int or float), and applies a conversion factor if available. Arguments : var (str): The name of the column to convert. data_type (Literal['int64', 'float64']): The target data type to convert the column to ( int64 or float64 ). df (pd.DataFrame): The DataFrame containing the data. var_data_json (dict): A dictionary containing additional information for the conversion, specifically the conversion factor. Returns : None : This function does not return any value. It modifies the DataFrame in place. Logs : If a ValueError occurs during conversion (e.g., invalid data), an error is logged. If a KeyError occurs (e.g., the column does not exist in the DataFrame), an error is logged. If any other unexpected Exception occurs, an error is logged. manage_datetime_type(var: str, data_type: str, df: pd.DataFrame) -> None Converts the specified column var in the DataFrame df to a datetime format, normalizing the date (removing time components). Arguments : var (str): The name of the column to convert to datetime. data_type (str): The target data type (used in logging) to convert the column to. df (pd.DataFrame): The DataFrame containing the data. Returns : None : This function does not return any value. It modifies the DataFrame in place. Logs : If a ValueError occurs during conversion (e.g., invalid datetime format), an error is logged. If a KeyError occurs (e.g., the column does not exist in the DataFrame), an error is logged. If any other unexpected Exception occurs, an error is logged. Notes The function uses pd.to_datetime to convert the column to datetime format and normalizes the date, setting the time to midnight (00:00:00). In case of errors, detailed log messages are recorded with information about the variable and the type of error that occurred. set_liveraim_format(df: pd.DataFrame, var_data_json: dict, level_data_json: dict) -> None Converts the columns (variables) in the DataFrame df to the proper format for liveraim. The format (data types, categories codification, etc.) is configured in var_data_json and level_data_json dictionaries. Uses the functions defined above to transform each column according to its data type defined in var_data_json . Arguments : df (pd.DataFrame): The DataFrame containing the data to be transformed. var_data_json (dict): Contains relevant information about the variables, used to transform the data into a common format. level_data_json (dict): Contains information about categorical variables, their levels, and the description codification of each level. Returns : None : This function does not return any value. It modifies the DataFrame in place. Logs Logs an info message when the data formatting process begins. Logs a warning if the variables in the DataFrame are not a subset of the variables described in var_data_json or level_data_json . type_converter(value, data_type: str, dictionary: dict) -> any Converts a value to a specified data type using a provided converter map. Arguments : value (any): The value to convert. data_type (str): The target data type (e.g., 'int', 'float', 'str'). dictionary (dict): A dictionary mapping data type strings to conversion functions. Returns : any : The converted value. Logs If an Exception occurs during the conversion process, an error is logged with details about the value, target data type, and the error itself. The program will then exit. levels_data_to_json(...) -> dict Converts a DataFrame containing configuration data about categorical variable levels into a dictionary and saves it as a JSON file if a path is provided. This dictionary will be used as a lighter object to format the variables. Arguments : var_column (str): Column name for the variable name. original_var_column (str): Column name for the original variable name. original_code_column (str): Column name for the original code value. common_code_column (str): Column name for the common code value. description_column (str): Column name for the description. original_data_type_column (str): Column name for the original data type. xlsx_file (str, optional): Path to the Excel file to read the DataFrame from. Default is None . df (pd.DataFrame, optional): DataFrame containing the data. If None , the xlsx_file is used. Default is None . final_path (str, optional): Path to save the JSON file. Default is None . Returns : dict : Dictionary containing the converted data. Logs Logs an error if the Excel file is not found ( FileNotFoundError ), if there's an issue with the format ( ValueError ), or if an unexpected exception occurs while reading the Excel file. During the dictionary creation process, no specific logs are recorded, but the function handles type conversion using the type_converter function and logs errors accordingly. If a JSON file is saved, a confirmation message is printed after saving the file. var_data_to_json(var_data: pd.DataFrame) -> dict Converts a DataFrame containing configuration data about core variables into a dictionary. This dictionary will be used as a lighter object to convert cohort databases into a homogeneous format. Arguments : var_data (pd.DataFrame): DataFrame containing the metadata about the core variables needed to set a common format between cohorts. Returns : dict : A dictionary containing the variables metadata stored in the DataFrame var_data . panel_data_to_json(panel_data: pd.DataFrame, var_data_json: dict) -> dict Converts a DataFrame containing information about the distribution of variables in panels into a JSON dictionary. It adds information from var_data_json to allow the building of the panels. Arguments : panel_data (pd.DataFrame): DataFrame containing the distribution of the variables in the chosen panels. var_data_json (dict): Dictionary containing relevant information about the variables needed to build the panels, such as bounds for numerical variables and units. Returns : dict : A dictionary containing the information from panel_data with additional data needed to build the panels, such as bounds and units for numerical variables. Logs Logs an error if panel_data is empty and will not proceed with the conversion. var_data_merge(cohort_list: list[Cohort]) -> tuple[pd.DataFrame, dict, bool] Merges the var_data DataFrames of each cohort and creates a new common var_data DataFrame to be used in the instantiation of the merged cohort (Liveraim). It selects a subset of columns in the var_data DataFrames from the different cohorts, using the var_data from the first cohort as a reference to check for discrepancies. Only the selected columns will be taken into account for merging. It also checks for any unmatched variables across cohorts and reports discrepancies. Arguments : cohort_list (list[Cohort]): A list of cohorts whose var_data DataFrames will be merged. Returns : pd.DataFrame : The merged var_data DataFrame containing the selected columns from all cohorts. dict[str, tuples] : A dictionary ( merge_meta ) containing rows (in tuple format) from each cohort\u2019s var_data that have at least one discrepant value in a selected column (i.e., this row does not exist in the reference var_data DataFrame). bool : True if the merging process was successful without discrepancies. False if there were discrepancies or if the input format was incorrect. Logs Logs an error if there is an issue with the cohort_list , such as being empty or not containing a list of valid Cohort objects. No specific logs are recorded during the comparison of var_data rows, but discrepancies are recorded in merge_meta . Additional Information : The function defines an internal compare_tuples function to handle tuple comparison, especially in cases where NaN values are present. If any cohort has rows in var_data that do not match the reference cohort, correct_merge is set to False . merge_cohorts(cohorts_list: list[Cohort], cohort_name: str) -> Cohort Merges the cohorts in cohorts_list . First, it merges the raw_data of each cohort, the var_data DataFrame (using the var_data_merge method), and the level_data DataFrame. It then processes the merged data to add the appropriate columns, drop duplicates, and finally creates a new instance of a Cohort using the merged data. Arguments : cohorts_list (list[Cohort]): A list of cohorts to be merged into a new cohort. cohort_name (str): The name of the new merged cohort. Returns : Cohort : The new merged_cohort initialized with the merged data from the cohorts in cohorts_list . Logs Logs an info message indicating the start of the cohort merging process. Logs an error if there is a problem during the merging of var_data , including information about discrepancies recorded in merge_meta . Additional Information : The function concatenates the raw_data , var_data , and level_data from each cohort. After merging, it removes duplicates in the merged var_data and level_data . The level_data is processed to ensure the correct mapping between original and final values, and the description column is dropped. The resulting merged cohort is initialized with the merged data, using 'id' as the ID variable and 'inclusion_date' as the date variable.","title":"Functions"},{"location":"modules_documentation/cohort_utils_doc/#classes","text":"","title":"Classes"},{"location":"modules_documentation/cohort_utils_doc/#class-cohort","text":"","title":"Class Cohort"},{"location":"modules_documentation/cohort_utils_doc/#description","text":"This class is the main object for data processing and export in the LiverAim project. It takes raw_data (presumably preprocessed) as input, along with configuration data about the selected/core variables ( var_data ), configuration data about the levels of the selected/core categorical variables ( level_data ), and other relevant parameters that define each cohort. It then centralizes the process of homogenization and formatting of the cohort, creating the attribute homogenized_data , which has a common format across cohorts. It also includes a method to distribute the variables in the final panels according to the configuration data in panel_metadata (see initial data and configuration data section for more specifications about this object).","title":"Description"},{"location":"modules_documentation/cohort_utils_doc/#attributes","text":"cohort_name (str): The name of the cohort being processed. status (str): The status of the cohort. Set to \"ongoing\" by default. raw_data (pd.DataFrame): Preprocessed data extracted from the DataPreprocessor class. This contains all the variables in the cohort in their original format, along with additional variables added in the preprocessor class. var_data (pd.DataFrame): Preprocessed variable configuration data extracted from the DataPreprocessor class. This DataFrame contains the original variable information along with additional metadata added during preprocessing. var_data_json (dict): A dictionary containing the same information as var_data , in a JSON-like format. level_data (pd.DataFrame): Preprocessed level configuration data extracted from the DataPreprocessor class. This DataFrame contains original categorical variable information along with additional configuration data. level_data_json (dict): A dictionary containing the same information as level_data , in a JSON-like format. id_variable (str): The name of the variable used for patient IDs. date_variable (str): The name of the variable representing the inclusion date for patients. selected_variables (list): A list of the selected/core variables of the cohort. These are the original variable names from the cohort data. panel_data (pd.DataFrame, optional): Configuration data about panels and the distribution of variables in each panel. panel_data_json (dict, optional): A JSON-like dictionary containing the same information as panel_data . var_translate_dict (dict): A dictionary used to translate specific cohort variable names to the common format names. homogeneous_data (pd.DataFrame, optional): A DataFrame containing the selected variables, processed to fit the common LiverAim format. final_data (dict[str, pd.DataFrame], optional): A dictionary where each key is a panel name, and the corresponding value is a DataFrame with the panel's data.","title":"Attributes"},{"location":"modules_documentation/cohort_utils_doc/#methods","text":"__init__(self, cohort_name: str, raw_data: pd.DataFrame, var_data: pd.DataFrame, level_data: pd.DataFrame, id_variable: str, date_variable: str, status: str = \"ongoing\") : Initializes the Cohort class with cohort data, variable and level configuration data, and relevant parameters such as ID and inclusion date variables. In particular it calls the setup method, responsible of creating the main attributes. Then, it initializes the attributes _homogeneous_data and fina_data as None , so the class can check if they have been instantiatied or not. Arguments : cohort_name (str): The name of the cohort. raw_data (pd.DataFrame): The preprocessed cohort data. var_data (pd.DataFrame): The preprocessed variable metadata. level_data (pd.DataFrame): The preprocessed level metadata. id_variable (str): The patient ID variable. date_variable (str): The inclusion date variable. status (str, optional): The status of the cohort, defaults to \"ongoing\" . setup(self, cohort_name: str, raw_data: pd.DataFrame, var_data: pd.DataFrame, level_data: pd.DataFrame, id_variable: str, date_variable: str, status: str = \"ongoing\") : Sets up the main attributes of the cohort. This method is called within __init__ but can also be used for updating data and configuration data. Arguments : Same as __init__ . Returns : None . homogenize_data(self) -> Cohort : Processes the raw data, selecting core variables, translating them to the final common name (using the var_translate_dict attribute) and formatting them according to the common LiverAim structure. It calls the function set_liveraim_format described in the section Functions . Sets the homogeneous_data attribute. Returns : Cohort : Returns the Cohort instance with the homogeneous_data attribute set. set_final_data(self, panel_data: dict[str, pd.DataFrame]) -> Cohort : Creates and sets the final dataset structure based on panel data. The panel_data provides information on which variables belong to each panel and which should be \"melted\". Arguments : panel_data (dict[str, pd.DataFrame]): A dictionary with panel names as keys and DataFrames describing the panel structure as values. Returns : Cohort : Returns the updated Cohort instance. set_var_translate_dict(self) -> None : Sets the var_translate_dict attribute using the data in var_data to translate variable names to the common LiverAim format. Returns : None . set_selected_variables(self) -> None : Sets the selected_variables attribute, which is a list of core variables based on the configuration data in var_data . Returns : None . set_panel_data(self, panel_data: pd.DataFrame) -> None : Sets the panel_data and panel_data_json attributes using the provided panel data. Returns : None . set_id(self, id_name: str = \"liveraim_id\") -> Cohort : Sets a new liveraim_id variable, a commmon-format identifier for each patient, and updates the cohort data with the new variable. Arguments : id_name (str, optional): The name of the new ID variable, defaults to \"liveraim_id\" . Returns : Cohort : Returns the updated Cohort instance.","title":"Methods"},{"location":"modules_documentation/cohort_utils_doc/#logs","text":"In __init__ : Logs the initialization process and any errors related to setting up the cohort attributes. In homogenize_data : Logs the start and completion of the data homogenization process. In set_final_data : Logs the start and completion of the final data structuring process. In set_id : Logs the start and completion of the process to set the common ID ( liveraim_id ).","title":"Logs"},{"location":"modules_documentation/cohort_utils_doc/#general-operation","text":"The instantiation of a Cohort requires three fundamental elements (in addition to other less critical parameters) that come from preprocessing by the DataPreprocessor class (see Data Processing utils ): raw_data : The cohort database, which is the result of merging different versions of the database for the same cohort. var_data : A DataFrame containing configuration data for the selected variables, along with those added during preprocessing. level_data : A DataFrame containing configuration data for the levels of selected categorical variables, as well as those added during preprocessing. During instantiation, the attributes described in the class are created. Specifically, for the var_data and level_data DataFrames, an attribute with the same data is created but in dictionary format. This facilitates access to the information and subtly improves performance. Variables used as the ID and inclusion date are also defined, as they are particularly important. Additionally, a list of selected variables for the final dataset and a dictionary for translating their respective names into the common format are created. Once the class is instantiated, the homogenize_data method can be called. This method uses raw_data to select, translate, and transform variables, creating a new DataFrame. This new DataFrame is stored in the homogenized_data attribute. These transformations (e.g., data type assignment, unit conversion, and level mapping for categorical variables) are centralized in the set_liveraim_format function. After the data for all cohorts is homogenized, the merge_cohorts function takes a list of cohorts and merges them, returning a Cohort object in the common format. It does so by concatenating the homogenized_data DataFrames from each cohort and using them as the raw_data parameter to instantiate a new cohort. Similarly, the level_data and var_data files from each cohort are correctly \"merged\" for use as parameters in this new cohort instantiation. As a result, a new Cohort object is created, containing as raw_data the homogenized data from all the cohorts. The set_id method allows the creation of a new identifier variable in the raw_data attribute while updating the configuration data. This is useful for creating a common identifier across all cohorts once they have been merged (after homogenization). Although the raw_data produced by merge_cohorts has the correct format, merging DataFrames can sometimes alter some of the data types. To prevent this, it is recommended to call the homogenize_data method again, which ensures that the data types are correct. If you wish to create the final tables or panels (presumably after homogenizing, merging cohorts, and creating a new common ID variable), the set_final_data method generates a dictionary containing the different DataFrames, each representing a panel. It requires the panel_data dictionary as a parameter, which contains the configuration data for the panels. This method selects the variables that should appear in each panel and transforms the DataFrames to long format if necessary. The dictionary with the final DataFrames is stored in the final_data attribute.","title":"General Operation"},{"location":"modules_documentation/cohort_utils_doc/#usage-in-main-execution","text":"... # Create a list with the names of the cohorts to be used. cohort_name_list: list[str] = [cc.LIVERSCREEN_NAME, cc.GLUCOFIB_NAME, cc.ALCOFIB_NAME] # Create an empty list to store all the cohort objects. cohort_list = [] # Iterates throgh all the cohort names to creat a cohort object for each cohort. for cohort_name in cohort_name_list: ... # Preprocess the configuration data and the databases with an instance of DataPreprocessor, #in this case cohort_preprocessor data, var_data, level_data = cohort_preprocessor.get_data() # Intantiate a Cohort object using the config data and call the homogenize_data method. cohort = Cohort (cohort_name=cohort_name, raw_data=data, var_data=var_data, level_data=level_data, id_variable ='id', date_variable='date_0').homogenize_data() cohort_list.append(cohort) # Creates a cohort by merging all cohorts in cohort list liverscreen_cohort = merge_cohorts(cohort_list).set_id().homogenize_data() # Sets the attribute final_data, with all the final panels. liverscreen_cohort.set_final_data(panel_data)","title":"Usage in main execution"},{"location":"modules_documentation/configuration_module/","text":"config Package Documentation The config module contains the parameters and variables required for the proper functioning of the code. These can be modified to adapt the execution to different environments, offer flexibility, and add future functionalities. This allows you to change the behavior of the program without needing to access and modify the source code. The module contains the following files: main_config.py : Main variables and parameters cohort_config.py : Cohort Class related parameters. connection_config.py : connection parameters. new_var_config.py : Definition of new or derived varibles. new_levels_config.py : Definition of the levels of the new/derived categorical variables. reference_names.py : Names of the columns of the innitial data files ( var_data , level_data and panel_metadata ) The following sections describe the variables contained in each of these files, their purpose, and possible modifications. main_config module General paremeters EXECUTION_DATETIME (str) : DateTime of the execution. Used for debugging, logging and exportation purposes. NUMERIC_TYPES (list) : List of numeric python types compatibles with the databse. CONFIG_OBJ_NAMES (list) : List with the names of the configuration data objects in the code (such as var_data , level_data , etc.) BUILD_DUMMIES (bool) : Indicates wheter a dummy dataset of the execution must be build and exported to .feather files. Quality control paramenters ALPHA (str) : Error threshold used during the numeric QC checks. EXPORT_QC_RECORD (bool) : With a value of True , it exports the data generated during the quality control process. For more details, see the section Quality Control Utils . With a value of False , quality control is still performed, but the generated data is not saved after execution (except for what is printed in the log). NAN_RECORD_EXPORT_FOLDER : ... QC_REPORT_FOLDER (str) : name of the folder taht centralizes the QC: it will contain the QC output and the code for processing this data and the rendered report. QC_DATA_FOLDER (str) : name of the folder where all the data generated during the quality control checks will be dumped (if EXPORT_QC_RECORD is set to true). This folder is itended to be a subdirectory fo the directory above QC_REPORT_FOLDER Variables used to read data files (databases, var_data, level_data, panel_metadata) The following variables are mainly used in the module file_reading_utils . PANEL_DATA_FILE_NAME (str) : Name of the file containing the panel data (should be an .xlsx file). CONFIG_DATA_METADATA_FILE_PATH (str) : Path of the file containing the metadata of the config data tables (i.e. data types of each column) that will be exported to the database. **** LIVERSCREEN_VAR_FILE_NAME (str) : Name of the file containing the var_data for the Liverscreen cohort (should be an .xlsx file). GLUCOFIB_VAR_FILE_NAME (str) : Name of the file containing the var_data for the Glucofib cohort (should be an .xlsx file). ALCOFIB_VAR_FILE_NAME (str) : Name of the file containing the var_data for the Alcofib cohort (should be an .xlsx file). DECIDE_VAR_FILE_NAME (str) : Name of the file containing the var_data for the Decide cohort (should be an .xlsx file). MARINA_VAR_FILE_NAME (str) : Name of the file containing the var_data for the Marina cohort (should be an .xlsx file). GALAALD_VAR_FILE_NAME (str) : Name of the file containing the var_data for the Galaald cohort (should be an .xlsx file). VAR_FILE_NAMES (dict) : Dictionary containing the cohort names (as defined in the cohort_config module) as keys, and the corresponding var_data file names as values (i.e., the names defined above). This dictionary should not be modified unless a new cohort is added: the dictionary is automatically generated using the other variables, ensuring it stays up to date. LIVERSCREEN_LEVEL_FILE_NAME (str) : Name of the file containing the level_data for the Liverscreen cohort (should be an .xlsx file). GLUCOFIB_LEVEL_FILE_NAME (str) : Name of the file containing the level_data for the Glucofib cohort (should be an .xlsx file). ALCOFIB_LEVEL_FILE_NAME (str) : Name of the file containing the level_data for the Alcofib cohort (should be an .xlsx file). DECIDE_LEVEL_FILE_NAME (str) : Name of the file containing the level_data for the Decide cohort (should be an .xlsx file). MARINA_LEVEL_FILE_NAME (str) : Name of the file containing the level_data for the Marina cohort (should be an .xlsx file). GALAALD_LEVEL_FILE_NAME (str) : Name of the file containing the level_data for the Galaald cohort (should be an .xlsx file). LEVEL_FILE_NAMES (dict) : Dictionary containing the cohort names (as defined in the cohort_config module) as keys, and the corresponding level_data file names as values (i.e., the names defined above). This dictionary should not be modified unless a new cohort is added: the dictionary is automatically generated using the other variables, ensuring it stays up to date. LIVERSCREEN_COMB_VARS_FILE_NAME (str) : Name of the file containing the var_comb_data for the Liverscreen cohort (should be an .json file). GLUCOFIB_COMB_VARS_FILE_NAME (str) : Name of the file containing the var_comb_data for the Glucofib cohort (should be an .json file). ALCOFIB_COMB_VARS_FILE_NAME (str) : Name of the file containing the var_comb_data for the Alcofib cohort (should be an .json file). MARINA1_COMB_VARS_FILE_NAME (str) : Name of the file containing the var_comb_data for the Marina cohort (should be an .json file). DECIDE_COMB_VARS_FILE_NAME (str) : Name of the file containing the var_comb_data for the Decide cohort (should be an .json file). GALAALD_COMB_VARS_FILE_NAME (str) : Name of the file containing the var_comb_data for the Galaald cohort (should be an .json file). COMB_VARS_FILE_NAMES (dict) : Dictionary containing the cohort names (as defined in the cohort_config module) as keys, and the corresponding comb_var_data file names as values (i.e., the names defined above). This dictionary should not be modified unless a new cohort is added: the dictionary is automatically generated using the other variables, ensuring it stays up to date. If you wish to use other var_data , level_data , or panel_metadata files (for testing, verification, etc.), you can modify the value of these variables to read those files. COMMON_DATA_FOLDER (str) : Name of the folder containing all the data. This folder is not tracked by git and must follow the structure described in Structure of data/cohort_name/ Directory . DATABASES_FOLDER (str) : Name of the folder, inside each cohort specific directory, that will contain the raw data recieved from the partners. READING_METHODS_DICT (dict) : Dictionary containing strings as keys that indicate different types of files and their assiciated reading method as the value. DATABASES_FORMAT (dict) : Dictionary containig the name of each cohort as key and the type of file of their raw databases file. It is used with the READING_METHODS_DICT to read the cohorts databases properly. Variables used to export data EXPORT_FILES (bool) : With a value of True , it exports the final data (the data warehouse, with the panel structure described in ??? ) to .csv and .feather files. With a value of False , these files are not generated. For more details, see the section File Exporting Utils Module . CREATE_SQL_DB (bool) : Con un valor de True se exporta los datos a una base de datos MySQL (i.e. se crea el datawarehouse). Con un valor de False no se exportan los datos a MySQL. DATAWAREHOUSE_PATH (str) : Name of the folder where the data warehouse in .csv and .feather will be exported. ANALYSIS_REPORT_SCRIPT_PATH (str) : absolute path to the .Rmd script used to generate the initial descriptive analysis of the data, in case the results are exported to files. COLS_IN_PANELS (dict) : Dictionary mapping each panel name to the list of column names that should be exported for that panel in .feather/.csv files. Used to customize the structure of the data warehouse. COLS_IN_SQL_PANELS (dict) : Dictionary mapping each panel name to the list of column names that should be exported for that panel in the SQL DB. Used to customize the structure of the data warehouse. Note : This module is usually imported using the abreviation config : from config import main_config as config cohort_config module This module contians variables related to the configuration and instantiation of each Cohort. ALCOFIB_NAME (str) : Name to be used to refer to the Alcofib cohort during the execution of the main code. GLUCOFIB_NAME (str) : Name to be used to refer to the Glucofib cohort during the execution of the main code. LIVERSCREEN_NAME (str) : Name to be used to refer to the Liverscreen cohort during the execution of the main code. DECIDE_NAME (str) : Name to be used to refer to the Decide cohort during the execution of the main code. MARINA1_NAME (str) : Name to be used to refer to the Marina cohort during the execution of the main code. LIVERAIM_NAME (str) : Name to be used to refer to the Liveraim merged database during the execution of the main code. GALAALD_NAME (str) : Name to be used to refer to the Galaald merged database during the execution of the main code. ALL_COHORTS (list) : List of all the cohort names. This variable is created using the variables defined above and should be only modified to add new elements (i.e. new cohorts). WP1_COHORT_NAME_LIST (list) : List of cohort names that will be used for the processing of WP1. This variable is created using the variables defined above. WP2_COHORT_NAME_LIST (list) : List of cohort names that will be used for the processing of WP2. This variable is created using the variables defined above. For each cohort, the following variables are also defined (for brevity, the term <COHORT_NAME> will be used to refer to the name of the specific cohort): COHOR<T_NAME>_INCL_DATE_VAR (str) : Original name of the 'inclusion date' variable. <COHORT_NAME>_AGE_VAR (str) : Original name of the 'age' variable. <COHORT_NAME>_ID_VAR (str) : Original name of the 'identifier' variable. <COHORT_NAME>_COHORT_STATUS (str) : Value of the categorical status variable. For more details about this variable, refer to the section on the new_levels_config module . COHORTS_DATA (dict) : A dictionary that contains the previously defined variables for each cohort. This dictionary is imported (instead of importing each variable individually) into the code, making it easier to access these variables. The dictionary keys are the names of each cohort, and the values are the corresponding variables for that cohort as described above. PERCENT_VARS (list) : List of variable names (using their common mapped name) that should be represented with percentage (%) units. This list is used during formatting and processing to handle inconsistencies in how these variables are expressed. Note : This module is usually imported using the abreviation cc : from config import cohort_config as cc connection_config module This module defines: Database connection parameters for MySQL. Dictionaries for mapping variable types across platforms/languages. Connection Parameters It's important to understand how the connection parameters for the MySQL database are managed. To connect to the database (as explained in the MySQL connection configuration section), five parameters are required: USER : The username used to connect to the database (must have appropriate permissions). PASSWORD : The user's password to access the database. DATABASE_NAME : The name of the database to be accessed (the user must have permissions to access the database). PORT : The connection port. HOST : The hostname. If accessing a local database, this is usually localhost , while for remote access, it is typically the server name or IP address. Each of these parameters must be created as environment variables . These can be virtual or global environment variables, but they must be accessible through Python's os package. The names of these environment variables (which correspond to each of the connection parameters mentioned above) can be chosen at will. Once these variables are created, you need to modify the connection_config.py file, where the variables USER (str) , PASSWORD (str) , DATABASE_NAME (str) , PORT (str) , HOST (str) are listed. These variables should be assigned the name of the environment variable you want to use for that parameter . For example, suppose you want to connect to a database named example_db . First, you would create the following environment variables (ensuring that their values are correct to establish the connection properly): USER=me_myself DB_PASSWORD=super_secret_password DATABASE_NAME=example_db PORT=22 HOST=localhost Next, access the connection_config.py file in the config module and assign the environment variable names to the file variables: USER=\"USER\" PASSWORD=\"DB_PASSWORD\" DATABASE_NAME=\"DATABASE_NAME\" PORT=\"PORT\" HOST=\"HOST\" If the values of the environment variables are correct, the connection should work. See the section Testing the connection for more details to check your connection. Although this configuration may seem unnecessarily complicated, it has been implemented this way for two reasons: Isolated sensitive data : This configuration allows you to isolate any private or sensitive data, such as usernames or passwords. This way, this information is not recorded or written anywhere in the code, but is accessed through independently configured environment variables. It allows multiple environment variables to be configured simultaneously. For example, if we want to connect to a remote database, we can simply add new environment variables (without modifying the previous ones) that reflect this second connection: REMOTE_USER=me_myself REMOTE_DB_PASSWORD=an_other_super_secret_password REMOTE_DATABASE_NAME=example_db REMOTE_PORT=8085 REMOTE_HOST=remote_host Then, we only need to modify the connection_config.py file to update the connection parameters as follows: USER=\"REMOTE_USER\" PASSWORD=\"REMOTE_DB_PASSWORD\" DATABASE_NAME=\"REMOTE_DATABASE_NAME\" PORT=\"REMOTE_PORT\" HOST=\"REMOTE_HOST\" This way, we reference other environment variables (isolated from the code) that allow the connection to another database. Mapping types dictionaries Two main dictionaries are declared in this module: sql_type_mapping (dict) : Used to map from python datatypes to SLQ datatyes. This dictionary is used while creating the tables for the SQL database, and indicate what SQL datatype corresponds with each python datatype. If new datatypes are added to the database, make sure to update the new information here. Also, the values (SQL datatypes) can be modified to fit better the data that is going to be stored (e.g. the datatype 'category' is currently mapped to 'String(255)', but it can be changed to Strings(31) instead, to reduce memory consumption). py_type_mapping (dict) : Used to map from datatpyes as string to the dataype class itself (e.g. from 'int64' to int ) Note : This module is usually imported using the abbreviation con_config from config import connection_config as con_config new_var_config module During data preprocessing (see the data_processing_utils module section), some variables are added that do not appear in the initial row_data of the cohorts. These can be calculated or secondary variables, cohort indicators, etc. This module stores the data for these variables necessary to include them in the var_data object (refer to the cohort_utils module for more information), which is essential for the creation of the database. For each new variable, the following parameters corresponding to the columns of the var_data object need to be added. The column names (defined in the reference_names module ) are used for this purpose. This module is usually imported in the code with the alias rf . The parameters to be added are as follows: rf.VAR_ORIGINAL_NAME_COLUMN : The name of the variable in the original cohort (presumably the same as the final name of the variable since it is a new variable added 'after the fact'). rf.VAR_LIVERAIM_NAME_COLUMN : The final name of the variable in the DB (presumably the same as the original name of the variable since it is a new variable added 'after the fact'). rf.VAR_PANEL_COLUMN : The panel where the variable will appear (obsolete feature, this column should be removed from var_data and the code) rf.VAR_DATA_TYPE_COLUMN : The data type of the added variable. Ensure that it is a data type compatible with the program. rf.ORIGINAL_UNITS_COLUMN : The units of the variable in the original cohort (presumably the same as the final units since it is a new variable added 'after the fact'). rf.FINAL_UNITS_COLUMN : The units of the variable in the final database (presumably the same as the original units since it is a new variable added 'after the fact'). rf.VAR_DESCRIPTION_COLUMN : A brief description of the new variable. rf.VAR_CONVERSION_FACTOR_COLUMN : Conversion factor to change from the original units to the final units. Since it is a 'new' variable, the conversion factor should be 1. Note 1 : Some of these parameters may not be necessary (or simply do not apply to the variable being added). In that case, you can omit the declaration of that parameter/column, and it will appear in the var_data object as NaN . To facilitate use and simplify the code, these new variable parameters are not listed directly in the new_var_config.py file. Instead, they should be added as a dictionary following the format exemplified below: VAR_NAME_DATA_ROW = { rf.VAR_ORIGINAL_NAME_COLUMN: ['var_name'], rf.VAR_LIVERAIM_NAME_COLUMN: ['var_name'], rf.VAR_PANEL_COLUMN: ['blood_test'], rf.VAR_DATA_TYPE_COLUMN: ['double64[ns]'], rf.ORIGINAL_UNITS_COLUMN: ['mg/dl'], rf.FINAL_UNITS_COLUMN: ['mg/dl'], rf.VAR_DESCRIPTION_COLUMN: [\"A brief description of the new variable\"], rf.VAR_CONVERSION_FACTOR_COLUMN: [1] } Note 2 : Note that the values used to initialize the dictionary are declared as lists (i.e., enclosed in square brackets [] ). It is important to maintain this format for the code to function correctly (this is due to how pandas internally handles the transformation of dictionaries to DataFrames). Note 3 : This module is usually imported using the abbreviation new_vars : from config import new_var_config as new_vars new_levels_config module This module has a structure and function similar to the new_var_config module. In this case, only new categorical variables are included, and the file contains the data needed to update the level_data object. Note : To illustrate the addition of a categorical variable, we will use the status variable (already included in the module). This variable is of type string and has three levels: finished (coded as 0), ongoing (coded as 1), and withdrawn (coded as 2). The following describes what elements should be added to the new_levels_config.py file to include a new categorical variable: Level Codification Since the different levels of a variable are coded as integers (generally starting from 0), each level should be listed as a variable. These variables should have their corresponding code as the value. The naming convention followed is variable-name_level-name = code . For example, if we are adding the status variable (which has three levels), we should include: STATUS_FINISHED = '0' STATUS_ONGOING = '1' STATUS_WITHDRAWN = '2' Level Mapping Next, a dictionary must be created to map the levels of this variable, following the naming convention VARIABLE-NAME_LEVELS . In this case: STATUS_LEVELS = {STATUS_FINISHED: 'finished', STATUS_ONGOING: 'ongoing', STATUS_WITHDRAWN: 'withdrawn'} Creation of New level_data Rows Finally, a list of dictionaries is created (each dictionary corresponds to a row in level_data , where the key is the column name and the value is the value of that row for that column). This list will be imported wherever these metadata need to be included in the level_data object. (For more information on the structure of this object, you can refer to the Initial data and configuration data section). In summary, for each level, a row must be included in the level_data object. Each row should contain the corresponding information for each of the columns in that object (to do this, as in the var_data module, the reference_names module is used to reference the column names), specifically: rf.LEVEL_ORIGINAL_NAME_COLUMN : Column with the original cohort name of the variable to which the level belongs. rf.LEVEL_LIVERAIM_NAME_COLUMN : Column with the final name of the variable to which the level belongs. rf.LEVEL_ORIGINAL_VALUE_COLUMN : Value of the level in the original cohort (presumably the same as the final value since it is a variable added 'after the fact'). rf.LEVEL_LIVERAIM_VALUE_COLUMN : Value of the level in the final database (presumably the same as the original value since it is a variable added 'after the fact'). rf.LEVEL_DESCRIPTION_COLUMN : Description of the meaning of that level. rf.LEVEL_ORIGINAL_DATA_TYPE_COLUMN : Original data type of the variable. To create the list of dictionaries, the variable VARIABLE-NAME_LEVEL_DATA_ROW is created, which is initialized using list comprehension. Continuing with the example of the status variable and maintaining the naming convention, we would need to add these lines of code: STATUS_LEVEL_DATA_ROW = [{ rf.LEVEL_ORIGINAL_NAME_COLUMN: 'status', rf.LEVEL_LIVERAIM_NAME_COLUMN: 'status', rf.LEVEL_ORIGINAL_VALUE_COLUMN: key, rf.LEVEL_LIVERAIM_VALUE_COLUMN: key, rf.LEVEL_DESCRIPTION_COLUMN: value, rf.LEVEL_ORIGINAL_DATA_TYPE_COLUMN: 'str' } for key, value in STATUS_LEVELS.items()] Important Note : This code can be used as a template to add more levels/categorical variables. It is important to note that this list comprehension relies on the parameters previously defined in the STATUS_LEVELS dictionary, so if the steps described are followed, only the values of the keys original name , liveraim name , description , and data type need to be modified. Note : This module is usually imported using the abbreviation new_levels : from config import new_levels_config as new_levels reference_names module Note : It is recommended to read the section Initial data and configuration before continuing with this section. This module lists the variables that reference the column names of the three initial metadata files, namely: level_data var_data panel_metadata These names are widely used throughout the code. The variables follow a specific naming convention: the first part of the variable name refers to the file/object it belongs to, and the last part is COLUMN , indicating that the variable contains the name of a column from a .xlsx file (or a DataFrame). This helps differentiate them from similar variables. For instance, the variable that stores the name of the column in the var_data file containing the description of each variable is named VAR_DESCRIPTION_COLUMN . At the risk of being redundant, the variables defined in this module are listed below: var_data column variables Variables that reference the columns of the var_data file/object (note that each row corresponds to a variable): VAR_ORIGINAL_NAME_COLUMN : Name of the column where the variable's original name in the cohort is defined. VAR_LIVERAIM_NAME_COLUMN : Name of the column where the final name (in the common format) of the variable is defined. VAR_PANEL_COLUMN : Name of the column where the panel to which the variable belongs is defined. VAR_DATA_TYPE_COLUMN : Name of the column where the data type of the variable is defined. VAR_ORIGINAL_UNITS_COLUMN : Name of the column where the original units of the variable in the cohort are defined. VAR_DESCRIPTION_COLUMN : Name of the column where the variable is described. VAR_FINAL_UNITS_COLUMN : Name of the column where the final units of the variable in the final database are defined. VAR_LOWER_BOUND_COLUMN : Name of the column where the acceptable lower bound for the variable's values is defined. VAR_UPPER_BOUND_COLUMN : Name of the column where the acceptable upper bound for the variable's values is defined. VAR_CONVERSION_FACTOR_COLUMN : Name of the column where the conversion factor used to transform values from the original cohort to the final units is defined. level_data column variables Variables que referencian las columnas del archivo/objeto level_data (hay que recordar que cada fila corresponde a un nivel de una variable): LEVEL_ORIGINAL_NAME_COLUMN : Name of the column where the original name of the variable in the cohort is defined. LEVEL_LIVERAIM_NAME_COLUMN : Name of the column where the final name (in the common format) of the variable is defined. LEVEL_ORIGINAL_VALUE_COLUMN : Name of the column where the level value in the original cohort is defined. LEVEL_LIVERAIM_VALUE_COLUMN : Name of the column where the level value in the final database is defined. LEVEL_DESCRIPTION_COLUMN : Name of the column where the variable is described. LEVEL_ORIGINAL_DATA_TYPE_COLUMN : Name of the column where the data type of the variable in the original cohort is defined. panel_metadata column variables Variables that reference the columns of the panel_metadata file/object. This file is composed of different tabs, each describing a panel. Each row in each tab corresponds to a variable that should appear in the panel (whether in long or wide format): PANEL_VAR_NAME_COLUMN : Name of the column containing the variable names (in the common format) that will appear in the panel. PANEL_MELT_COLUMN : Name of the column that indicates whether the variable should appear in long or wide format. final_data column variables The following variables specify the names of some of the columns in the tables with the final data (the final DB). In particular, they describe the names of the columns in the 'long' format tables that will contain the variable name and its corresponding value. FINAL_DATA_VARIABLE_NAME : Name of the column in the final long-format tables where the variable names will appear. FINAL_DATA_VALUE_NAME : Name of the column in the final long-format tables where the variable values will appear. This module should only be changed if the column names in level_data , var_data and panel_metadata are changed. ids_to_drop.txt File Inside the configuration module directory, a file named ids_to_drop.txt can be placed (this file is not tracked by Git). It should contain a raw list of patient identifiers, one per line. Each patient listed in this file will be excluded from the data warehouse during data processing. This provides a simple way to remove specific patients from the data warehouse\u2014e.g., due to incorrect identification, misspelled IDs, duplicates, or ID collisions\u2014by adding them to this list.","title":"Configuration module"},{"location":"modules_documentation/configuration_module/#config-package-documentation","text":"The config module contains the parameters and variables required for the proper functioning of the code. These can be modified to adapt the execution to different environments, offer flexibility, and add future functionalities. This allows you to change the behavior of the program without needing to access and modify the source code. The module contains the following files: main_config.py : Main variables and parameters cohort_config.py : Cohort Class related parameters. connection_config.py : connection parameters. new_var_config.py : Definition of new or derived varibles. new_levels_config.py : Definition of the levels of the new/derived categorical variables. reference_names.py : Names of the columns of the innitial data files ( var_data , level_data and panel_metadata ) The following sections describe the variables contained in each of these files, their purpose, and possible modifications.","title":"config Package Documentation"},{"location":"modules_documentation/configuration_module/#main_config-module","text":"","title":"main_config module"},{"location":"modules_documentation/configuration_module/#general-paremeters","text":"EXECUTION_DATETIME (str) : DateTime of the execution. Used for debugging, logging and exportation purposes. NUMERIC_TYPES (list) : List of numeric python types compatibles with the databse. CONFIG_OBJ_NAMES (list) : List with the names of the configuration data objects in the code (such as var_data , level_data , etc.) BUILD_DUMMIES (bool) : Indicates wheter a dummy dataset of the execution must be build and exported to .feather files.","title":"General paremeters"},{"location":"modules_documentation/configuration_module/#quality-control-paramenters","text":"ALPHA (str) : Error threshold used during the numeric QC checks. EXPORT_QC_RECORD (bool) : With a value of True , it exports the data generated during the quality control process. For more details, see the section Quality Control Utils . With a value of False , quality control is still performed, but the generated data is not saved after execution (except for what is printed in the log). NAN_RECORD_EXPORT_FOLDER : ... QC_REPORT_FOLDER (str) : name of the folder taht centralizes the QC: it will contain the QC output and the code for processing this data and the rendered report. QC_DATA_FOLDER (str) : name of the folder where all the data generated during the quality control checks will be dumped (if EXPORT_QC_RECORD is set to true). This folder is itended to be a subdirectory fo the directory above QC_REPORT_FOLDER","title":"Quality control paramenters"},{"location":"modules_documentation/configuration_module/#variables-used-to-read-data-files-databases-var_data-level_data-panel_metadata","text":"The following variables are mainly used in the module file_reading_utils . PANEL_DATA_FILE_NAME (str) : Name of the file containing the panel data (should be an .xlsx file). CONFIG_DATA_METADATA_FILE_PATH (str) : Path of the file containing the metadata of the config data tables (i.e. data types of each column) that will be exported to the database. **** LIVERSCREEN_VAR_FILE_NAME (str) : Name of the file containing the var_data for the Liverscreen cohort (should be an .xlsx file). GLUCOFIB_VAR_FILE_NAME (str) : Name of the file containing the var_data for the Glucofib cohort (should be an .xlsx file). ALCOFIB_VAR_FILE_NAME (str) : Name of the file containing the var_data for the Alcofib cohort (should be an .xlsx file). DECIDE_VAR_FILE_NAME (str) : Name of the file containing the var_data for the Decide cohort (should be an .xlsx file). MARINA_VAR_FILE_NAME (str) : Name of the file containing the var_data for the Marina cohort (should be an .xlsx file). GALAALD_VAR_FILE_NAME (str) : Name of the file containing the var_data for the Galaald cohort (should be an .xlsx file). VAR_FILE_NAMES (dict) : Dictionary containing the cohort names (as defined in the cohort_config module) as keys, and the corresponding var_data file names as values (i.e., the names defined above). This dictionary should not be modified unless a new cohort is added: the dictionary is automatically generated using the other variables, ensuring it stays up to date. LIVERSCREEN_LEVEL_FILE_NAME (str) : Name of the file containing the level_data for the Liverscreen cohort (should be an .xlsx file). GLUCOFIB_LEVEL_FILE_NAME (str) : Name of the file containing the level_data for the Glucofib cohort (should be an .xlsx file). ALCOFIB_LEVEL_FILE_NAME (str) : Name of the file containing the level_data for the Alcofib cohort (should be an .xlsx file). DECIDE_LEVEL_FILE_NAME (str) : Name of the file containing the level_data for the Decide cohort (should be an .xlsx file). MARINA_LEVEL_FILE_NAME (str) : Name of the file containing the level_data for the Marina cohort (should be an .xlsx file). GALAALD_LEVEL_FILE_NAME (str) : Name of the file containing the level_data for the Galaald cohort (should be an .xlsx file). LEVEL_FILE_NAMES (dict) : Dictionary containing the cohort names (as defined in the cohort_config module) as keys, and the corresponding level_data file names as values (i.e., the names defined above). This dictionary should not be modified unless a new cohort is added: the dictionary is automatically generated using the other variables, ensuring it stays up to date. LIVERSCREEN_COMB_VARS_FILE_NAME (str) : Name of the file containing the var_comb_data for the Liverscreen cohort (should be an .json file). GLUCOFIB_COMB_VARS_FILE_NAME (str) : Name of the file containing the var_comb_data for the Glucofib cohort (should be an .json file). ALCOFIB_COMB_VARS_FILE_NAME (str) : Name of the file containing the var_comb_data for the Alcofib cohort (should be an .json file). MARINA1_COMB_VARS_FILE_NAME (str) : Name of the file containing the var_comb_data for the Marina cohort (should be an .json file). DECIDE_COMB_VARS_FILE_NAME (str) : Name of the file containing the var_comb_data for the Decide cohort (should be an .json file). GALAALD_COMB_VARS_FILE_NAME (str) : Name of the file containing the var_comb_data for the Galaald cohort (should be an .json file). COMB_VARS_FILE_NAMES (dict) : Dictionary containing the cohort names (as defined in the cohort_config module) as keys, and the corresponding comb_var_data file names as values (i.e., the names defined above). This dictionary should not be modified unless a new cohort is added: the dictionary is automatically generated using the other variables, ensuring it stays up to date. If you wish to use other var_data , level_data , or panel_metadata files (for testing, verification, etc.), you can modify the value of these variables to read those files. COMMON_DATA_FOLDER (str) : Name of the folder containing all the data. This folder is not tracked by git and must follow the structure described in Structure of data/cohort_name/ Directory . DATABASES_FOLDER (str) : Name of the folder, inside each cohort specific directory, that will contain the raw data recieved from the partners. READING_METHODS_DICT (dict) : Dictionary containing strings as keys that indicate different types of files and their assiciated reading method as the value. DATABASES_FORMAT (dict) : Dictionary containig the name of each cohort as key and the type of file of their raw databases file. It is used with the READING_METHODS_DICT to read the cohorts databases properly.","title":"Variables used to read data files (databases, var_data, level_data, panel_metadata)"},{"location":"modules_documentation/configuration_module/#variables-used-to-export-data","text":"EXPORT_FILES (bool) : With a value of True , it exports the final data (the data warehouse, with the panel structure described in ??? ) to .csv and .feather files. With a value of False , these files are not generated. For more details, see the section File Exporting Utils Module . CREATE_SQL_DB (bool) : Con un valor de True se exporta los datos a una base de datos MySQL (i.e. se crea el datawarehouse). Con un valor de False no se exportan los datos a MySQL. DATAWAREHOUSE_PATH (str) : Name of the folder where the data warehouse in .csv and .feather will be exported. ANALYSIS_REPORT_SCRIPT_PATH (str) : absolute path to the .Rmd script used to generate the initial descriptive analysis of the data, in case the results are exported to files. COLS_IN_PANELS (dict) : Dictionary mapping each panel name to the list of column names that should be exported for that panel in .feather/.csv files. Used to customize the structure of the data warehouse. COLS_IN_SQL_PANELS (dict) : Dictionary mapping each panel name to the list of column names that should be exported for that panel in the SQL DB. Used to customize the structure of the data warehouse. Note : This module is usually imported using the abreviation config : from config import main_config as config","title":"Variables used to export data"},{"location":"modules_documentation/configuration_module/#cohort_config-module","text":"This module contians variables related to the configuration and instantiation of each Cohort. ALCOFIB_NAME (str) : Name to be used to refer to the Alcofib cohort during the execution of the main code. GLUCOFIB_NAME (str) : Name to be used to refer to the Glucofib cohort during the execution of the main code. LIVERSCREEN_NAME (str) : Name to be used to refer to the Liverscreen cohort during the execution of the main code. DECIDE_NAME (str) : Name to be used to refer to the Decide cohort during the execution of the main code. MARINA1_NAME (str) : Name to be used to refer to the Marina cohort during the execution of the main code. LIVERAIM_NAME (str) : Name to be used to refer to the Liveraim merged database during the execution of the main code. GALAALD_NAME (str) : Name to be used to refer to the Galaald merged database during the execution of the main code. ALL_COHORTS (list) : List of all the cohort names. This variable is created using the variables defined above and should be only modified to add new elements (i.e. new cohorts). WP1_COHORT_NAME_LIST (list) : List of cohort names that will be used for the processing of WP1. This variable is created using the variables defined above. WP2_COHORT_NAME_LIST (list) : List of cohort names that will be used for the processing of WP2. This variable is created using the variables defined above. For each cohort, the following variables are also defined (for brevity, the term <COHORT_NAME> will be used to refer to the name of the specific cohort): COHOR<T_NAME>_INCL_DATE_VAR (str) : Original name of the 'inclusion date' variable. <COHORT_NAME>_AGE_VAR (str) : Original name of the 'age' variable. <COHORT_NAME>_ID_VAR (str) : Original name of the 'identifier' variable. <COHORT_NAME>_COHORT_STATUS (str) : Value of the categorical status variable. For more details about this variable, refer to the section on the new_levels_config module . COHORTS_DATA (dict) : A dictionary that contains the previously defined variables for each cohort. This dictionary is imported (instead of importing each variable individually) into the code, making it easier to access these variables. The dictionary keys are the names of each cohort, and the values are the corresponding variables for that cohort as described above. PERCENT_VARS (list) : List of variable names (using their common mapped name) that should be represented with percentage (%) units. This list is used during formatting and processing to handle inconsistencies in how these variables are expressed. Note : This module is usually imported using the abreviation cc : from config import cohort_config as cc","title":"cohort_config module"},{"location":"modules_documentation/configuration_module/#connection_config-module","text":"This module defines: Database connection parameters for MySQL. Dictionaries for mapping variable types across platforms/languages.","title":"connection_config module"},{"location":"modules_documentation/configuration_module/#connection-parameters","text":"It's important to understand how the connection parameters for the MySQL database are managed. To connect to the database (as explained in the MySQL connection configuration section), five parameters are required: USER : The username used to connect to the database (must have appropriate permissions). PASSWORD : The user's password to access the database. DATABASE_NAME : The name of the database to be accessed (the user must have permissions to access the database). PORT : The connection port. HOST : The hostname. If accessing a local database, this is usually localhost , while for remote access, it is typically the server name or IP address. Each of these parameters must be created as environment variables . These can be virtual or global environment variables, but they must be accessible through Python's os package. The names of these environment variables (which correspond to each of the connection parameters mentioned above) can be chosen at will. Once these variables are created, you need to modify the connection_config.py file, where the variables USER (str) , PASSWORD (str) , DATABASE_NAME (str) , PORT (str) , HOST (str) are listed. These variables should be assigned the name of the environment variable you want to use for that parameter . For example, suppose you want to connect to a database named example_db . First, you would create the following environment variables (ensuring that their values are correct to establish the connection properly): USER=me_myself DB_PASSWORD=super_secret_password DATABASE_NAME=example_db PORT=22 HOST=localhost Next, access the connection_config.py file in the config module and assign the environment variable names to the file variables: USER=\"USER\" PASSWORD=\"DB_PASSWORD\" DATABASE_NAME=\"DATABASE_NAME\" PORT=\"PORT\" HOST=\"HOST\" If the values of the environment variables are correct, the connection should work. See the section Testing the connection for more details to check your connection. Although this configuration may seem unnecessarily complicated, it has been implemented this way for two reasons: Isolated sensitive data : This configuration allows you to isolate any private or sensitive data, such as usernames or passwords. This way, this information is not recorded or written anywhere in the code, but is accessed through independently configured environment variables. It allows multiple environment variables to be configured simultaneously. For example, if we want to connect to a remote database, we can simply add new environment variables (without modifying the previous ones) that reflect this second connection: REMOTE_USER=me_myself REMOTE_DB_PASSWORD=an_other_super_secret_password REMOTE_DATABASE_NAME=example_db REMOTE_PORT=8085 REMOTE_HOST=remote_host Then, we only need to modify the connection_config.py file to update the connection parameters as follows: USER=\"REMOTE_USER\" PASSWORD=\"REMOTE_DB_PASSWORD\" DATABASE_NAME=\"REMOTE_DATABASE_NAME\" PORT=\"REMOTE_PORT\" HOST=\"REMOTE_HOST\" This way, we reference other environment variables (isolated from the code) that allow the connection to another database.","title":"Connection Parameters"},{"location":"modules_documentation/configuration_module/#mapping-types-dictionaries","text":"Two main dictionaries are declared in this module: sql_type_mapping (dict) : Used to map from python datatypes to SLQ datatyes. This dictionary is used while creating the tables for the SQL database, and indicate what SQL datatype corresponds with each python datatype. If new datatypes are added to the database, make sure to update the new information here. Also, the values (SQL datatypes) can be modified to fit better the data that is going to be stored (e.g. the datatype 'category' is currently mapped to 'String(255)', but it can be changed to Strings(31) instead, to reduce memory consumption). py_type_mapping (dict) : Used to map from datatpyes as string to the dataype class itself (e.g. from 'int64' to int ) Note : This module is usually imported using the abbreviation con_config from config import connection_config as con_config","title":"Mapping types dictionaries"},{"location":"modules_documentation/configuration_module/#new_var_config-module","text":"During data preprocessing (see the data_processing_utils module section), some variables are added that do not appear in the initial row_data of the cohorts. These can be calculated or secondary variables, cohort indicators, etc. This module stores the data for these variables necessary to include them in the var_data object (refer to the cohort_utils module for more information), which is essential for the creation of the database. For each new variable, the following parameters corresponding to the columns of the var_data object need to be added. The column names (defined in the reference_names module ) are used for this purpose. This module is usually imported in the code with the alias rf . The parameters to be added are as follows: rf.VAR_ORIGINAL_NAME_COLUMN : The name of the variable in the original cohort (presumably the same as the final name of the variable since it is a new variable added 'after the fact'). rf.VAR_LIVERAIM_NAME_COLUMN : The final name of the variable in the DB (presumably the same as the original name of the variable since it is a new variable added 'after the fact'). rf.VAR_PANEL_COLUMN : The panel where the variable will appear (obsolete feature, this column should be removed from var_data and the code) rf.VAR_DATA_TYPE_COLUMN : The data type of the added variable. Ensure that it is a data type compatible with the program. rf.ORIGINAL_UNITS_COLUMN : The units of the variable in the original cohort (presumably the same as the final units since it is a new variable added 'after the fact'). rf.FINAL_UNITS_COLUMN : The units of the variable in the final database (presumably the same as the original units since it is a new variable added 'after the fact'). rf.VAR_DESCRIPTION_COLUMN : A brief description of the new variable. rf.VAR_CONVERSION_FACTOR_COLUMN : Conversion factor to change from the original units to the final units. Since it is a 'new' variable, the conversion factor should be 1. Note 1 : Some of these parameters may not be necessary (or simply do not apply to the variable being added). In that case, you can omit the declaration of that parameter/column, and it will appear in the var_data object as NaN . To facilitate use and simplify the code, these new variable parameters are not listed directly in the new_var_config.py file. Instead, they should be added as a dictionary following the format exemplified below: VAR_NAME_DATA_ROW = { rf.VAR_ORIGINAL_NAME_COLUMN: ['var_name'], rf.VAR_LIVERAIM_NAME_COLUMN: ['var_name'], rf.VAR_PANEL_COLUMN: ['blood_test'], rf.VAR_DATA_TYPE_COLUMN: ['double64[ns]'], rf.ORIGINAL_UNITS_COLUMN: ['mg/dl'], rf.FINAL_UNITS_COLUMN: ['mg/dl'], rf.VAR_DESCRIPTION_COLUMN: [\"A brief description of the new variable\"], rf.VAR_CONVERSION_FACTOR_COLUMN: [1] } Note 2 : Note that the values used to initialize the dictionary are declared as lists (i.e., enclosed in square brackets [] ). It is important to maintain this format for the code to function correctly (this is due to how pandas internally handles the transformation of dictionaries to DataFrames). Note 3 : This module is usually imported using the abbreviation new_vars : from config import new_var_config as new_vars","title":"new_var_config module"},{"location":"modules_documentation/configuration_module/#new_levels_config-module","text":"This module has a structure and function similar to the new_var_config module. In this case, only new categorical variables are included, and the file contains the data needed to update the level_data object. Note : To illustrate the addition of a categorical variable, we will use the status variable (already included in the module). This variable is of type string and has three levels: finished (coded as 0), ongoing (coded as 1), and withdrawn (coded as 2). The following describes what elements should be added to the new_levels_config.py file to include a new categorical variable:","title":"new_levels_config module"},{"location":"modules_documentation/configuration_module/#level-codification","text":"Since the different levels of a variable are coded as integers (generally starting from 0), each level should be listed as a variable. These variables should have their corresponding code as the value. The naming convention followed is variable-name_level-name = code . For example, if we are adding the status variable (which has three levels), we should include: STATUS_FINISHED = '0' STATUS_ONGOING = '1' STATUS_WITHDRAWN = '2'","title":"Level Codification"},{"location":"modules_documentation/configuration_module/#level-mapping","text":"Next, a dictionary must be created to map the levels of this variable, following the naming convention VARIABLE-NAME_LEVELS . In this case: STATUS_LEVELS = {STATUS_FINISHED: 'finished', STATUS_ONGOING: 'ongoing', STATUS_WITHDRAWN: 'withdrawn'}","title":"Level Mapping"},{"location":"modules_documentation/configuration_module/#creation-of-new-level_data-rows","text":"Finally, a list of dictionaries is created (each dictionary corresponds to a row in level_data , where the key is the column name and the value is the value of that row for that column). This list will be imported wherever these metadata need to be included in the level_data object. (For more information on the structure of this object, you can refer to the Initial data and configuration data section). In summary, for each level, a row must be included in the level_data object. Each row should contain the corresponding information for each of the columns in that object (to do this, as in the var_data module, the reference_names module is used to reference the column names), specifically: rf.LEVEL_ORIGINAL_NAME_COLUMN : Column with the original cohort name of the variable to which the level belongs. rf.LEVEL_LIVERAIM_NAME_COLUMN : Column with the final name of the variable to which the level belongs. rf.LEVEL_ORIGINAL_VALUE_COLUMN : Value of the level in the original cohort (presumably the same as the final value since it is a variable added 'after the fact'). rf.LEVEL_LIVERAIM_VALUE_COLUMN : Value of the level in the final database (presumably the same as the original value since it is a variable added 'after the fact'). rf.LEVEL_DESCRIPTION_COLUMN : Description of the meaning of that level. rf.LEVEL_ORIGINAL_DATA_TYPE_COLUMN : Original data type of the variable. To create the list of dictionaries, the variable VARIABLE-NAME_LEVEL_DATA_ROW is created, which is initialized using list comprehension. Continuing with the example of the status variable and maintaining the naming convention, we would need to add these lines of code: STATUS_LEVEL_DATA_ROW = [{ rf.LEVEL_ORIGINAL_NAME_COLUMN: 'status', rf.LEVEL_LIVERAIM_NAME_COLUMN: 'status', rf.LEVEL_ORIGINAL_VALUE_COLUMN: key, rf.LEVEL_LIVERAIM_VALUE_COLUMN: key, rf.LEVEL_DESCRIPTION_COLUMN: value, rf.LEVEL_ORIGINAL_DATA_TYPE_COLUMN: 'str' } for key, value in STATUS_LEVELS.items()] Important Note : This code can be used as a template to add more levels/categorical variables. It is important to note that this list comprehension relies on the parameters previously defined in the STATUS_LEVELS dictionary, so if the steps described are followed, only the values of the keys original name , liveraim name , description , and data type need to be modified. Note : This module is usually imported using the abbreviation new_levels : from config import new_levels_config as new_levels","title":"Creation of New level_data Rows"},{"location":"modules_documentation/configuration_module/#reference_names-module","text":"Note : It is recommended to read the section Initial data and configuration before continuing with this section. This module lists the variables that reference the column names of the three initial metadata files, namely: level_data var_data panel_metadata These names are widely used throughout the code. The variables follow a specific naming convention: the first part of the variable name refers to the file/object it belongs to, and the last part is COLUMN , indicating that the variable contains the name of a column from a .xlsx file (or a DataFrame). This helps differentiate them from similar variables. For instance, the variable that stores the name of the column in the var_data file containing the description of each variable is named VAR_DESCRIPTION_COLUMN . At the risk of being redundant, the variables defined in this module are listed below:","title":"reference_names module"},{"location":"modules_documentation/configuration_module/#var_data-column-variables","text":"Variables that reference the columns of the var_data file/object (note that each row corresponds to a variable): VAR_ORIGINAL_NAME_COLUMN : Name of the column where the variable's original name in the cohort is defined. VAR_LIVERAIM_NAME_COLUMN : Name of the column where the final name (in the common format) of the variable is defined. VAR_PANEL_COLUMN : Name of the column where the panel to which the variable belongs is defined. VAR_DATA_TYPE_COLUMN : Name of the column where the data type of the variable is defined. VAR_ORIGINAL_UNITS_COLUMN : Name of the column where the original units of the variable in the cohort are defined. VAR_DESCRIPTION_COLUMN : Name of the column where the variable is described. VAR_FINAL_UNITS_COLUMN : Name of the column where the final units of the variable in the final database are defined. VAR_LOWER_BOUND_COLUMN : Name of the column where the acceptable lower bound for the variable's values is defined. VAR_UPPER_BOUND_COLUMN : Name of the column where the acceptable upper bound for the variable's values is defined. VAR_CONVERSION_FACTOR_COLUMN : Name of the column where the conversion factor used to transform values from the original cohort to the final units is defined.","title":"var_data column variables"},{"location":"modules_documentation/configuration_module/#level_data-column-variables","text":"Variables que referencian las columnas del archivo/objeto level_data (hay que recordar que cada fila corresponde a un nivel de una variable): LEVEL_ORIGINAL_NAME_COLUMN : Name of the column where the original name of the variable in the cohort is defined. LEVEL_LIVERAIM_NAME_COLUMN : Name of the column where the final name (in the common format) of the variable is defined. LEVEL_ORIGINAL_VALUE_COLUMN : Name of the column where the level value in the original cohort is defined. LEVEL_LIVERAIM_VALUE_COLUMN : Name of the column where the level value in the final database is defined. LEVEL_DESCRIPTION_COLUMN : Name of the column where the variable is described. LEVEL_ORIGINAL_DATA_TYPE_COLUMN : Name of the column where the data type of the variable in the original cohort is defined.","title":"level_data column variables"},{"location":"modules_documentation/configuration_module/#panel_metadata-column-variables","text":"Variables that reference the columns of the panel_metadata file/object. This file is composed of different tabs, each describing a panel. Each row in each tab corresponds to a variable that should appear in the panel (whether in long or wide format): PANEL_VAR_NAME_COLUMN : Name of the column containing the variable names (in the common format) that will appear in the panel. PANEL_MELT_COLUMN : Name of the column that indicates whether the variable should appear in long or wide format.","title":"panel_metadata column variables"},{"location":"modules_documentation/configuration_module/#final_data-column-variables","text":"The following variables specify the names of some of the columns in the tables with the final data (the final DB). In particular, they describe the names of the columns in the 'long' format tables that will contain the variable name and its corresponding value. FINAL_DATA_VARIABLE_NAME : Name of the column in the final long-format tables where the variable names will appear. FINAL_DATA_VALUE_NAME : Name of the column in the final long-format tables where the variable values will appear. This module should only be changed if the column names in level_data , var_data and panel_metadata are changed.","title":"final_data column variables"},{"location":"modules_documentation/configuration_module/#ids_to_droptxt-file","text":"Inside the configuration module directory, a file named ids_to_drop.txt can be placed (this file is not tracked by Git). It should contain a raw list of patient identifiers, one per line. Each patient listed in this file will be excluded from the data warehouse during data processing. This provides a simple way to remove specific patients from the data warehouse\u2014e.g., due to incorrect identification, misspelled IDs, duplicates, or ID collisions\u2014by adding them to this list.","title":"ids_to_drop.txt File"},{"location":"modules_documentation/data_processing_utils_doc/","text":"data_processing_utils Documentation Overview The goal of this module is to prepare the raw data received from various partners so that it can later be homogenized (using the cohort_utils module) and exported (using the file_exporting_utils and sql_exporting_utils modules). This module performs the following tasks: Add calculated variables based on the original data. This includes updating the objects with configuration data, such as var_data and level_data with new information, which will be used in the cohort_utils module to create the Cohort object. Examples of added variables include: status , birth_date , exit_date , cohort , etc. Combine variables that measure the same quantity but in different units to reduce the number of missing values in a variable. Merge versions of the same database to obtain each patient's data in its latest version. This includes identifying patients with status='withdrawn' , i.e., patients who do not appear in the latest version but do appear in previous versions. Check compatibility of the databases from different cohorts. General Structure The module consists of the following key components: DataPreprocessor : A class responsible for centralizing the preprocessing described above. It is designed to be flexible and customizable based on the specifics of each cohort's data: for each cohort, DataPreprocessor allows for personalized transformations through cohort-specific methods that apply the appropriate preprocessing steps. VarCombiner : A helper class responsible for combining the variables specified in the var_comb_data dictionary. It is used within DataPreprocessor during data preprocessing. Auxiliary functions : helper functions to check and transform data structure. Functions and Classes In this sections are described the classes and functions defined in the module: Functions : check_df_formats(df_dict: dict[str, pd.DataFrame]) -> bool Checks if the format (variable names and data types) of all DataFrames in df_dict are compatible for concatenation. It ensures that the columns are in the same order (this can be modified by sorting the columns) and checks if the data types are consistent. Arguments : df_dict (dict[str, pd.DataFrame]): A dictionary containing DataFrames to check for compatibility. Returns : bool : Returns True if the DataFrames are compatible for concatenation, even if data types differ (a warning is logged). Returns False if the formats are incompatible. logs In check_df_formats function: If there are discrepancies between the names of the columns of the different versions of the same databse, an error is logged. If there are discrepancies between the data types of the same column between the different versions of the same database, a warning is logged. Classes : Note : Due to how pandas handles DataFrame modifications, changes made to the data attribute ( data ) (generally modifying and/or adding columns) occur 'inplace'. Therefore, when passing the DataFrame by reference into a method or function, any modifications to it will be reflected in the original, and returning the object is not necessary. However, changes made to var_data and level_data objects do require returning the modified object in the function. Class DataPreprocessor Description This class centralizes data preprocessing tasks for cohort databases. It processes the dataframe columns/variables, updates configuration data ( var_data and level_data ), and handles cohort-specific transformations. In particular, it first merges all the database versions into a single DataFrame, keeping the data of each patient from the las version. This single dataframe will be stored in the attribute data . Then new calculated/secundary variables are added to it, such as status , exit_date , etc. Moreover, it combines variables using the var_combine_data atrribute (if it exists). During the preprocessing, the configuration data ( var_data and level_data dataframes) are updated to fit the structure of data . Once the preprocessing is finished, data , var_data and level_data is returned so they can be used, for example, to instantiate a Cohort . Attributes cohort_databases (dict[str, pd.DataFrame]): A dictionary containing the cohort databases (all available versions). The keys are the version dates, and the values are the corresponding pandas DataFrames. (See the format specified in Structure of data/cohort_name/databases/ Directory ) var_data (pd.DataFrame): A DataFrame with variable configuration data, which contains metadata and other information about the variables in the cohort. level_data (pd.DataFrame): A DataFrame with level configuration data. This stores information about the levels or categories within variables (e.g., categorical variables). cohort_name (str): The name of the cohort being processed. This name is used to identify the cohort in various operations, including variable configuration updates and transformations. data (pd.DataFrame): A pandas DataFrame that stores the merged cohort data. This attribute is generated after merging all versions of the cohort databases and applying preprocessing steps. comb_var_data (pd.DataFrame, optional): A DataFrame containing metadata about the variables that need to be combined. If this attribute exists, it will be used to combine variables in the cohort during preprocessing. Methods __init__(self, cohort_databases: dict[str, pd.DataFrame], var_data: pd.DataFrame, level_data: pd.DataFrame, cohort_name: str, **kwargs) : Initializes the DataPreprocessor class with cohort databases (all the avaliable versions) and the configuration data objects: var_data and level_data . Also sets up cohort-specific information like the name of some relevant variables in the specific cohort: age, inclusion date, and ID variables. Arguments : cohort_databases (dict[str, pd.DataFrame]): A dictionary containing cohort databases. Keys should be the version date (ckeck the format specified in Structure of data/cohort_name/databases/ Directory ) var_data (pd.DataFrame): A DataFrame with variable configuration data. level_data (pd.DataFrame): A DataFrame with level configuration data. cohort_name (str): The name of the cohort being processed. set_new_variables(self) -> tuple : Sets new variables for the cohort, including status, birth date, exit date, and cohort name. Updates the variable and level configuration objects accordingly. To do so, it calls an specific method for each variable, such as: set_status_metadata , set_birth_date_columns , etc. Returns tuple : var_data : the var_data object updated with the information about the new variables added. level_data : the level_data object updated with the information aobut the new variable levels added. preprocess(self) -> self : Centralizes the preprocessing logic for different cohorts by identifying the cohort and applying the corresponding cohort-specific method. Returns : self (PreProcessor) : Returns the updated class itself. preprocess_alcofib(self) : Preprocesses the ALCOFIB cohort data by applying transformations specific to that cohort. preprocess_glucofib(self) : Preprocesses the GLUCOFIB cohort data by applying transformations specific to that cohort. preprocess_liverscreen(self) : Preprocesses the LIVERSCREEN cohort data. If a combination dictionary ( comb_var_dict ) is provided, it combines variables according to that configuration. preprocess_decide(self) : Preprocesses the DECIDE cohort data by applying transformations specific to that cohort. preprocess_marina1(self) : Preprocesses the MARINA1 cohort data by applying transformations specific to that cohort. set_status_metadata(self) : Method to update the configuration objects var_data and level_data with the new variable status . This variable is added as a column to the _data attribute when the method get_last_version is called. In addition, it updates both the var_data and level_data DataFrames with the new configuration data. This method uses the configuration defined in config.new_var_config and config.new_levels_config to update the var_data and level_data DataFrames. Returns : var_data (pd.DataFrame): The updated DataFrame with metadata for the status variable. level_data (pd.DataFrame): The updated DataFrame with metadata for the status levels. set_birth_date_column(self) -> pd.DataFrame : Calculates and adds a \" birth_date \" column to the dataset based on the inclusion date and age varaibles (specified during the instantiation of the class). Updates the var_data DataFrame with the new configuration data for the birth date variable. If first checks if both columns, age and inclusion date, are in the raw data dataframe. Then, for those patients with values in both columns, computes the birth_date . Returns : var_data (pd.DataFrame): The updated DataFrame with configuration data for the birth date variable. set_exit_date_column(self) : Adds the \" exit_date \" column to the dataset, which represents the date a patient leaves the cohort or the end of the study. If the study is ongoing, the current date is used. Returns : var_data (pd.DataFrame): The updated DataFrame with metadata for the exit date variable. set_cohort_column(self) : Adds the \"cohort\" column to the dataset, representing the cohort name for each patient. Uses the cohort_name attribute to do so. Updates the var_data DataFrame with metadata for this variable. Returns : var_data (pd.DataFrame): The updated DataFrame with metadata for the cohort variable. get_last_version(self, cohort_databases: dict[str, pd.DataFrame]) -> pd.DataFrame : Merges all versions of the cohort databases and assigns the patient status based on whether they appear in the latest version. Removes duplicates, keeping only the oldest data for each patient. If incompatibilites are found that might prevent the databases to merge properly (this is checked using the function check_df_formats ) only the last version is returned and a warning is logged. Arguments : cohort_databases (dict[str, pd.DataFrame]): A dictionary of cohort database versions. Returns : pd.DataFrame : The final version of the cohort data, with the status variable updated. get_data(self) Returns all processed data, including the main dataset, var_data , and level_data . This is a sort of a getter for all the processed data. Returns : tuple : A tuple containing the main dataset, var_data , and level_data DataFrames. Logs In __init__ : An error is logged if problems ocurre while calling configuration variables in the config package. In set_status_metadata : An error is logged if any issues occur while updating var_data or level_data , particularly when concatenating the new rows in the former DataFrame. In set_birth_date_column : If any of the variables used to compute birth_date (i.e. age or inclusion date) is not a subset of the columns in data , an error is logged. In any errors occurs while computing and assigning the new colimn, an error message is logged. In set_exit_date_column : If there is an error while assigning the correct exit_date based on the status, an error is logged. In get_last_version : If an error occurs while assigning the column date_version, an error is logged. If an error occurs while merging al the version databases, an error is logged. If an error occurs while dropping the duplicates in the merged data and assigning the status column, an error message is logged. Class VarCombiner Description Combines variables in a DataFrame based on a reference dictionary ( comb_var_dict ). This class helps merge variables and apply conversion factors as needed. In this version, the main functionality of the class is to combine variables of the same magnitude that are expressed in different units so the count of missings is reduced. To do so, it uses the var_combine_dict , where the variables (and their conversion factor) to be combined are specified. For more information about the structure of this dictionary check the section comb_var_data file . The process of combining the data works as follows (it's important to understand the structure of comb_var_dict ). For each key-value pair in comb_var_dict with the following structure: <final_variable>: {<variable_to_combine_1>: <conversion_factor_1>, <variable_to_combine_2>: <conversion_factor_2>, ...} The process performs the following steps: If final_variable is also one of the variables_to_combine_n and its value is not NaN , that value is directly used as the final_variable value. If final_variable is not present among the variables_to_combine_n or its value is NaN , the method iterates through all the variables_to_combine_n . It searches for the first variable with a numeric value, applies the corresponding conversion factor, and assigns the result as the value for final_variable . If no numeric values are found among the variables_to_combine_n , the value for final_variable is set to NaN . Attributes comb_var_dict (dict) : A dictionary where keys are reference variables (the name of the final variable to be obtained) and values are dictionaries with variables to combine and their respective conversion factors. ref_vars (list[str]) : A list of reference variables extracted from the keys of comb_var_dict , i.e. the final variables to be obtained. original_data (pd.DataFrame) : A DataFrame containing the original data before any transformations. This is the raw data from each cohort. transformed_data (pd.DataFrame) : A copy of the original DataFrame that will be transformed by combining variables. Methods __init__(self, comb_var_dict: dict, df: pd.DataFrame) Initializes the VarCombiner class with a dictionary of reference variables and conversion factors, along with the DataFrame to be transformed. It creates the attributes described above, setting transformed_data as a copy of original_data (i.e. a copy of df ). Arguments : comb_var_dict (dict): A dictionary with reference variables and their corresponding conversion factors. df (pd.DataFrame): The DataFrame containing the data to be combined. _combine_vars(self, row: pd.Series, ref_var: str, var_conv_factors: dict) -> float Combines variables within a row by applying conversion factors if the reference variable is missing (using the algorithm described in the class's Description ). This function is intended to be used inside the apply method from pandas, specifically in the method _combine_data . Arguments : row (pd.Series): A row of the DataFrame. ref_var (str): The reference variable to check. var_conv_factors (dict): A dictionary where keys are alternative variables and values are conversion factors. Returns : float : The combined value for the reference variable or NaN if no valid variables are found. _combine_data(self, df:pd.DataFrame, comb_var_dict: dict) -> pd.DataFrame: Applies the variable combination logic (i.e. the _combine_vars method) across all the variables in the comb_var_dict keys. Arguments : df (pd.DataFrame): The DataFrame to apply the variable combination to. comb_var_dict (dict): The dictionary containing the reference variables and their corresponding combining variables and factors. Returns : pd.DataFrame : The DataFrame with the combined variables. combine_all_data(self) -> 'VarCombiner' Combines all data according to the comb_var_dict and returns the updated instance of the VarCombiner class. Returns : VarCombiner : The updated instance of the class with combined data. Logs In original_data.setter : If the variables to combine are not a subset of the columns of the data dataframe ( original_data ), an error message is logged. Class ConfigDataManager Description This class is responsible for transforming and combining configuration objects defined in the CONFIG_OBJ_NAMES list ( var_data , level_data , comb_var_data , and panel_data ) from the main_config . The class prepares the configuration data in a format suitable for export to SQL by converting the objects into pandas DataFrames and merging them as needed. Attributes config_obj_names (list[str]): List of the names of the configuration objects that need to be transformed (e.g., var_data , level_data , comb_var_data , panel_data ). config_data (dict): Dictionary holding the configuration data used to create the database. The keys correspond to each type of configuration data, and the values are dictionaries that store configuration data for each cohort. combined_config_data (dict[str, pd.DataFrame]): Dictionary that holds combined configuration data for each configuration type in DataFrame format, suitable for export. Methods __init__(self, all_data: dict = None) : Initializes the ConfigDataManager class by setting up the structure for storing configuration data and optionally rearranging the configuration data if provided through all_data . Arguments : all_data (dict, optional): A dictionary containing all the configuration data. If provided, it will be used to populate the config_data attribute. instantiate_config_data(self) -> None : Initializes an empty dictionary structure for storing configuration data based on config_obj_names . append_config_data(self, cohort: str = None, **kwargs) -> None : Appends configuration data for a specific cohort and configuration type to the config_data attribute. To add data for a particular cohort, you must pass the cohort's name using the cohort parameter. The configuration data is then appended by specifying the configuration type as a keyword argument (e.g., var_data=var_data_object), where the argument's name corresponds to the type of configuration data (such as var_data, level_data), and its value contains the actual data for that cohort. Arguments : cohort (str): The name of the cohort to which the configuration data belongs. **kwargs : Keyword arguments where keys represent the configuration type (e.g., var_data , level_data , comb_var_data , panel_data ) and values are the configuration data to append. Notes : Only configuration types present in config_obj_names will be accepted. If a key in kwargs is not in config_obj_names , a warning is logged. set_config_data_with_all_data(self, all_data: dict) -> ConfigDataManager : Rearranges the configuration data from all_data into the config_data attribute, creating a key for each config data type. Arguments : all_data (dict): Dictionary containing all the configuration data needed to create the database. Returns : ConfigDataManager : The instance with the updated config_data . _combine_config_data_dfs(self, config_data_dict: dict) -> pd.DataFrame : Takes the dataframes containing configuration data for each cohort, adds the 'Cohort' column, and concatenates them into a single DataFrame. The result is a single datafarme containing all the config data related to a specific config data type. Arguments : config_data_dict (dict): Dictionary with all the dataframes from each cohort that need to be merged. Returns : pd.DataFrame : Concatenation of all the configuration dataframes for a specific configuration data type. _comb_var_data_to_df(self) -> dict[str, pd.DataFrame] : Converts the comb_var_data dictionaries in config_data into pandas DataFrames and returns a dictionary where keys are the cohort names and values are the dataframes (result of the conversion of the dictionaries). Returns : dict[str, pd.DataFrame] : A dictionary with comb_var_data transformed into DataFrames. combine_panel_data(self) -> pd.DataFrame : Combines the panel_data for all cohorts into a single DataFrame, adding a 'panel' column to indicate the corresponding panel that the data is referring to. Returns : pd.DataFrame : The combined panel_data for all cohorts. If no panel_data is present, a warning is logged, and the method returns None . _dict_to_df(self, config_dict: dict) -> pd.DataFrame : Converts a dictionary with the structure of comb_var_data into a pandas DataFrame. Arguments : config_dict (dict): Dictionary to be transformed into a DataFrame. Returns : pd.DataFrame : DataFrame containing the configuration data. combine_config_data(self) -> ConfigDataManager : Iterates through all the configuration object types in config_data and applies the _combine_config_data_dfs method to combine the data for each type. It then calls transform_all_config_data_datatypes to ensure the columns have the proper data types for exporting to a MySQL database. Returns : ConfigDataManager : The instance with the combined configuration data in combined_config_data . _combine_config_obj_type(self, config_obj_type: str, config_data: dict) : Combines configuration data for individual cohorts based on the configuration object type (e.g., var_data , level_data , comb_var_data , panel_data ). Arguments : config_obj_type (str): The type of configuration object to combine. config_data (dict): Dictionary containing the configuration data for the specified type. transform_column_data_types(self, config_data: pd.DataFrame, data_types_map: dict) -> pd.DataFrame : Maps the datatypes of config_data columns following the mapping in data_types_map and returns the updated DataFrame. Arguments : config_data (pd.DataFrame): The DataFrame whose column types need to be converted. data_types_map (dict): A mapping of column names to their corresponding data types. Returns : pd.DataFrame : The DataFrame with updated column types. transform_all_config_data_datatypes(self) -> None : Iterates through all the configuration data DataFrames and applies transform_column_data_types to ensure the columns have the correct data types for export to MySQL. Notes : The method reads the datatype mapping from a metadata file and applies it to the combined configuration data. General Operation The data preprocessing is carried out iteratively for each cohort. Once completed, the resulting objects are used to create a Cohort class, responsible for homogenizing and formatting the data (see the section cohort_utils ). The data preprocessing begins with the instantiation of the DataPreprocessor class, which requires as parameters (for each cohort) the var_data , level_data , and databases objects. Additionally, the var_comb_data parameter can be used to combine variables following the specifications of this object. These objects are read using the DataReader class from the file_reading_utils module. For more details on the structure of these objects, see the section Initial data and configuration data . During preprocessing, three main modifications will be made: Merging of the different versions of the database. Duplicates will be removed, resulting in a single dataframe with the latest version of data for each patient. New variables will be added , and the configuration data will be updated accordingly. Variables will be combined following the specifications of var_comb_data . The preprocessing pipeline follows these steps: The necessary attributes for preprocessing are initialized. See the __init__ method explained in the section class DataPreprocessor /methods . In particular, the attributes cohort_name , age_column , inclusion_date_column , id_var , and status are initialized, which represent the name of the variable referring to age, inclusion date, etc. Additionally, if the comb_var_dict argument is included, it is also initialized as an attribute. The preprocess method is called, centralizing the data processing. In this method, the cohort being processed is identified (using the cohort_name attribute), and the method responsible for applying the specific transformations for that cohort is called. Within the methods specific to each cohort, named according to the rule preprocess_cohort-name , the specific modifications for each cohort are applied. Generally, in the current version, the set_new_variables method is called (for all cohorts). This method performs the following actions: Calls the get_last_version and set_status_metadata methods. The first merges the dataframes from each version of the same cohort into a single dataframe, which is stored in the data attribute. It adds the version_date variable and removes duplicates for each patient, keeping only the data from the latest version for each patient (the available data, i.e., the databases, are wide-format dataframes, so ther e should be no repetitions in the id_var column). Once duplicates are removed, this method creates the status column/variable, assigning the values ongoing , finished , or withdrawn based on the version_date variable. The second method updates the var_data and level_data dataframes with the new information about the status variable. This method returns the updated dataframes, which are stored in the corresponding attributes. Calls the set_birth_date_column , set_exit_date_column , and set_cohort_column methods. These create the respective variables in the data dataframe, and each returns the updated var_data and level_data dataframes, ensuring the respective attributes are updated with the new information. If the DataPreprocessor class is initialized with the var_comb_data parameter, the combination of the variables specified in this object is carried out. For this, the VarCombiner class is instantiated, which requires var_comb_data and data as parameters. For more information on the variable combination process, see the sections Class VarCombiner /description or Combination of variables . In this case, the transformed_data attribute of VarCombiner is used (after the combination is applied) to update data . Once all preprocessing is completed, the preprocessed data can be accessed using the get_data method. Usage Example from data_processing_utils import DataPreprocessor # Initialize DataPreprocessor with required arguments preprocessor = DataPreprocessor(cohort_databases, var_data, level_data, cohort_name) # Preprocess the data preprocessor.preprocess() # Get the final processed data processed_data, var_data, level_data = preprocessor.get_data() Usage in main execution","title":"Preprocessing utils"},{"location":"modules_documentation/data_processing_utils_doc/#data_processing_utils-documentation","text":"","title":"data_processing_utils Documentation"},{"location":"modules_documentation/data_processing_utils_doc/#overview","text":"The goal of this module is to prepare the raw data received from various partners so that it can later be homogenized (using the cohort_utils module) and exported (using the file_exporting_utils and sql_exporting_utils modules). This module performs the following tasks: Add calculated variables based on the original data. This includes updating the objects with configuration data, such as var_data and level_data with new information, which will be used in the cohort_utils module to create the Cohort object. Examples of added variables include: status , birth_date , exit_date , cohort , etc. Combine variables that measure the same quantity but in different units to reduce the number of missing values in a variable. Merge versions of the same database to obtain each patient's data in its latest version. This includes identifying patients with status='withdrawn' , i.e., patients who do not appear in the latest version but do appear in previous versions. Check compatibility of the databases from different cohorts.","title":"Overview"},{"location":"modules_documentation/data_processing_utils_doc/#general-structure","text":"The module consists of the following key components: DataPreprocessor : A class responsible for centralizing the preprocessing described above. It is designed to be flexible and customizable based on the specifics of each cohort's data: for each cohort, DataPreprocessor allows for personalized transformations through cohort-specific methods that apply the appropriate preprocessing steps. VarCombiner : A helper class responsible for combining the variables specified in the var_comb_data dictionary. It is used within DataPreprocessor during data preprocessing. Auxiliary functions : helper functions to check and transform data structure.","title":"General Structure"},{"location":"modules_documentation/data_processing_utils_doc/#functions-and-classes","text":"In this sections are described the classes and functions defined in the module:","title":"Functions and Classes"},{"location":"modules_documentation/data_processing_utils_doc/#functions","text":"","title":"Functions:"},{"location":"modules_documentation/data_processing_utils_doc/#check_df_formatsdf_dict-dictstr-pddataframe-bool","text":"Checks if the format (variable names and data types) of all DataFrames in df_dict are compatible for concatenation. It ensures that the columns are in the same order (this can be modified by sorting the columns) and checks if the data types are consistent. Arguments : df_dict (dict[str, pd.DataFrame]): A dictionary containing DataFrames to check for compatibility. Returns : bool : Returns True if the DataFrames are compatible for concatenation, even if data types differ (a warning is logged). Returns False if the formats are incompatible.","title":"check_df_formats(df_dict: dict[str, pd.DataFrame]) -&gt; bool"},{"location":"modules_documentation/data_processing_utils_doc/#logs","text":"In check_df_formats function: If there are discrepancies between the names of the columns of the different versions of the same databse, an error is logged. If there are discrepancies between the data types of the same column between the different versions of the same database, a warning is logged.","title":"logs"},{"location":"modules_documentation/data_processing_utils_doc/#classes","text":"Note : Due to how pandas handles DataFrame modifications, changes made to the data attribute ( data ) (generally modifying and/or adding columns) occur 'inplace'. Therefore, when passing the DataFrame by reference into a method or function, any modifications to it will be reflected in the original, and returning the object is not necessary. However, changes made to var_data and level_data objects do require returning the modified object in the function.","title":"Classes:"},{"location":"modules_documentation/data_processing_utils_doc/#class-datapreprocessor","text":"","title":"Class DataPreprocessor"},{"location":"modules_documentation/data_processing_utils_doc/#description","text":"This class centralizes data preprocessing tasks for cohort databases. It processes the dataframe columns/variables, updates configuration data ( var_data and level_data ), and handles cohort-specific transformations. In particular, it first merges all the database versions into a single DataFrame, keeping the data of each patient from the las version. This single dataframe will be stored in the attribute data . Then new calculated/secundary variables are added to it, such as status , exit_date , etc. Moreover, it combines variables using the var_combine_data atrribute (if it exists). During the preprocessing, the configuration data ( var_data and level_data dataframes) are updated to fit the structure of data . Once the preprocessing is finished, data , var_data and level_data is returned so they can be used, for example, to instantiate a Cohort .","title":"Description"},{"location":"modules_documentation/data_processing_utils_doc/#attributes","text":"cohort_databases (dict[str, pd.DataFrame]): A dictionary containing the cohort databases (all available versions). The keys are the version dates, and the values are the corresponding pandas DataFrames. (See the format specified in Structure of data/cohort_name/databases/ Directory ) var_data (pd.DataFrame): A DataFrame with variable configuration data, which contains metadata and other information about the variables in the cohort. level_data (pd.DataFrame): A DataFrame with level configuration data. This stores information about the levels or categories within variables (e.g., categorical variables). cohort_name (str): The name of the cohort being processed. This name is used to identify the cohort in various operations, including variable configuration updates and transformations. data (pd.DataFrame): A pandas DataFrame that stores the merged cohort data. This attribute is generated after merging all versions of the cohort databases and applying preprocessing steps. comb_var_data (pd.DataFrame, optional): A DataFrame containing metadata about the variables that need to be combined. If this attribute exists, it will be used to combine variables in the cohort during preprocessing.","title":"Attributes"},{"location":"modules_documentation/data_processing_utils_doc/#methods","text":"__init__(self, cohort_databases: dict[str, pd.DataFrame], var_data: pd.DataFrame, level_data: pd.DataFrame, cohort_name: str, **kwargs) : Initializes the DataPreprocessor class with cohort databases (all the avaliable versions) and the configuration data objects: var_data and level_data . Also sets up cohort-specific information like the name of some relevant variables in the specific cohort: age, inclusion date, and ID variables. Arguments : cohort_databases (dict[str, pd.DataFrame]): A dictionary containing cohort databases. Keys should be the version date (ckeck the format specified in Structure of data/cohort_name/databases/ Directory ) var_data (pd.DataFrame): A DataFrame with variable configuration data. level_data (pd.DataFrame): A DataFrame with level configuration data. cohort_name (str): The name of the cohort being processed. set_new_variables(self) -> tuple : Sets new variables for the cohort, including status, birth date, exit date, and cohort name. Updates the variable and level configuration objects accordingly. To do so, it calls an specific method for each variable, such as: set_status_metadata , set_birth_date_columns , etc. Returns tuple : var_data : the var_data object updated with the information about the new variables added. level_data : the level_data object updated with the information aobut the new variable levels added. preprocess(self) -> self : Centralizes the preprocessing logic for different cohorts by identifying the cohort and applying the corresponding cohort-specific method. Returns : self (PreProcessor) : Returns the updated class itself. preprocess_alcofib(self) : Preprocesses the ALCOFIB cohort data by applying transformations specific to that cohort. preprocess_glucofib(self) : Preprocesses the GLUCOFIB cohort data by applying transformations specific to that cohort. preprocess_liverscreen(self) : Preprocesses the LIVERSCREEN cohort data. If a combination dictionary ( comb_var_dict ) is provided, it combines variables according to that configuration. preprocess_decide(self) : Preprocesses the DECIDE cohort data by applying transformations specific to that cohort. preprocess_marina1(self) : Preprocesses the MARINA1 cohort data by applying transformations specific to that cohort. set_status_metadata(self) : Method to update the configuration objects var_data and level_data with the new variable status . This variable is added as a column to the _data attribute when the method get_last_version is called. In addition, it updates both the var_data and level_data DataFrames with the new configuration data. This method uses the configuration defined in config.new_var_config and config.new_levels_config to update the var_data and level_data DataFrames. Returns : var_data (pd.DataFrame): The updated DataFrame with metadata for the status variable. level_data (pd.DataFrame): The updated DataFrame with metadata for the status levels. set_birth_date_column(self) -> pd.DataFrame : Calculates and adds a \" birth_date \" column to the dataset based on the inclusion date and age varaibles (specified during the instantiation of the class). Updates the var_data DataFrame with the new configuration data for the birth date variable. If first checks if both columns, age and inclusion date, are in the raw data dataframe. Then, for those patients with values in both columns, computes the birth_date . Returns : var_data (pd.DataFrame): The updated DataFrame with configuration data for the birth date variable. set_exit_date_column(self) : Adds the \" exit_date \" column to the dataset, which represents the date a patient leaves the cohort or the end of the study. If the study is ongoing, the current date is used. Returns : var_data (pd.DataFrame): The updated DataFrame with metadata for the exit date variable. set_cohort_column(self) : Adds the \"cohort\" column to the dataset, representing the cohort name for each patient. Uses the cohort_name attribute to do so. Updates the var_data DataFrame with metadata for this variable. Returns : var_data (pd.DataFrame): The updated DataFrame with metadata for the cohort variable. get_last_version(self, cohort_databases: dict[str, pd.DataFrame]) -> pd.DataFrame : Merges all versions of the cohort databases and assigns the patient status based on whether they appear in the latest version. Removes duplicates, keeping only the oldest data for each patient. If incompatibilites are found that might prevent the databases to merge properly (this is checked using the function check_df_formats ) only the last version is returned and a warning is logged. Arguments : cohort_databases (dict[str, pd.DataFrame]): A dictionary of cohort database versions. Returns : pd.DataFrame : The final version of the cohort data, with the status variable updated. get_data(self) Returns all processed data, including the main dataset, var_data , and level_data . This is a sort of a getter for all the processed data. Returns : tuple : A tuple containing the main dataset, var_data , and level_data DataFrames.","title":"Methods"},{"location":"modules_documentation/data_processing_utils_doc/#logs_1","text":"In __init__ : An error is logged if problems ocurre while calling configuration variables in the config package. In set_status_metadata : An error is logged if any issues occur while updating var_data or level_data , particularly when concatenating the new rows in the former DataFrame. In set_birth_date_column : If any of the variables used to compute birth_date (i.e. age or inclusion date) is not a subset of the columns in data , an error is logged. In any errors occurs while computing and assigning the new colimn, an error message is logged. In set_exit_date_column : If there is an error while assigning the correct exit_date based on the status, an error is logged. In get_last_version : If an error occurs while assigning the column date_version, an error is logged. If an error occurs while merging al the version databases, an error is logged. If an error occurs while dropping the duplicates in the merged data and assigning the status column, an error message is logged.","title":"Logs"},{"location":"modules_documentation/data_processing_utils_doc/#class-varcombiner","text":"","title":"Class VarCombiner"},{"location":"modules_documentation/data_processing_utils_doc/#description_1","text":"Combines variables in a DataFrame based on a reference dictionary ( comb_var_dict ). This class helps merge variables and apply conversion factors as needed. In this version, the main functionality of the class is to combine variables of the same magnitude that are expressed in different units so the count of missings is reduced. To do so, it uses the var_combine_dict , where the variables (and their conversion factor) to be combined are specified. For more information about the structure of this dictionary check the section comb_var_data file . The process of combining the data works as follows (it's important to understand the structure of comb_var_dict ). For each key-value pair in comb_var_dict with the following structure: <final_variable>: {<variable_to_combine_1>: <conversion_factor_1>, <variable_to_combine_2>: <conversion_factor_2>, ...} The process performs the following steps: If final_variable is also one of the variables_to_combine_n and its value is not NaN , that value is directly used as the final_variable value. If final_variable is not present among the variables_to_combine_n or its value is NaN , the method iterates through all the variables_to_combine_n . It searches for the first variable with a numeric value, applies the corresponding conversion factor, and assigns the result as the value for final_variable . If no numeric values are found among the variables_to_combine_n , the value for final_variable is set to NaN .","title":"Description"},{"location":"modules_documentation/data_processing_utils_doc/#attributes_1","text":"comb_var_dict (dict) : A dictionary where keys are reference variables (the name of the final variable to be obtained) and values are dictionaries with variables to combine and their respective conversion factors. ref_vars (list[str]) : A list of reference variables extracted from the keys of comb_var_dict , i.e. the final variables to be obtained. original_data (pd.DataFrame) : A DataFrame containing the original data before any transformations. This is the raw data from each cohort. transformed_data (pd.DataFrame) : A copy of the original DataFrame that will be transformed by combining variables.","title":"Attributes"},{"location":"modules_documentation/data_processing_utils_doc/#methods_1","text":"__init__(self, comb_var_dict: dict, df: pd.DataFrame) Initializes the VarCombiner class with a dictionary of reference variables and conversion factors, along with the DataFrame to be transformed. It creates the attributes described above, setting transformed_data as a copy of original_data (i.e. a copy of df ). Arguments : comb_var_dict (dict): A dictionary with reference variables and their corresponding conversion factors. df (pd.DataFrame): The DataFrame containing the data to be combined. _combine_vars(self, row: pd.Series, ref_var: str, var_conv_factors: dict) -> float Combines variables within a row by applying conversion factors if the reference variable is missing (using the algorithm described in the class's Description ). This function is intended to be used inside the apply method from pandas, specifically in the method _combine_data . Arguments : row (pd.Series): A row of the DataFrame. ref_var (str): The reference variable to check. var_conv_factors (dict): A dictionary where keys are alternative variables and values are conversion factors. Returns : float : The combined value for the reference variable or NaN if no valid variables are found. _combine_data(self, df:pd.DataFrame, comb_var_dict: dict) -> pd.DataFrame: Applies the variable combination logic (i.e. the _combine_vars method) across all the variables in the comb_var_dict keys. Arguments : df (pd.DataFrame): The DataFrame to apply the variable combination to. comb_var_dict (dict): The dictionary containing the reference variables and their corresponding combining variables and factors. Returns : pd.DataFrame : The DataFrame with the combined variables. combine_all_data(self) -> 'VarCombiner' Combines all data according to the comb_var_dict and returns the updated instance of the VarCombiner class. Returns : VarCombiner : The updated instance of the class with combined data.","title":"Methods"},{"location":"modules_documentation/data_processing_utils_doc/#logs_2","text":"In original_data.setter : If the variables to combine are not a subset of the columns of the data dataframe ( original_data ), an error message is logged.","title":"Logs"},{"location":"modules_documentation/data_processing_utils_doc/#class-configdatamanager","text":"","title":"Class ConfigDataManager"},{"location":"modules_documentation/data_processing_utils_doc/#description_2","text":"This class is responsible for transforming and combining configuration objects defined in the CONFIG_OBJ_NAMES list ( var_data , level_data , comb_var_data , and panel_data ) from the main_config . The class prepares the configuration data in a format suitable for export to SQL by converting the objects into pandas DataFrames and merging them as needed.","title":"Description"},{"location":"modules_documentation/data_processing_utils_doc/#attributes_2","text":"config_obj_names (list[str]): List of the names of the configuration objects that need to be transformed (e.g., var_data , level_data , comb_var_data , panel_data ). config_data (dict): Dictionary holding the configuration data used to create the database. The keys correspond to each type of configuration data, and the values are dictionaries that store configuration data for each cohort. combined_config_data (dict[str, pd.DataFrame]): Dictionary that holds combined configuration data for each configuration type in DataFrame format, suitable for export.","title":"Attributes"},{"location":"modules_documentation/data_processing_utils_doc/#methods_2","text":"__init__(self, all_data: dict = None) : Initializes the ConfigDataManager class by setting up the structure for storing configuration data and optionally rearranging the configuration data if provided through all_data . Arguments : all_data (dict, optional): A dictionary containing all the configuration data. If provided, it will be used to populate the config_data attribute. instantiate_config_data(self) -> None : Initializes an empty dictionary structure for storing configuration data based on config_obj_names . append_config_data(self, cohort: str = None, **kwargs) -> None : Appends configuration data for a specific cohort and configuration type to the config_data attribute. To add data for a particular cohort, you must pass the cohort's name using the cohort parameter. The configuration data is then appended by specifying the configuration type as a keyword argument (e.g., var_data=var_data_object), where the argument's name corresponds to the type of configuration data (such as var_data, level_data), and its value contains the actual data for that cohort. Arguments : cohort (str): The name of the cohort to which the configuration data belongs. **kwargs : Keyword arguments where keys represent the configuration type (e.g., var_data , level_data , comb_var_data , panel_data ) and values are the configuration data to append. Notes : Only configuration types present in config_obj_names will be accepted. If a key in kwargs is not in config_obj_names , a warning is logged. set_config_data_with_all_data(self, all_data: dict) -> ConfigDataManager : Rearranges the configuration data from all_data into the config_data attribute, creating a key for each config data type. Arguments : all_data (dict): Dictionary containing all the configuration data needed to create the database. Returns : ConfigDataManager : The instance with the updated config_data . _combine_config_data_dfs(self, config_data_dict: dict) -> pd.DataFrame : Takes the dataframes containing configuration data for each cohort, adds the 'Cohort' column, and concatenates them into a single DataFrame. The result is a single datafarme containing all the config data related to a specific config data type. Arguments : config_data_dict (dict): Dictionary with all the dataframes from each cohort that need to be merged. Returns : pd.DataFrame : Concatenation of all the configuration dataframes for a specific configuration data type. _comb_var_data_to_df(self) -> dict[str, pd.DataFrame] : Converts the comb_var_data dictionaries in config_data into pandas DataFrames and returns a dictionary where keys are the cohort names and values are the dataframes (result of the conversion of the dictionaries). Returns : dict[str, pd.DataFrame] : A dictionary with comb_var_data transformed into DataFrames. combine_panel_data(self) -> pd.DataFrame : Combines the panel_data for all cohorts into a single DataFrame, adding a 'panel' column to indicate the corresponding panel that the data is referring to. Returns : pd.DataFrame : The combined panel_data for all cohorts. If no panel_data is present, a warning is logged, and the method returns None . _dict_to_df(self, config_dict: dict) -> pd.DataFrame : Converts a dictionary with the structure of comb_var_data into a pandas DataFrame. Arguments : config_dict (dict): Dictionary to be transformed into a DataFrame. Returns : pd.DataFrame : DataFrame containing the configuration data. combine_config_data(self) -> ConfigDataManager : Iterates through all the configuration object types in config_data and applies the _combine_config_data_dfs method to combine the data for each type. It then calls transform_all_config_data_datatypes to ensure the columns have the proper data types for exporting to a MySQL database. Returns : ConfigDataManager : The instance with the combined configuration data in combined_config_data . _combine_config_obj_type(self, config_obj_type: str, config_data: dict) : Combines configuration data for individual cohorts based on the configuration object type (e.g., var_data , level_data , comb_var_data , panel_data ). Arguments : config_obj_type (str): The type of configuration object to combine. config_data (dict): Dictionary containing the configuration data for the specified type. transform_column_data_types(self, config_data: pd.DataFrame, data_types_map: dict) -> pd.DataFrame : Maps the datatypes of config_data columns following the mapping in data_types_map and returns the updated DataFrame. Arguments : config_data (pd.DataFrame): The DataFrame whose column types need to be converted. data_types_map (dict): A mapping of column names to their corresponding data types. Returns : pd.DataFrame : The DataFrame with updated column types. transform_all_config_data_datatypes(self) -> None : Iterates through all the configuration data DataFrames and applies transform_column_data_types to ensure the columns have the correct data types for export to MySQL. Notes : The method reads the datatype mapping from a metadata file and applies it to the combined configuration data.","title":"Methods"},{"location":"modules_documentation/data_processing_utils_doc/#general-operation","text":"The data preprocessing is carried out iteratively for each cohort. Once completed, the resulting objects are used to create a Cohort class, responsible for homogenizing and formatting the data (see the section cohort_utils ). The data preprocessing begins with the instantiation of the DataPreprocessor class, which requires as parameters (for each cohort) the var_data , level_data , and databases objects. Additionally, the var_comb_data parameter can be used to combine variables following the specifications of this object. These objects are read using the DataReader class from the file_reading_utils module. For more details on the structure of these objects, see the section Initial data and configuration data . During preprocessing, three main modifications will be made: Merging of the different versions of the database. Duplicates will be removed, resulting in a single dataframe with the latest version of data for each patient. New variables will be added , and the configuration data will be updated accordingly. Variables will be combined following the specifications of var_comb_data . The preprocessing pipeline follows these steps: The necessary attributes for preprocessing are initialized. See the __init__ method explained in the section class DataPreprocessor /methods . In particular, the attributes cohort_name , age_column , inclusion_date_column , id_var , and status are initialized, which represent the name of the variable referring to age, inclusion date, etc. Additionally, if the comb_var_dict argument is included, it is also initialized as an attribute. The preprocess method is called, centralizing the data processing. In this method, the cohort being processed is identified (using the cohort_name attribute), and the method responsible for applying the specific transformations for that cohort is called. Within the methods specific to each cohort, named according to the rule preprocess_cohort-name , the specific modifications for each cohort are applied. Generally, in the current version, the set_new_variables method is called (for all cohorts). This method performs the following actions: Calls the get_last_version and set_status_metadata methods. The first merges the dataframes from each version of the same cohort into a single dataframe, which is stored in the data attribute. It adds the version_date variable and removes duplicates for each patient, keeping only the data from the latest version for each patient (the available data, i.e., the databases, are wide-format dataframes, so ther e should be no repetitions in the id_var column). Once duplicates are removed, this method creates the status column/variable, assigning the values ongoing , finished , or withdrawn based on the version_date variable. The second method updates the var_data and level_data dataframes with the new information about the status variable. This method returns the updated dataframes, which are stored in the corresponding attributes. Calls the set_birth_date_column , set_exit_date_column , and set_cohort_column methods. These create the respective variables in the data dataframe, and each returns the updated var_data and level_data dataframes, ensuring the respective attributes are updated with the new information. If the DataPreprocessor class is initialized with the var_comb_data parameter, the combination of the variables specified in this object is carried out. For this, the VarCombiner class is instantiated, which requires var_comb_data and data as parameters. For more information on the variable combination process, see the sections Class VarCombiner /description or Combination of variables . In this case, the transformed_data attribute of VarCombiner is used (after the combination is applied) to update data . Once all preprocessing is completed, the preprocessed data can be accessed using the get_data method.","title":"General Operation"},{"location":"modules_documentation/data_processing_utils_doc/#usage-example","text":"from data_processing_utils import DataPreprocessor # Initialize DataPreprocessor with required arguments preprocessor = DataPreprocessor(cohort_databases, var_data, level_data, cohort_name) # Preprocess the data preprocessor.preprocess() # Get the final processed data processed_data, var_data, level_data = preprocessor.get_data()","title":"Usage Example"},{"location":"modules_documentation/data_processing_utils_doc/#usage-in-main-execution","text":"","title":"Usage in main execution"},{"location":"modules_documentation/file_exporting_utils_doc/","text":"file_exporting_utils Documentation Overview The file_exporting_utils module is responsible for exporting data produced during the execution of the main program into various file formats, such as .csv and .feather . It ensures that the exported files are saved within a structured directory system, which includes a timestamp-based execution folder and separate subfolders for each file format. This module facilitates the following functionalities: + Export to CSV : Exports pandas DataFrames into .csv files. + Export to Feather : Exports pandas DataFrames into .feather files. + Dynamic directory creation : Creates structured directories based on the execution time to organize the exported files. The FileExporter class is used to centralize all export operations, handling the creation of necessary subdirectories, file formatting, and logging potential issues during the export process. This class is initialized with a common root folder and dynamically creates subdirectories for each export type. General Structure The module includes the definition of the main class resposible of managing the exportation: FileExporter : This is the main class of the module and is responsible for handling data exports to both CSV and Feather formats. It also manages the creation of subdirectories based on the current execution time and logs any errors that occur during the export process. When the class is instantiated, it autoatically creates the directories needed for the exportations. Class Description FileExporter Description: The FileExporter class is designed to export pandas DataFrames to .csv and .feather formats, ensuring the exported files are organized into separate directories for each execution. It dynamically creates directories for each type of export (CSV and Feather) based on the execution time and logs the export process. Attributes: common_export_folder (str) : The root directory where all exported files will be saved. execution_time (str) : A timestamp indicating when the export was initiated, based on the current execution time. execution_export_folder (str) : A folder specific to the current execution, where all export files will be stored. feather_folder (str) : Subfolder where .feather files will be saved. csv_folder (str) : Subfolder where .csv files will be saved. Methods: __init__(self, common_export_folder: str) -> None : Initializes the FileExporter object and creates the necessary export folders. Arguments : common_export_folder (str): The root directory where export files will be stored. _create_folder(self, folder: str) -> None : Creates a folder in the file system. Logs an error if the folder could not be created. Arguments : folder (str): The path of the folder to create. _create_csv_exports_folder(self, parent_folder: str) -> str : Creates the subfolder for storing CSV files within the specified parent folder. Arguments : parent_folder (str): The directory where the CSV subfolder will be created. Returns : str : The path to the created CSV folder. _create_feather_exports_folder(self, parent_folder: str) -> str : Creates the subfolder for storing Feather files within the specified parent folder. Arguments : parent_folder (str): The directory where the Feather subfolder will be created. Returns : str : The path to the created Feather folder. export_to_csv(self, df: pd.DataFrame, df_name: str = \"Unspecified file\") -> None : Exports the given pandas DataFrame to a CSV file in the CSV export subfolder. Arguments : df (pandas.DataFrame): The DataFrame to export. df_name (str): The name of the file (without extension). Defaults to \"Unspecified file\". Returns : None export_to_feather(self, df: pd.DataFrame, df_name: str = \"Unspecified file\") -> None : Exports the given pandas DataFrame to a Feather file in the Feather export subfolder. Arguments : df (pandas.DataFrame): The DataFrame to export. df_name (str): The name of the file (without extension). Defaults to \"Unspecified file\". Returns : None Logs The FileExporter class logs various events during the execution of its methods: Folder creation : Logs whether folders were successfully created or not. CSV and Feather exports : Logs success messages when DataFrames are exported correctly. Logs error messages if any issues arise during the export process. Usage Example The following is an example of how to use the FileExporter class: from file_exporter import FileExporter import pandas as pd # Example DataFrame df = pd.DataFrame({ 'col1': [1, 2, 3], 'col2': [4, 5, 6] }) # Create a FileExporter object exporter = FileExporter(common_export_folder=\"/path/to/export/folder\") # Export the DataFrame to CSV exporter.export_to_csv(df, df_name=\"example_data\") # Export the DataFrame to Feather exporter.export_to_feather(df, df_name=\"example_data\") In this example, the FileExporter is instantiated with a root export folder. When the class is isntantiated, the directories are automatically created. The DataFrame is then exported to both CSV and Feather formats. Output Description The FileExporter class creates the following directory structure for each execution in the main directory: <common_export_folder>/ execution_<timestamp>/ csv_files/ LIVERAIM_DATA_<file_name>.csv feather_files/ LIVERAIM_DATA_<file_name>.feather Each execution will create a new folder using the timestamp to store the exported files. The subfolders csv_files and feather_files contain the respective exported data formats. LIVERAIM_DATA_example_data.csv LIVERAIM_DATA_example_data.feather These files will be named based on the prefix LIVERAIM_DATA followed by the user-provided file name. In particular, for the main.py execution, the file names would be the panel names. File names would be, for example: LIVERAIM_DATA_population.csv LIVERAIM_DATA_population.feather During the main execution, the name common_export_folder is imported from the variable DATAWAREHOUSE_PATH , declared in the main_config module . Error Handling The FileExporter handles common export errors such as: FileNotFoundError : If the directory does not exist or cannot be accessed. PermissionError : If there are insufficient permissions to write to the directory. OSError : General errors related to file system operations. These errors are logged, and the process continues without crashing the program.","title":"File Exporting utils"},{"location":"modules_documentation/file_exporting_utils_doc/#file_exporting_utils-documentation","text":"","title":"file_exporting_utils Documentation"},{"location":"modules_documentation/file_exporting_utils_doc/#overview","text":"The file_exporting_utils module is responsible for exporting data produced during the execution of the main program into various file formats, such as .csv and .feather . It ensures that the exported files are saved within a structured directory system, which includes a timestamp-based execution folder and separate subfolders for each file format. This module facilitates the following functionalities: + Export to CSV : Exports pandas DataFrames into .csv files. + Export to Feather : Exports pandas DataFrames into .feather files. + Dynamic directory creation : Creates structured directories based on the execution time to organize the exported files. The FileExporter class is used to centralize all export operations, handling the creation of necessary subdirectories, file formatting, and logging potential issues during the export process. This class is initialized with a common root folder and dynamically creates subdirectories for each export type.","title":"Overview"},{"location":"modules_documentation/file_exporting_utils_doc/#general-structure","text":"The module includes the definition of the main class resposible of managing the exportation: FileExporter : This is the main class of the module and is responsible for handling data exports to both CSV and Feather formats. It also manages the creation of subdirectories based on the current execution time and logs any errors that occur during the export process. When the class is instantiated, it autoatically creates the directories needed for the exportations.","title":"General Structure"},{"location":"modules_documentation/file_exporting_utils_doc/#class-description","text":"","title":"Class Description"},{"location":"modules_documentation/file_exporting_utils_doc/#fileexporter","text":"","title":"FileExporter"},{"location":"modules_documentation/file_exporting_utils_doc/#description","text":"The FileExporter class is designed to export pandas DataFrames to .csv and .feather formats, ensuring the exported files are organized into separate directories for each execution. It dynamically creates directories for each type of export (CSV and Feather) based on the execution time and logs the export process.","title":"Description:"},{"location":"modules_documentation/file_exporting_utils_doc/#attributes","text":"common_export_folder (str) : The root directory where all exported files will be saved. execution_time (str) : A timestamp indicating when the export was initiated, based on the current execution time. execution_export_folder (str) : A folder specific to the current execution, where all export files will be stored. feather_folder (str) : Subfolder where .feather files will be saved. csv_folder (str) : Subfolder where .csv files will be saved.","title":"Attributes:"},{"location":"modules_documentation/file_exporting_utils_doc/#methods","text":"__init__(self, common_export_folder: str) -> None : Initializes the FileExporter object and creates the necessary export folders. Arguments : common_export_folder (str): The root directory where export files will be stored. _create_folder(self, folder: str) -> None : Creates a folder in the file system. Logs an error if the folder could not be created. Arguments : folder (str): The path of the folder to create. _create_csv_exports_folder(self, parent_folder: str) -> str : Creates the subfolder for storing CSV files within the specified parent folder. Arguments : parent_folder (str): The directory where the CSV subfolder will be created. Returns : str : The path to the created CSV folder. _create_feather_exports_folder(self, parent_folder: str) -> str : Creates the subfolder for storing Feather files within the specified parent folder. Arguments : parent_folder (str): The directory where the Feather subfolder will be created. Returns : str : The path to the created Feather folder. export_to_csv(self, df: pd.DataFrame, df_name: str = \"Unspecified file\") -> None : Exports the given pandas DataFrame to a CSV file in the CSV export subfolder. Arguments : df (pandas.DataFrame): The DataFrame to export. df_name (str): The name of the file (without extension). Defaults to \"Unspecified file\". Returns : None export_to_feather(self, df: pd.DataFrame, df_name: str = \"Unspecified file\") -> None : Exports the given pandas DataFrame to a Feather file in the Feather export subfolder. Arguments : df (pandas.DataFrame): The DataFrame to export. df_name (str): The name of the file (without extension). Defaults to \"Unspecified file\". Returns : None","title":"Methods:"},{"location":"modules_documentation/file_exporting_utils_doc/#logs","text":"The FileExporter class logs various events during the execution of its methods: Folder creation : Logs whether folders were successfully created or not. CSV and Feather exports : Logs success messages when DataFrames are exported correctly. Logs error messages if any issues arise during the export process.","title":"Logs"},{"location":"modules_documentation/file_exporting_utils_doc/#usage-example","text":"The following is an example of how to use the FileExporter class: from file_exporter import FileExporter import pandas as pd # Example DataFrame df = pd.DataFrame({ 'col1': [1, 2, 3], 'col2': [4, 5, 6] }) # Create a FileExporter object exporter = FileExporter(common_export_folder=\"/path/to/export/folder\") # Export the DataFrame to CSV exporter.export_to_csv(df, df_name=\"example_data\") # Export the DataFrame to Feather exporter.export_to_feather(df, df_name=\"example_data\") In this example, the FileExporter is instantiated with a root export folder. When the class is isntantiated, the directories are automatically created. The DataFrame is then exported to both CSV and Feather formats.","title":"Usage Example"},{"location":"modules_documentation/file_exporting_utils_doc/#output-description","text":"The FileExporter class creates the following directory structure for each execution in the main directory: <common_export_folder>/ execution_<timestamp>/ csv_files/ LIVERAIM_DATA_<file_name>.csv feather_files/ LIVERAIM_DATA_<file_name>.feather Each execution will create a new folder using the timestamp to store the exported files. The subfolders csv_files and feather_files contain the respective exported data formats. LIVERAIM_DATA_example_data.csv LIVERAIM_DATA_example_data.feather These files will be named based on the prefix LIVERAIM_DATA followed by the user-provided file name. In particular, for the main.py execution, the file names would be the panel names. File names would be, for example: LIVERAIM_DATA_population.csv LIVERAIM_DATA_population.feather During the main execution, the name common_export_folder is imported from the variable DATAWAREHOUSE_PATH , declared in the main_config module .","title":"Output Description"},{"location":"modules_documentation/file_exporting_utils_doc/#error-handling","text":"The FileExporter handles common export errors such as: FileNotFoundError : If the directory does not exist or cannot be accessed. PermissionError : If there are insufficient permissions to write to the directory. OSError : General errors related to file system operations. These errors are logged, and the process continues without crashing the program.","title":"Error Handling"},{"location":"modules_documentation/file_reading_utils_doc/","text":"file_reading_utils Documentation Overview The file_reading_utils module is responsible for reading various types of data files (databases and metadata) required for building the data warehouse. The primary class in this module is DataReader , which centralizes the reading process for all relevant files, including raw cohort data, variable metadata ( var_data ), level metadata ( level_data ), panel metadata ( panel_data ) and metadata to combine variables ( comb_var_data ). This module is sensitive to the structure of the directory where the data is stored, and it also includes the function hash_file , which calculates the hash of a file. This can be useful for logging purposes, as it helps trace the files used during data reading. General Structure The module contains the following key components: DataReader : The main class used for reading raw cohort data and metadata, managing logging, and ensuring the directory structure is correct. hash_file : A utility function for generating the hash of a file. stringfy : A helper function to convert a list of dictionaries into a formatted string for logging purposes. Class Description Class DataReader Description: The DataReader class centralizes the process of reading various data files required for creating a data warehouse. This includes reading raw cohort data, variable and level metadata, panel data and combination variable data. It manages logging, error handling, and ensures proper file access and reading. Attributes: reading_methods_dict (dict) : Contains the methods for reading different types of data (e.g., .csv , .dta , .xlsx ). databases_format_dict (dict) : Specifies the format of the raw databases for each cohort, used in conjunction with reading_methods_dict to read cohort data. common_data_folder (str) : The directory where all the data files to be read are stored. databases_folder (str) : The subfolder where the raw data for each cohort is stored. level_file_names (dict) : A dictionary containing the filenames of the level data for each cohort. var_file_names (dict) : A dictionary containing the filenames of the variable data for each cohort. comb_vars_file_names (dict) : A dictionary containing the filenames of the combined variables data for each cohort. panel_file_name (str) : The filename for the panel data. cohorts (list[str]) : A list containing the names of the cohorts to be read. reading_exceptions (tuple) : A tuple of common exceptions that may arise while reading files. all_data (dict) : A dictionary containing all the data required to create the data warehouse. read_files_log (dict) : A dictionary containing the filenames and their hashes for logging purposes. Methods: __init__(self, cohort_names_list: list) -> None : Initializes the DataReader with a list of cohort names and sets up the necessary attributes and configurations. Arguments : cohort_names_list (list): List of cohort names that will be read. check_cohort_names(self, cohort_names_list: list) -> list : Checks if the provided cohort names are valid based on the available configuration. Logs an error if any name is incorrect. Arguments : cohort_names_list (list): List of cohort names to validate. Returns : list : The validated list of cohort names. read_cohort_databases(self, cohort: str, engine='csv') -> dict[str, pd.DataFrame] : Reads all versions of the raw data for a specified cohort. The files must be stored in the same directory and have the same format. Arguments : cohort (str): Name of the cohort whose data will be read. engine (str): The engine used to read the files (default is 'csv'). Returns : dict[str, pd.DataFrame] : A dictionary with version names as keys and DataFrames as values. read_meta_data(self, cohort_name: str, meta: Literal['var', 'level']) -> pd.DataFrame : Reads the specified metadata (either variable or level data) for a cohort and returns it as a DataFrame. Arguments : cohort_name (str): The name of the cohort whose metadata will be read. meta (Literal): Specifies whether to read variable ( var ) or level ( level ) metadata. Returns : pd.DataFrame : The metadata DataFrame. read_panel_data(self) -> dict[str, pd.DataFrame] : Reads the panel data, which contains additional metadata for the panels. Returns : dict[str, pd.DataFrame] : A dictionary with the panel data. read_combined_vars_data(self, cohort_name: str) -> dict : Reads the combined variables data for a specified cohort and returns it as a dictionary. Arguments : cohort_name (str): The name of the cohort whose combined variables data will be read. Returns : dict : The combined variables data as a dictionary. : read_all_cohort_data(self, cohort_name: str) -> dict Reads all data related to a specific cohort, including metadata and raw data. Arguments : cohort_name (str): The name of the cohort whose data will be read. Returns : dict : A dictionary containing the raw data, variable data, and level data for the cohort. read_all_data(self) -> dict : Reads all data for all cohorts, including metadata and panel data. Returns : dict : A dictionary containing the data for all cohorts and panel data. set_all_data(self) -> None : Sets the all_data attribute by calling the read_all_data method. Utility Functions hash_file(file_name: str, algorithm: str=\"sha256\") -> str : Generates the hash of a specified file using the provided hashing algorithm (default is SHA-256). Arguments : file_name (str): The path of the file to hash. algorithm (str): The hashing algorithm to use (default is \"sha256\"). Returns : str : The hash of the file in hexadecimal format. stringfy(files: list[dict]) -> str : Converts a list of dictionaries into a string format for logging purposes. Arguments : files (list[dict]): A list of simple dictionaries. Returns : str : The formatted string representation of the input. test_datareader() -> None : Tests the instantiation of the DataReader class and prints the relative path and names of the files that have been read. For testing purpose. If this module ( file_reading_utils ) is executed directly as the main program, this function will be executed. General Operation Cuando se instanc\u00eda la clase DataReader se desencadenan, secuencialmente, las siguientes acciones: Instanciaci\u00f3n de los atributos b\u00e1sicos importados desde los modulos de configuraci\u00f3n ( config ) como reading_methods_dict , common_data_folder , etc. Chequeo de los nombres de las cohortes para las cuales se va a leer los datos. Esto se hace llamando al m\u00e9todo check_cohort_names . Lectura de todos los datos, que se almacenan en el atributo all_data . Esto se hace llamando al m\u00e9todo read_all_data . Consulta el apartado output description para m\u00e1s detalles. Este m\u00e9todo llama internamente, y para cada cohorte, al m\u00e9todo read_cohort_data , encargado de leer todos los datos necesarios de una cohorte. Adem\u00e1s, lee el archivo panel_data . Logs The DataReader class logs various events during the reading process: - Cohort Name Validation : Logs an error if any invalid cohort names are provided. - File Reading : Logs success or failure messages for each file read (raw data, metadata, panel data). For the database files, it also logs the name of each version and its hash. - Hash Calculation : Logs errors if any issues occur while calculating file hashes. Usage Example The following is an example of how to use the DataReader class to read cohort data and metadata: from data_reader import DataReader import config.cohort_config as cc # List of cohorts to read cohorts_list = cc.COHORT_NAME_LIST # Instantiate the DataReader class data_reader = DataReader(cohorts_list) # Access the all_data attribute, which contains all cohort data all_data = data_reader.all_data # Access data for a specific cohort liverscreen_data = all_data['Liverscreen']['data'] var_data = all_data['Liverscreen']['var_data'] level_data = all_data['Liverscreen']['level_data'] In this example, the DataReader is instantiated with a list of cohort names, and the data for each cohort is read into the all_data attribute. The raw data, variable metadata, and level metadata for each cohort can be accessed from this attribute. Output Description The DataReader class reads and stores the following data for each cohort: Raw Data : The raw data (multiple versions if available) for each cohort. Variable Metadata (var_data): Metadata describing the variables in the cohort. Level Metadata (level_data): Metadata describing the levels for categorical variables. Combined Variables Data : Information required to combine specific variables. Additionally, the panel data is read and stored as a dictionary. The structure of all_data is: all_data = { 'Liverscreen': { 'data': {version_name: DataFrame, ...}, 'var_data': DataFrame, 'level_data': DataFrame, 'comb_var_data': dict, }, 'Glucofib': {... }, ... 'panel_data': { 'panel_name': DataFrame, ... } } Error Handling The DataReader handles several common errors that may arise during the file reading process, including: FileNotFoundError : Raised if a specified file or directory does not exist. pd.errors.EmptyDataError : Raised if a file is empty. pd.errors.ParserError : Raised if there is a parsing error while reading a file. json.JSONDecodeError : Raised if there is an error decoding JSON files. UnicodeDecodeError : Raised if there is an encoding issue while reading a file. These errors are logged, and empty DataFrames or dictionaries are returned in case of failure.","title":"File Reading utils"},{"location":"modules_documentation/file_reading_utils_doc/#file_reading_utils-documentation","text":"","title":"file_reading_utils Documentation"},{"location":"modules_documentation/file_reading_utils_doc/#overview","text":"The file_reading_utils module is responsible for reading various types of data files (databases and metadata) required for building the data warehouse. The primary class in this module is DataReader , which centralizes the reading process for all relevant files, including raw cohort data, variable metadata ( var_data ), level metadata ( level_data ), panel metadata ( panel_data ) and metadata to combine variables ( comb_var_data ). This module is sensitive to the structure of the directory where the data is stored, and it also includes the function hash_file , which calculates the hash of a file. This can be useful for logging purposes, as it helps trace the files used during data reading.","title":"Overview"},{"location":"modules_documentation/file_reading_utils_doc/#general-structure","text":"The module contains the following key components: DataReader : The main class used for reading raw cohort data and metadata, managing logging, and ensuring the directory structure is correct. hash_file : A utility function for generating the hash of a file. stringfy : A helper function to convert a list of dictionaries into a formatted string for logging purposes.","title":"General Structure"},{"location":"modules_documentation/file_reading_utils_doc/#class-description","text":"","title":"Class Description"},{"location":"modules_documentation/file_reading_utils_doc/#class-datareader","text":"","title":"Class DataReader"},{"location":"modules_documentation/file_reading_utils_doc/#description","text":"The DataReader class centralizes the process of reading various data files required for creating a data warehouse. This includes reading raw cohort data, variable and level metadata, panel data and combination variable data. It manages logging, error handling, and ensures proper file access and reading.","title":"Description:"},{"location":"modules_documentation/file_reading_utils_doc/#attributes","text":"reading_methods_dict (dict) : Contains the methods for reading different types of data (e.g., .csv , .dta , .xlsx ). databases_format_dict (dict) : Specifies the format of the raw databases for each cohort, used in conjunction with reading_methods_dict to read cohort data. common_data_folder (str) : The directory where all the data files to be read are stored. databases_folder (str) : The subfolder where the raw data for each cohort is stored. level_file_names (dict) : A dictionary containing the filenames of the level data for each cohort. var_file_names (dict) : A dictionary containing the filenames of the variable data for each cohort. comb_vars_file_names (dict) : A dictionary containing the filenames of the combined variables data for each cohort. panel_file_name (str) : The filename for the panel data. cohorts (list[str]) : A list containing the names of the cohorts to be read. reading_exceptions (tuple) : A tuple of common exceptions that may arise while reading files. all_data (dict) : A dictionary containing all the data required to create the data warehouse. read_files_log (dict) : A dictionary containing the filenames and their hashes for logging purposes.","title":"Attributes:"},{"location":"modules_documentation/file_reading_utils_doc/#methods","text":"__init__(self, cohort_names_list: list) -> None : Initializes the DataReader with a list of cohort names and sets up the necessary attributes and configurations. Arguments : cohort_names_list (list): List of cohort names that will be read. check_cohort_names(self, cohort_names_list: list) -> list : Checks if the provided cohort names are valid based on the available configuration. Logs an error if any name is incorrect. Arguments : cohort_names_list (list): List of cohort names to validate. Returns : list : The validated list of cohort names. read_cohort_databases(self, cohort: str, engine='csv') -> dict[str, pd.DataFrame] : Reads all versions of the raw data for a specified cohort. The files must be stored in the same directory and have the same format. Arguments : cohort (str): Name of the cohort whose data will be read. engine (str): The engine used to read the files (default is 'csv'). Returns : dict[str, pd.DataFrame] : A dictionary with version names as keys and DataFrames as values. read_meta_data(self, cohort_name: str, meta: Literal['var', 'level']) -> pd.DataFrame : Reads the specified metadata (either variable or level data) for a cohort and returns it as a DataFrame. Arguments : cohort_name (str): The name of the cohort whose metadata will be read. meta (Literal): Specifies whether to read variable ( var ) or level ( level ) metadata. Returns : pd.DataFrame : The metadata DataFrame. read_panel_data(self) -> dict[str, pd.DataFrame] : Reads the panel data, which contains additional metadata for the panels. Returns : dict[str, pd.DataFrame] : A dictionary with the panel data. read_combined_vars_data(self, cohort_name: str) -> dict : Reads the combined variables data for a specified cohort and returns it as a dictionary. Arguments : cohort_name (str): The name of the cohort whose combined variables data will be read. Returns : dict : The combined variables data as a dictionary. : read_all_cohort_data(self, cohort_name: str) -> dict Reads all data related to a specific cohort, including metadata and raw data. Arguments : cohort_name (str): The name of the cohort whose data will be read. Returns : dict : A dictionary containing the raw data, variable data, and level data for the cohort. read_all_data(self) -> dict : Reads all data for all cohorts, including metadata and panel data. Returns : dict : A dictionary containing the data for all cohorts and panel data. set_all_data(self) -> None : Sets the all_data attribute by calling the read_all_data method.","title":"Methods:"},{"location":"modules_documentation/file_reading_utils_doc/#utility-functions","text":"hash_file(file_name: str, algorithm: str=\"sha256\") -> str : Generates the hash of a specified file using the provided hashing algorithm (default is SHA-256). Arguments : file_name (str): The path of the file to hash. algorithm (str): The hashing algorithm to use (default is \"sha256\"). Returns : str : The hash of the file in hexadecimal format. stringfy(files: list[dict]) -> str : Converts a list of dictionaries into a string format for logging purposes. Arguments : files (list[dict]): A list of simple dictionaries. Returns : str : The formatted string representation of the input. test_datareader() -> None : Tests the instantiation of the DataReader class and prints the relative path and names of the files that have been read. For testing purpose. If this module ( file_reading_utils ) is executed directly as the main program, this function will be executed.","title":"Utility Functions"},{"location":"modules_documentation/file_reading_utils_doc/#general-operation","text":"Cuando se instanc\u00eda la clase DataReader se desencadenan, secuencialmente, las siguientes acciones: Instanciaci\u00f3n de los atributos b\u00e1sicos importados desde los modulos de configuraci\u00f3n ( config ) como reading_methods_dict , common_data_folder , etc. Chequeo de los nombres de las cohortes para las cuales se va a leer los datos. Esto se hace llamando al m\u00e9todo check_cohort_names . Lectura de todos los datos, que se almacenan en el atributo all_data . Esto se hace llamando al m\u00e9todo read_all_data . Consulta el apartado output description para m\u00e1s detalles. Este m\u00e9todo llama internamente, y para cada cohorte, al m\u00e9todo read_cohort_data , encargado de leer todos los datos necesarios de una cohorte. Adem\u00e1s, lee el archivo panel_data .","title":"General Operation"},{"location":"modules_documentation/file_reading_utils_doc/#logs","text":"The DataReader class logs various events during the reading process: - Cohort Name Validation : Logs an error if any invalid cohort names are provided. - File Reading : Logs success or failure messages for each file read (raw data, metadata, panel data). For the database files, it also logs the name of each version and its hash. - Hash Calculation : Logs errors if any issues occur while calculating file hashes.","title":"Logs"},{"location":"modules_documentation/file_reading_utils_doc/#usage-example","text":"The following is an example of how to use the DataReader class to read cohort data and metadata: from data_reader import DataReader import config.cohort_config as cc # List of cohorts to read cohorts_list = cc.COHORT_NAME_LIST # Instantiate the DataReader class data_reader = DataReader(cohorts_list) # Access the all_data attribute, which contains all cohort data all_data = data_reader.all_data # Access data for a specific cohort liverscreen_data = all_data['Liverscreen']['data'] var_data = all_data['Liverscreen']['var_data'] level_data = all_data['Liverscreen']['level_data'] In this example, the DataReader is instantiated with a list of cohort names, and the data for each cohort is read into the all_data attribute. The raw data, variable metadata, and level metadata for each cohort can be accessed from this attribute.","title":"Usage Example"},{"location":"modules_documentation/file_reading_utils_doc/#output-description","text":"The DataReader class reads and stores the following data for each cohort: Raw Data : The raw data (multiple versions if available) for each cohort. Variable Metadata (var_data): Metadata describing the variables in the cohort. Level Metadata (level_data): Metadata describing the levels for categorical variables. Combined Variables Data : Information required to combine specific variables. Additionally, the panel data is read and stored as a dictionary. The structure of all_data is: all_data = { 'Liverscreen': { 'data': {version_name: DataFrame, ...}, 'var_data': DataFrame, 'level_data': DataFrame, 'comb_var_data': dict, }, 'Glucofib': {... }, ... 'panel_data': { 'panel_name': DataFrame, ... } }","title":"Output Description"},{"location":"modules_documentation/file_reading_utils_doc/#error-handling","text":"The DataReader handles several common errors that may arise during the file reading process, including: FileNotFoundError : Raised if a specified file or directory does not exist. pd.errors.EmptyDataError : Raised if a file is empty. pd.errors.ParserError : Raised if there is a parsing error while reading a file. json.JSONDecodeError : Raised if there is an error decoding JSON files. UnicodeDecodeError : Raised if there is an encoding issue while reading a file. These errors are logged, and empty DataFrames or dictionaries are returned in case of failure.","title":"Error Handling"},{"location":"modules_documentation/metadata_utils_doc/","text":"metadata_utils Documentation VarData Var","title":"metadata_utils Documentation"},{"location":"modules_documentation/metadata_utils_doc/#metadata_utils-documentation","text":"","title":"metadata_utils Documentation"},{"location":"modules_documentation/metadata_utils_doc/#vardata","text":"","title":"VarData"},{"location":"modules_documentation/metadata_utils_doc/#var","text":"","title":"Var"},{"location":"modules_documentation/qc_checks_utils_doc/","text":"qc_checks_utils Documentation Overview The qc_checks_utils module is responsible for performing the various quality control and validation checks that are conducted before, during, and after data processing. The module also handles the export of the quality controls records and logs the relevant alerts in the execution log. The module includes the following checks: Non-numeric value detection : Detects the presence of non numeric values ( -inf , inf ). NaN analysis : Analizes the presence of NaN values (missings) in both raw_data and homogeneous_data. Conversion validation : Checks if the transformation and formatting of variables has been done properly. Data consistency checks : Checks if the final data structure fits the one specified in the metadata ( var_data , level_data , panel_metadata ). Each of these checks is managed by a class ('checker' from now on) designed specifically for that purpose. A final object, QCChecker , is responsible for coordinating all the aforementioned classes and functionalities. Quality control is performed on a Cohort object, utilizing its attributes to conduct the various checks. For more information about this class, refer to the cohort_utils section. General Structure The module has the following general structure, based on the following classes: GeneralChecker : This class serves as a base template for all other \"checker\" classes. It requires a Cohort object as a parameter for initialization and sets the common attributes that all checker classes need, such as the cohort name, raw data, homogeneous data, among others. Specific Checker Classes: NonNumericChecker : Responsible for checking the presence of non-numeric values in the numeric variables of the cohort's homogeneous data. Inherits from GeneralChecker . NaNChecker : Analyzes the presence of NaN values in both raw and homogeneous data and records the differences between them. Inherits from GeneralChecker . ConversionChecker : Checks for errors that may have occurred during the conversion of numeric and categorical variables in the homogenization process. Inherits from GeneralChecker . ConsistencyChecker : Reviews the consistency of the final data by comparing it with the structure of the original metadata defined in the configuration files. Inherits from GeneralChecker . QCChecker : This class centralizes all quality control (QC) checks by integrating the specific checker classes. It coordinates the execution of all checks and exports the results. Inherits from GeneralChecker . Checkers description GeneralChecker Description: The GeneralChecker class serves as a general template for all the \"checker\" classes in the quality control module. It acts as the base class (or parent class) from which all other specific checker classes inherit. It provides a common structure and essential attributes (extracted from the Cohort class) for performing quality control checks on data. Attributes: name (str) : The name of the cohort being checked. raw_data (pandas.DataFrame) : A DataFrame containing the raw data from the cohort. This is the original, unprocessed dataset. homogeneous_data (pandas.DataFrame) : A DataFrame containing the homogenized data from the cohort. This dataset has undergone processing and formatting to unify formats and values. See section Dataflow for an overview of this process. Check section data_processing_util and cohort_utils var_data_json (dict) : A dictionary containing metadata about the variables used to create the database. This metadata is also used in quality control checks. level_data_json (dict) : A dictionary containing metadata about variable levels, which is also used for quality control checks. var_translate_dict (dict) : A dictionary used to translate the original variable names to the common variable names used in the homogenized dataset. selected_variables (list) : A list of the selected (or core) variables of the cohort. selected_variables_homogenized (list) : A list of the selected variables, translated to the common format used in the homogenized dataset. alpha (float) : A parameter used as a threshold for detecting discrepancies in numerical conversions. Ideally, any difference greater than 0 should be reported. However, as some rounding errors are introduced while performin datatype adjustment and conversions, small differences between the expected and the real value can be due to this errors. If a discrepancy is bigger than alpha it is reported. Otherwise it is ignored. exporting_exceptions (tuple) : A tuple containing common exceptions that might occur during the data export process. This includes FileNotFoundError , PermissionError , OSError , among others. Methods: The GeneralChecker class does not define specific methods, as its main purpose is to provide common structure and attributes for the derived classes. The specific methods for quality control checks are implemented in the subclasses. Usage: GeneralChecker is used as a base class for all other checker classes in the module. Logs This class does not generate any logs. NonNumericChecker Description: The NonNumericChecker class is used to check, report, and export information about non-numeric values (such as -inf and inf ) in the cohort homogeneous_data . This class inherits from GeneralChecker and handles the detection of non-numeric values in the numeric variables of the homogenized data. Non-numeric values (such as -inf and inf ) behave differently than NaN values, so they are detected and managed separately. Attributes: Attributes inherited from GeneralChecker : All attributes from the GeneralChecker class. non_numeric_record (pandas.DataFrame) : A DataFrame containing the count of non-numeric values for each numeric variable in the homogenized data of the cohort. Methods: check_non_numeric_values() : This method checks for non-numeric values (e.g., -inf , inf , but not NaN ) in every numeric variable (all the variables whose final datatype is in main_config.NUMERIC_TYPES ) in the homogenized data. If non-numeric values are found, they are recorded in a list of errors and returned as a DataFrame. Return : error_df (pandas.DataFrame) - A DataFrame containing the count of non-numeric values for each numeric variable. export_qc_data(export_path: str) : This method exports the non-numeric values check results to a specified path. It exports the non_numeric_record DataFrame as a CSV file. Returns : None Arguments : export_path (str): The file path where the check record will be exported. Logs In method check_non_numeric_values : If a variable being analyzed does not appear in the data, a warning is logged. If a numeric variable (column) in the raw_data contains at lest one non_numeric value, a warning is logged. In method export_qc_data : If there is any error while exporting generated data, an Error is logged. NaNChecker Description: The NaNChecker class is responsible for analyzing the presence of NaN values in both the raw data ( raw_data ) and the homogenized data ( homogeneous_data ) of a cohort. This class inherits from GeneralChecker and provides functionality to check, compare, and export information about missing ( NaN ) values. It also compares the NaN counts between the raw and homogenized data to detect any discrepancies that might have occurred during data processing. Attributes: Attributes inherited from GeneralChecker : All attributes from the GeneralChecker class. raw_nan_record_df (pandas.DataFrame) : A DataFrame containing the count of NaN values for each selected variable in the raw data. raw_nan_record_dict (dict) : A dictionary containing the count of NaN values for each selected variable in the raw data. homogeneous_nan_record_df (pandas.DataFrame) : A DataFrame containing the count of NaN values for each selected variable in the homogenized data. homogeneous_nan_record_dict (dict) : A dictionary containing the count of NaN values for each selected variable in the homogenized data. nan_diff_record_dict (dict) : A dictionary that stores the differences in NaN counts between the raw and homogenized data for each variable. Methods: get_df_nan_record(df: pandas.DataFrame, column_list: list = None) -> tuple[pandas.DataFrame, dict] : This method analyzes a DataFrame and returns both a DataFrame and a dictionary containing the NaN counts for each specified column in column_list(or all columns if no column list is provided). Arguments : df (pandas.DataFrame): The DataFrame to be analyzed for NaN values. column_list (list, optional): A list of column names to check for NaN values. If not provided, all columns in the DataFrame are checked. Return : nan_record_df (pandas.DataFrame): A DataFrame containing the count of NaN values for each column in the DataFrame. nan_record (dict): A dictionary containing the same information as nan_record_df , with column names as keys and NaN counts as values. log_nan_record() -> dict : This method compares the NaN counts between the raw data and the homogenized data to check for any discrepancies. It logs discrepancies in NaN counts and returns a dictionary with the differences for each variable. Return : nan_check_dict (dict): A dictionary containing the difference in NaN counts between the raw and homogenized data for each variable. export_qc_data(export_path: str) : This method exports the NaN check results to the specified path. It exports the raw_nan_record_df , homogeneous_nan_record_df , and nan_diff_record_dict as CSV files or JSON files as appropriate. Arguments : export_path (str): The file path where the check records will be exported. Return : None Logs: In method get_df_nan_record : If a column in the provided column list is not a subset of the columns in the DataFrame, a warning is logged indicating the missing variables. If there is an error while calculating the percentage of NaN values, an error is logged. In method log_nan_record : If there is a mismatch in the NaN counts for a variable between the raw and homogenized data, an error is logged with details of the discrepancy. If no discrepancies are found, an info log confirms that the NaN counts were successfully checked. In method export_qc_data : If there is any error while exporting the generated NaN check records, an error is logged with details about the exception. ConversionChecker Description: The ConversionChecker class is responsible for performing conversion checks in the cohort data. This class verifies if both numerical and categorical variables have been properly formatted during data processing. It inherits from GeneralChecker . Checks for discrepancies in numerical values (comparing means between the raw and homogenized data) and in categorical values (comparing category counts between the raw and homogenized data). It also allows exporting the results of these checks. Attributes: Attributes inherited from GeneralChecker : All attributes from the GeneralChecker class. mean_diff_dict (dict) : A dictionary containing the difference between means for each numerical variable in the raw data and the homogenized data. label_count_dict (dict) : contains the count of each level for every categorical variable. It also includes the count of not missing data of that specific variable. Its structure is as follows: { \"variable1\": { \"label_count\": { \"label1\": <int>, \"label2\": <int>, ... }, \"notna_count\": <int> } ... }, Methods: check_numeric_conversion(var: str, mean_diff_dict: dict) -> tuple[bool, float] : This method checks if the mean of a numerical variable in the raw data matches the mean in the homogenized data, taking into account any conversion factors. If the difference exceeds a specified threshold ( alpha ) it logs an error. Arguments : var (str): The name of the numerical variable to check. mean_diff_dict (dict): A dictionary to store the mean differences for each variable. Return : Tuple : means_check (bool): True if no significant discrepancies are found, False otherwise. mean_diff (float): The difference in the means between the raw data and the homogenized data. check_category_conversion(var: str) -> tuple[bool, dict, int] : This method checks if the category counts for a categorical variable match between the raw data and the homogenized data, considering the specific variable mapping. If discrepancies are found, it logs a warning. Arguments : var (str): The name of the categorical variable to check. Return : category_check (bool): True if no discrepancies are found, False otherwise. check_format_conversion() -> dict : This method centralizes the format conversion checks for both numerical and categorical variables. It calls the appropriate check methods for each variable and updates the mean_diff_dict with the results of the numerical checks. Return : mean_diff_dict (dict): A dictionary containing the mean differences between the raw and homogenized data for all numerical variables. export_qc_data(export_path: str) : This method exports the results of the conversion checks to the specified path. It exports the mean_diff_dict as a JSON file. Arguments : export_path (str): The file path where the check records will be exported. Logs: In method check_numeric_conversion : If the mean difference for a numerical variable exceeds the threshold, an error is logged with details about the discrepancy. In method check_category_conversion : If there is a mismatch in category counts between the raw and homogenized data, a warning is logged with the details of the discrepancy. In method check_format_conversion : Logs an informational message when the format conversion checks start. Logs the results of both numerical and categorical variable checks. In method export_qc_data : If there is any error while exporting the conversion check records, an error is logged with details about the exception. ConsistancyChecker Description: The ConsistancyChecker class is responsible for verifying the consistency of the cohort data. It checks if the final data (homogenized data) complies with the structure defined in the initial metadata (such as var_data , level_data , and panel_data files). This includes checking data types and ensuring that melted variables within panels have consistent data types. The class inherits from GeneralChecker and logs any discrepancies found during the consistency checks. Attributes: Attributes inherited from GeneralChecker : All attributes from the GeneralChecker class. Methods: check_data_types() -> None : This method checks if there are any discrepancies between the data types in the final homogenized data and the data types specified in the var_data metadata. If discrepancies are found, they are logged as warnings. Return : None panel_consistancy_datatypes_check(panel_data_json: dict[str, dict]) -> None : This method checks for consistency in data types within panels. Specifically, it verifies that all melted variables in a panel have the same data type. If discrepancies are found, they are logged as warnings. Arguments : panel_data_json (dict): A dictionary containing metadata about the structure of the panels. This is used to check the consistency of data types within the panels. Return : None Logs: In method check_data_types : If there are discrepancies between the data types in the homogenized data and those specified in the metadata, a warning is logged with details of the discrepancies. In method panel_consistancy_datatypes_check : If there are discrepancies in data types within a panel (i.e., melted variables with different data types), a warning is logged with details of the panel and the conflicting data types. OutliersChecker Description: The OutliersChecker class inherits from the GeneralChecker class and is responsible for detecting outliers in numerical data using a z-score threshold (> 2) and reporting the main percentiles for each variable. This class extends the functionality of GeneralChecker to enable comprehensive outlier detection across all numerical columns in the dataset. Attributes: Attributes inherited from GeneralChecker : All attributes from the GeneralChecker class. cohort_outliers_z_score ( dict ): A dictionary containing outliers for all numerical variables in the dataset. Each key represents a variable name, and its value represents the outliers count according to the set threshold (Default set to 2) cohort_outliers_percentile ( dict ): A dictionary containing the main percentiles for each numerical variable. Methods: get_column_outliers_percentile(df: pd.DataFrame, column: str, percentiles: list[float] = [0, 0.025, 0.25, 0.50, 0.75, 0.975, 1]) -> dict : This method calculates a set of percentiles for a specified variable (column) and returns them as a dictionary. It is intended to be used in get_cohort_outliers along with get_column_outliers_z_score for each numerical variable in the dataset. Arguments : df (pandas.DataFrame): The DataFrame containing the data to be analyzed. column (str): The name of the column/variable for which the percentiles will be calculated. percentiles (list[float]): A list of percentiles (in decimal form) to be calculated. Default is [0, 0.025, 0.25, 0.50, 0.75, 0.975, 1] . Return : percentiles_dict (dict): A dictionary containing the calculated value for each percentile defined in the percentiles argument. The structure is {percentile: value, ...} . get_column_outliers_z_score(df: pd.DataFrame, column: str, z_score_bound: float = 2, id_var: str = 'cohort_id') -> dict : This method calculates the z-score of a specified numerical variable (column) and identifies outliers based on the specified z_score_bound . It returns a dictionary with the count of outliers and the count of non-missing values for that column. This method is intended to be used in get_cohort_outliers along with get_column_outliers_percentile for each numerical variable in the dataset. Arguments : df (pandas.DataFrame): The DataFrame containing the data to be analyzed. column (str): The name of the column/variable for which the z-score will be calculated and outliers will be identified. z_score_bound (float): The threshold for identifying an outlier. If the absolute value of the z-score exceeds this parameter, the observation is considered an outlier. Default is 2 . id_var (str): The variable used as the identifier for each observation (e.g., cohort_id or liervaim_id ). Default is 'cohort_id' . Return : outliers_dict (dict): A dictionary with the count of outliers and the count of non-missing values for the specified column. The structure is {'outliers_count': int, 'not_missings_count': int } . get_cohort_outliers() -> tuple[dict, dict] : This method iterates through each numerical column in self.homogeneous_data , calling the get_column_outliers_z_score and get_column_outliers_percentile methods for each column. The results are stored in two dictionaries, one for outliers identified by the z-score and one for percentiles. Return : tuple(dict, dict) : A tuple containing two dictionaries: Z-score Outliers Dictionary (dict) : Contains the outliers identified using the z-score method. Each key is a variable name, and each value is the count of outliers for that variable. Percentiles Dictionary (dict) : Contains the percentiles for each numerical variable. Each key is a variable name, and each value is a dictionary structured as {percentile1: value, percentile2: value, ...} . export_qc_data(export_path: str) -> None : This method exports data related to the outliers analysis to the specified export_path . Specifically, it exports the dictionaries cohort_outliers_name_z_score and cohort_outliers_name_percentile . Arguments : export_path (str): The path where the quality check record will be saved. Logs: In method export_qc_data : If there is any error while exporting the outlier data to the file, an error is logged with the relevant details. QCChecker Description: The QCChecker class centralizes the quality control (QC) checks for the cohort data. This class orchestrates the execution of all other checker classes, including NonNumericChecker , NaNChecker , ConversionChecker , ConsistancyChecker and OtuliersChecker to ensure data quality. It also provides methods to generate cohort reports and export QC results. The class inherits from GeneralChecker . Attributes: Attributes inherited from GeneralChecker : All attributes from the GeneralChecker class. non_numeric_checker : An instance of the NonNumericChecker class, responsible for checking non-numeric values in the cohort data. nan_checker : An instance of the NaNChecker class, responsible for checking NaN values in the cohort data. conversion_checker : An instance of the ConversionChecker class, responsible for verifying the conversion of numerical and categorical variables. consistency_checker : An instance of the ConsistancyChecker class, responsible for checking the consistency of the final data with the initial metadata. outlier_checker : An instance of the OutliersChecker class, responsible for recording the count of outliers for each numerical variable (and the corresponding ID's). export_qc_record (bool) : A boolean value indicating whether the QC records should be exported. This is configured via the config module. Methods: cohort_report() -> None : This method logs a brief description of the raw data and homogenized data from the cohort. It reports the number of variables and patients in each dataset. Return : None export_qc_reports() -> None : This method centralizes the process of exporting the QC results. It first creates the directory where the QC data will be stored (see output description ), then calls the export methods of the individual checker classes to save the QC results to files. Return : None check_numeric_vars_coherence(data: pandas.DataFrame, var_data_json: dict) -> pandas.DataFrame : This method checks the coherence of numeric variables across different cohorts. It calculates the mean and standard deviation for each numeric variable in the selected variables and returns a DataFrame summarizing this information. The aim of the method is to detect if there is any odd behaviour between data from the different cohorts (such as huge difference in mean or sd between cohorts). This method must be used with the merged-homogenized data. Arguments : data (pandas.DataFrame): The homogenized and merged data from all cohorts. var_data_json (dict): A dictionary containing metadata about the variables, including data types. Return : combined_data_df (pandas.DataFrame): A DataFrame containing the mean and standard deviation for each numeric variable, grouped by cohort. Logs: In method cohort_report : Logs an informational message summarizing the structure of both the raw data and homogenized data, including the number of variables and patients in each. In method export_qc_reports : If there is any error while exporting the QC results, an error is logged with details about the exception. Output Description Several results generated during quality controls are exported if the QC_EXPORT_RECORD variable (declared in the main_config module) is set to True . The QC_REPORT_FOLDER directory stores all results related to quality control. This directory contains the code necessary to create the QC report if desired. QC_REPORT_FOLDER also contains a subfolder, defined by the QC_DATA_FOLDER variable, where all the raw outputs generated during QC are saved. Each execution (if QC_EXPORT_RECORD is set to True ) will generate a directory within QC_DATA_FOLDER named: qc_execution_yyyy-mm-dd_hh-mm-ss Where yyyy-mm-dd_hh-mm-ss corresponds to the EXECUTION_DATETIME . This directory will store, for each cohort, the files containing the QC-related data. Thus, the general structure of QC_DATA_FOLDER could be something like this: QC_REPORT_FOLDER/ \u251c\u2500\u2500 qc_report.qmd # Quarto/md file to create the report. \u2514\u2500\u2500 QC_DATA_FOLDER/ \u251c\u2500\u2500 qc_execution_2024-08-12_13-42-42/ \u2502 \u251c\u2500\u2500 Liverscreen \u2502 \u2502 \u2514\u2500\u2500 <QC_data_files> \u2502 \u251c\u2500\u2500 Alcofib \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 qc_execution_2024-08-12_13-42-42/ \u2514\u2500\u2500 ... The resulting files for each cohort are as follows: raw_nan_record_df.csv : Contains the count of NaN values for the core/selected variables of the raw data ( raw_data ). Variable Liverscreen NaN Count total_records % NaN 0 age_0 0 23624 0.00% 1 ethnicity 37 23624 0.16% 2 gender 0 23624 0.00% 3 hypertension 82 23624 0.35% 4 centralobesity 19 23624 0.08% 5 hightrigly 488 23624 2.07% ... homogeneous_nan_record_df.csv : Contains the count of NaN values for the selected/core variables of the homogeneous data ( homogeneous_data ). Variable Liverscreen NaN Count total_records % NaN 0 age 0 23624 0.00% 1 ethnicity 37 23624 0.16% 2 gender 0 23624 0.00% 3 hypertension 82 23624 0.35% 4 centralobesity 19 23624 0.08% 5 hightrigly 488 23624 0.00% ... non_numeric_record_df.csv : Records non-numeric values (such as -inf, inf) found in the numeric variables of the homogeneous data. Variable non_numeric_values_count 0 age 0 1 weight 0 2 height 0 3 bmi 0 4 hip 0 5 te 0 ... nan_diff_record_dict.json : Contains the differences in NaN counts between the raw and homogeneous data for each variable. { \"age\": 0, \"ethnicity\": 0, \"gender\": 0, \"hypertension\": 0, \"centralobesity\": 0, ... } mean_difference_dict.json : Records the differences in means between the raw and homogeneous data for numeric variables, after applying the conversion factor. { \"age\": 0.0, \"weight\": 0.0, \"height\": 0.0, \"bmi\": 0.0, \"hip\": 0.0, \"te\": 0.0, ... } percentile_outliers.json : Records the precentiles specified during the execution of get_column_outliers_percentile method for each numeric variable. The format is the following: ```json { \"age\": { \"0.0\": 36.0, \"0.025\": 45.0, \"0.25\": 62.0, \"0.5\": 71.0, \"0.75\": 77.0, \"0.975\": 84.375, \"1.0\": 88.0 }, \"weight\": { \"0.0\": 44.0, \"0.025\": 53.625, \"0.25\": 69.0, \"0.5\": 79.0, \"0.75\": 89.0, \"0.975\": 117.0, \"1.0\": 160.0 }, ... } ``` z_score_outliers.json : Records both the outliers according to the z-score value (and the specifided threshold, default set to 2) and the count of not missing data for each variable (so the % of outliers can be calculated in the qc report) ```json { \"age\": { \"outliers_count\": 20, \"not_missings_count\": 466 }, \"weight\": { \"outliers_count\": 17, \"not_missings_count\": 466 }, \"height\": { \"outliers_count\": 18, \"not_missings_count\": 466 }, ... } ``` Usage in main execution The use of this module during the main execution of the code is through the instantiation of the QCChecker . Each time a Cohort class is instantiated (and the database is internally processed to obtain the homogeneous_data ), a QCChecker object is created for that cohort (i.e., for each cohort listed in COHORT_NAME_LIST from the cohort_config module). When QCChecker is instantiated, it automatically creates instances of all the classes responsible for various validations and checks as its attributes (see QCChecker class ). Each of these attributes will execute its respective validation and check methods automatically, storing the generated records in their own attributes. If the QC_EXPORT_RECORD variable from the main_config module is set to True , the export_qc_reports method of the QCChecker will be executed in the main code and quality control records for this specific cohort will be exported (see output descrption ). After all the cohorts have been created and merged into the final Liveraim cohort, another instance of the QCChecker class is created to perform a quality control check on this final cohort. In this case, quality control checks related to the structure of the final panels will also be applied through the panel_consistancy_datatypes_check method of the ConsistancyChecker class. Other complementary functions Not yet implemented","title":"Quality Control utils"},{"location":"modules_documentation/qc_checks_utils_doc/#qc_checks_utils-documentation","text":"","title":"qc_checks_utils Documentation"},{"location":"modules_documentation/qc_checks_utils_doc/#overview","text":"The qc_checks_utils module is responsible for performing the various quality control and validation checks that are conducted before, during, and after data processing. The module also handles the export of the quality controls records and logs the relevant alerts in the execution log. The module includes the following checks: Non-numeric value detection : Detects the presence of non numeric values ( -inf , inf ). NaN analysis : Analizes the presence of NaN values (missings) in both raw_data and homogeneous_data. Conversion validation : Checks if the transformation and formatting of variables has been done properly. Data consistency checks : Checks if the final data structure fits the one specified in the metadata ( var_data , level_data , panel_metadata ). Each of these checks is managed by a class ('checker' from now on) designed specifically for that purpose. A final object, QCChecker , is responsible for coordinating all the aforementioned classes and functionalities. Quality control is performed on a Cohort object, utilizing its attributes to conduct the various checks. For more information about this class, refer to the cohort_utils section.","title":"Overview"},{"location":"modules_documentation/qc_checks_utils_doc/#general-structure","text":"The module has the following general structure, based on the following classes: GeneralChecker : This class serves as a base template for all other \"checker\" classes. It requires a Cohort object as a parameter for initialization and sets the common attributes that all checker classes need, such as the cohort name, raw data, homogeneous data, among others. Specific Checker Classes: NonNumericChecker : Responsible for checking the presence of non-numeric values in the numeric variables of the cohort's homogeneous data. Inherits from GeneralChecker . NaNChecker : Analyzes the presence of NaN values in both raw and homogeneous data and records the differences between them. Inherits from GeneralChecker . ConversionChecker : Checks for errors that may have occurred during the conversion of numeric and categorical variables in the homogenization process. Inherits from GeneralChecker . ConsistencyChecker : Reviews the consistency of the final data by comparing it with the structure of the original metadata defined in the configuration files. Inherits from GeneralChecker . QCChecker : This class centralizes all quality control (QC) checks by integrating the specific checker classes. It coordinates the execution of all checks and exports the results. Inherits from GeneralChecker .","title":"General Structure"},{"location":"modules_documentation/qc_checks_utils_doc/#checkers-description","text":"","title":"Checkers description"},{"location":"modules_documentation/qc_checks_utils_doc/#generalchecker","text":"","title":"GeneralChecker"},{"location":"modules_documentation/qc_checks_utils_doc/#description","text":"The GeneralChecker class serves as a general template for all the \"checker\" classes in the quality control module. It acts as the base class (or parent class) from which all other specific checker classes inherit. It provides a common structure and essential attributes (extracted from the Cohort class) for performing quality control checks on data.","title":"Description:"},{"location":"modules_documentation/qc_checks_utils_doc/#attributes","text":"name (str) : The name of the cohort being checked. raw_data (pandas.DataFrame) : A DataFrame containing the raw data from the cohort. This is the original, unprocessed dataset. homogeneous_data (pandas.DataFrame) : A DataFrame containing the homogenized data from the cohort. This dataset has undergone processing and formatting to unify formats and values. See section Dataflow for an overview of this process. Check section data_processing_util and cohort_utils var_data_json (dict) : A dictionary containing metadata about the variables used to create the database. This metadata is also used in quality control checks. level_data_json (dict) : A dictionary containing metadata about variable levels, which is also used for quality control checks. var_translate_dict (dict) : A dictionary used to translate the original variable names to the common variable names used in the homogenized dataset. selected_variables (list) : A list of the selected (or core) variables of the cohort. selected_variables_homogenized (list) : A list of the selected variables, translated to the common format used in the homogenized dataset. alpha (float) : A parameter used as a threshold for detecting discrepancies in numerical conversions. Ideally, any difference greater than 0 should be reported. However, as some rounding errors are introduced while performin datatype adjustment and conversions, small differences between the expected and the real value can be due to this errors. If a discrepancy is bigger than alpha it is reported. Otherwise it is ignored. exporting_exceptions (tuple) : A tuple containing common exceptions that might occur during the data export process. This includes FileNotFoundError , PermissionError , OSError , among others.","title":"Attributes:"},{"location":"modules_documentation/qc_checks_utils_doc/#methods","text":"The GeneralChecker class does not define specific methods, as its main purpose is to provide common structure and attributes for the derived classes. The specific methods for quality control checks are implemented in the subclasses.","title":"Methods:"},{"location":"modules_documentation/qc_checks_utils_doc/#usage","text":"GeneralChecker is used as a base class for all other checker classes in the module.","title":"Usage:"},{"location":"modules_documentation/qc_checks_utils_doc/#logs","text":"This class does not generate any logs.","title":"Logs"},{"location":"modules_documentation/qc_checks_utils_doc/#nonnumericchecker","text":"","title":"NonNumericChecker"},{"location":"modules_documentation/qc_checks_utils_doc/#description_1","text":"The NonNumericChecker class is used to check, report, and export information about non-numeric values (such as -inf and inf ) in the cohort homogeneous_data . This class inherits from GeneralChecker and handles the detection of non-numeric values in the numeric variables of the homogenized data. Non-numeric values (such as -inf and inf ) behave differently than NaN values, so they are detected and managed separately.","title":"Description:"},{"location":"modules_documentation/qc_checks_utils_doc/#attributes_1","text":"Attributes inherited from GeneralChecker : All attributes from the GeneralChecker class. non_numeric_record (pandas.DataFrame) : A DataFrame containing the count of non-numeric values for each numeric variable in the homogenized data of the cohort.","title":"Attributes:"},{"location":"modules_documentation/qc_checks_utils_doc/#methods_1","text":"check_non_numeric_values() : This method checks for non-numeric values (e.g., -inf , inf , but not NaN ) in every numeric variable (all the variables whose final datatype is in main_config.NUMERIC_TYPES ) in the homogenized data. If non-numeric values are found, they are recorded in a list of errors and returned as a DataFrame. Return : error_df (pandas.DataFrame) - A DataFrame containing the count of non-numeric values for each numeric variable. export_qc_data(export_path: str) : This method exports the non-numeric values check results to a specified path. It exports the non_numeric_record DataFrame as a CSV file. Returns : None Arguments : export_path (str): The file path where the check record will be exported.","title":"Methods:"},{"location":"modules_documentation/qc_checks_utils_doc/#logs_1","text":"In method check_non_numeric_values : If a variable being analyzed does not appear in the data, a warning is logged. If a numeric variable (column) in the raw_data contains at lest one non_numeric value, a warning is logged. In method export_qc_data : If there is any error while exporting generated data, an Error is logged.","title":"Logs"},{"location":"modules_documentation/qc_checks_utils_doc/#nanchecker","text":"","title":"NaNChecker"},{"location":"modules_documentation/qc_checks_utils_doc/#description_2","text":"The NaNChecker class is responsible for analyzing the presence of NaN values in both the raw data ( raw_data ) and the homogenized data ( homogeneous_data ) of a cohort. This class inherits from GeneralChecker and provides functionality to check, compare, and export information about missing ( NaN ) values. It also compares the NaN counts between the raw and homogenized data to detect any discrepancies that might have occurred during data processing.","title":"Description:"},{"location":"modules_documentation/qc_checks_utils_doc/#attributes_2","text":"Attributes inherited from GeneralChecker : All attributes from the GeneralChecker class. raw_nan_record_df (pandas.DataFrame) : A DataFrame containing the count of NaN values for each selected variable in the raw data. raw_nan_record_dict (dict) : A dictionary containing the count of NaN values for each selected variable in the raw data. homogeneous_nan_record_df (pandas.DataFrame) : A DataFrame containing the count of NaN values for each selected variable in the homogenized data. homogeneous_nan_record_dict (dict) : A dictionary containing the count of NaN values for each selected variable in the homogenized data. nan_diff_record_dict (dict) : A dictionary that stores the differences in NaN counts between the raw and homogenized data for each variable.","title":"Attributes:"},{"location":"modules_documentation/qc_checks_utils_doc/#methods_2","text":"get_df_nan_record(df: pandas.DataFrame, column_list: list = None) -> tuple[pandas.DataFrame, dict] : This method analyzes a DataFrame and returns both a DataFrame and a dictionary containing the NaN counts for each specified column in column_list(or all columns if no column list is provided). Arguments : df (pandas.DataFrame): The DataFrame to be analyzed for NaN values. column_list (list, optional): A list of column names to check for NaN values. If not provided, all columns in the DataFrame are checked. Return : nan_record_df (pandas.DataFrame): A DataFrame containing the count of NaN values for each column in the DataFrame. nan_record (dict): A dictionary containing the same information as nan_record_df , with column names as keys and NaN counts as values. log_nan_record() -> dict : This method compares the NaN counts between the raw data and the homogenized data to check for any discrepancies. It logs discrepancies in NaN counts and returns a dictionary with the differences for each variable. Return : nan_check_dict (dict): A dictionary containing the difference in NaN counts between the raw and homogenized data for each variable. export_qc_data(export_path: str) : This method exports the NaN check results to the specified path. It exports the raw_nan_record_df , homogeneous_nan_record_df , and nan_diff_record_dict as CSV files or JSON files as appropriate. Arguments : export_path (str): The file path where the check records will be exported. Return : None","title":"Methods:"},{"location":"modules_documentation/qc_checks_utils_doc/#logs_2","text":"In method get_df_nan_record : If a column in the provided column list is not a subset of the columns in the DataFrame, a warning is logged indicating the missing variables. If there is an error while calculating the percentage of NaN values, an error is logged. In method log_nan_record : If there is a mismatch in the NaN counts for a variable between the raw and homogenized data, an error is logged with details of the discrepancy. If no discrepancies are found, an info log confirms that the NaN counts were successfully checked. In method export_qc_data : If there is any error while exporting the generated NaN check records, an error is logged with details about the exception.","title":"Logs:"},{"location":"modules_documentation/qc_checks_utils_doc/#conversionchecker","text":"","title":"ConversionChecker"},{"location":"modules_documentation/qc_checks_utils_doc/#description_3","text":"The ConversionChecker class is responsible for performing conversion checks in the cohort data. This class verifies if both numerical and categorical variables have been properly formatted during data processing. It inherits from GeneralChecker . Checks for discrepancies in numerical values (comparing means between the raw and homogenized data) and in categorical values (comparing category counts between the raw and homogenized data). It also allows exporting the results of these checks.","title":"Description:"},{"location":"modules_documentation/qc_checks_utils_doc/#attributes_3","text":"Attributes inherited from GeneralChecker : All attributes from the GeneralChecker class. mean_diff_dict (dict) : A dictionary containing the difference between means for each numerical variable in the raw data and the homogenized data. label_count_dict (dict) : contains the count of each level for every categorical variable. It also includes the count of not missing data of that specific variable. Its structure is as follows: { \"variable1\": { \"label_count\": { \"label1\": <int>, \"label2\": <int>, ... }, \"notna_count\": <int> } ... },","title":"Attributes:"},{"location":"modules_documentation/qc_checks_utils_doc/#methods_3","text":"check_numeric_conversion(var: str, mean_diff_dict: dict) -> tuple[bool, float] : This method checks if the mean of a numerical variable in the raw data matches the mean in the homogenized data, taking into account any conversion factors. If the difference exceeds a specified threshold ( alpha ) it logs an error. Arguments : var (str): The name of the numerical variable to check. mean_diff_dict (dict): A dictionary to store the mean differences for each variable. Return : Tuple : means_check (bool): True if no significant discrepancies are found, False otherwise. mean_diff (float): The difference in the means between the raw data and the homogenized data. check_category_conversion(var: str) -> tuple[bool, dict, int] : This method checks if the category counts for a categorical variable match between the raw data and the homogenized data, considering the specific variable mapping. If discrepancies are found, it logs a warning. Arguments : var (str): The name of the categorical variable to check. Return : category_check (bool): True if no discrepancies are found, False otherwise. check_format_conversion() -> dict : This method centralizes the format conversion checks for both numerical and categorical variables. It calls the appropriate check methods for each variable and updates the mean_diff_dict with the results of the numerical checks. Return : mean_diff_dict (dict): A dictionary containing the mean differences between the raw and homogenized data for all numerical variables. export_qc_data(export_path: str) : This method exports the results of the conversion checks to the specified path. It exports the mean_diff_dict as a JSON file. Arguments : export_path (str): The file path where the check records will be exported.","title":"Methods:"},{"location":"modules_documentation/qc_checks_utils_doc/#logs_3","text":"In method check_numeric_conversion : If the mean difference for a numerical variable exceeds the threshold, an error is logged with details about the discrepancy. In method check_category_conversion : If there is a mismatch in category counts between the raw and homogenized data, a warning is logged with the details of the discrepancy. In method check_format_conversion : Logs an informational message when the format conversion checks start. Logs the results of both numerical and categorical variable checks. In method export_qc_data : If there is any error while exporting the conversion check records, an error is logged with details about the exception.","title":"Logs:"},{"location":"modules_documentation/qc_checks_utils_doc/#consistancychecker","text":"","title":"ConsistancyChecker"},{"location":"modules_documentation/qc_checks_utils_doc/#description_4","text":"The ConsistancyChecker class is responsible for verifying the consistency of the cohort data. It checks if the final data (homogenized data) complies with the structure defined in the initial metadata (such as var_data , level_data , and panel_data files). This includes checking data types and ensuring that melted variables within panels have consistent data types. The class inherits from GeneralChecker and logs any discrepancies found during the consistency checks.","title":"Description:"},{"location":"modules_documentation/qc_checks_utils_doc/#attributes_4","text":"Attributes inherited from GeneralChecker : All attributes from the GeneralChecker class.","title":"Attributes:"},{"location":"modules_documentation/qc_checks_utils_doc/#methods_4","text":"check_data_types() -> None : This method checks if there are any discrepancies between the data types in the final homogenized data and the data types specified in the var_data metadata. If discrepancies are found, they are logged as warnings. Return : None panel_consistancy_datatypes_check(panel_data_json: dict[str, dict]) -> None : This method checks for consistency in data types within panels. Specifically, it verifies that all melted variables in a panel have the same data type. If discrepancies are found, they are logged as warnings. Arguments : panel_data_json (dict): A dictionary containing metadata about the structure of the panels. This is used to check the consistency of data types within the panels. Return : None","title":"Methods:"},{"location":"modules_documentation/qc_checks_utils_doc/#logs_4","text":"In method check_data_types : If there are discrepancies between the data types in the homogenized data and those specified in the metadata, a warning is logged with details of the discrepancies. In method panel_consistancy_datatypes_check : If there are discrepancies in data types within a panel (i.e., melted variables with different data types), a warning is logged with details of the panel and the conflicting data types.","title":"Logs:"},{"location":"modules_documentation/qc_checks_utils_doc/#outlierschecker","text":"","title":"OutliersChecker"},{"location":"modules_documentation/qc_checks_utils_doc/#description_5","text":"The OutliersChecker class inherits from the GeneralChecker class and is responsible for detecting outliers in numerical data using a z-score threshold (> 2) and reporting the main percentiles for each variable. This class extends the functionality of GeneralChecker to enable comprehensive outlier detection across all numerical columns in the dataset.","title":"Description:"},{"location":"modules_documentation/qc_checks_utils_doc/#attributes_5","text":"Attributes inherited from GeneralChecker : All attributes from the GeneralChecker class. cohort_outliers_z_score ( dict ): A dictionary containing outliers for all numerical variables in the dataset. Each key represents a variable name, and its value represents the outliers count according to the set threshold (Default set to 2) cohort_outliers_percentile ( dict ): A dictionary containing the main percentiles for each numerical variable.","title":"Attributes:"},{"location":"modules_documentation/qc_checks_utils_doc/#methods_5","text":"get_column_outliers_percentile(df: pd.DataFrame, column: str, percentiles: list[float] = [0, 0.025, 0.25, 0.50, 0.75, 0.975, 1]) -> dict : This method calculates a set of percentiles for a specified variable (column) and returns them as a dictionary. It is intended to be used in get_cohort_outliers along with get_column_outliers_z_score for each numerical variable in the dataset. Arguments : df (pandas.DataFrame): The DataFrame containing the data to be analyzed. column (str): The name of the column/variable for which the percentiles will be calculated. percentiles (list[float]): A list of percentiles (in decimal form) to be calculated. Default is [0, 0.025, 0.25, 0.50, 0.75, 0.975, 1] . Return : percentiles_dict (dict): A dictionary containing the calculated value for each percentile defined in the percentiles argument. The structure is {percentile: value, ...} . get_column_outliers_z_score(df: pd.DataFrame, column: str, z_score_bound: float = 2, id_var: str = 'cohort_id') -> dict : This method calculates the z-score of a specified numerical variable (column) and identifies outliers based on the specified z_score_bound . It returns a dictionary with the count of outliers and the count of non-missing values for that column. This method is intended to be used in get_cohort_outliers along with get_column_outliers_percentile for each numerical variable in the dataset. Arguments : df (pandas.DataFrame): The DataFrame containing the data to be analyzed. column (str): The name of the column/variable for which the z-score will be calculated and outliers will be identified. z_score_bound (float): The threshold for identifying an outlier. If the absolute value of the z-score exceeds this parameter, the observation is considered an outlier. Default is 2 . id_var (str): The variable used as the identifier for each observation (e.g., cohort_id or liervaim_id ). Default is 'cohort_id' . Return : outliers_dict (dict): A dictionary with the count of outliers and the count of non-missing values for the specified column. The structure is {'outliers_count': int, 'not_missings_count': int } . get_cohort_outliers() -> tuple[dict, dict] : This method iterates through each numerical column in self.homogeneous_data , calling the get_column_outliers_z_score and get_column_outliers_percentile methods for each column. The results are stored in two dictionaries, one for outliers identified by the z-score and one for percentiles. Return : tuple(dict, dict) : A tuple containing two dictionaries: Z-score Outliers Dictionary (dict) : Contains the outliers identified using the z-score method. Each key is a variable name, and each value is the count of outliers for that variable. Percentiles Dictionary (dict) : Contains the percentiles for each numerical variable. Each key is a variable name, and each value is a dictionary structured as {percentile1: value, percentile2: value, ...} . export_qc_data(export_path: str) -> None : This method exports data related to the outliers analysis to the specified export_path . Specifically, it exports the dictionaries cohort_outliers_name_z_score and cohort_outliers_name_percentile . Arguments : export_path (str): The path where the quality check record will be saved.","title":"Methods:"},{"location":"modules_documentation/qc_checks_utils_doc/#logs_5","text":"In method export_qc_data : If there is any error while exporting the outlier data to the file, an error is logged with the relevant details.","title":"Logs:"},{"location":"modules_documentation/qc_checks_utils_doc/#qcchecker","text":"","title":"QCChecker"},{"location":"modules_documentation/qc_checks_utils_doc/#description_6","text":"The QCChecker class centralizes the quality control (QC) checks for the cohort data. This class orchestrates the execution of all other checker classes, including NonNumericChecker , NaNChecker , ConversionChecker , ConsistancyChecker and OtuliersChecker to ensure data quality. It also provides methods to generate cohort reports and export QC results. The class inherits from GeneralChecker .","title":"Description:"},{"location":"modules_documentation/qc_checks_utils_doc/#attributes_6","text":"Attributes inherited from GeneralChecker : All attributes from the GeneralChecker class. non_numeric_checker : An instance of the NonNumericChecker class, responsible for checking non-numeric values in the cohort data. nan_checker : An instance of the NaNChecker class, responsible for checking NaN values in the cohort data. conversion_checker : An instance of the ConversionChecker class, responsible for verifying the conversion of numerical and categorical variables. consistency_checker : An instance of the ConsistancyChecker class, responsible for checking the consistency of the final data with the initial metadata. outlier_checker : An instance of the OutliersChecker class, responsible for recording the count of outliers for each numerical variable (and the corresponding ID's). export_qc_record (bool) : A boolean value indicating whether the QC records should be exported. This is configured via the config module.","title":"Attributes:"},{"location":"modules_documentation/qc_checks_utils_doc/#methods_6","text":"cohort_report() -> None : This method logs a brief description of the raw data and homogenized data from the cohort. It reports the number of variables and patients in each dataset. Return : None export_qc_reports() -> None : This method centralizes the process of exporting the QC results. It first creates the directory where the QC data will be stored (see output description ), then calls the export methods of the individual checker classes to save the QC results to files. Return : None check_numeric_vars_coherence(data: pandas.DataFrame, var_data_json: dict) -> pandas.DataFrame : This method checks the coherence of numeric variables across different cohorts. It calculates the mean and standard deviation for each numeric variable in the selected variables and returns a DataFrame summarizing this information. The aim of the method is to detect if there is any odd behaviour between data from the different cohorts (such as huge difference in mean or sd between cohorts). This method must be used with the merged-homogenized data. Arguments : data (pandas.DataFrame): The homogenized and merged data from all cohorts. var_data_json (dict): A dictionary containing metadata about the variables, including data types. Return : combined_data_df (pandas.DataFrame): A DataFrame containing the mean and standard deviation for each numeric variable, grouped by cohort.","title":"Methods:"},{"location":"modules_documentation/qc_checks_utils_doc/#logs_6","text":"In method cohort_report : Logs an informational message summarizing the structure of both the raw data and homogenized data, including the number of variables and patients in each. In method export_qc_reports : If there is any error while exporting the QC results, an error is logged with details about the exception.","title":"Logs:"},{"location":"modules_documentation/qc_checks_utils_doc/#output-description","text":"Several results generated during quality controls are exported if the QC_EXPORT_RECORD variable (declared in the main_config module) is set to True . The QC_REPORT_FOLDER directory stores all results related to quality control. This directory contains the code necessary to create the QC report if desired. QC_REPORT_FOLDER also contains a subfolder, defined by the QC_DATA_FOLDER variable, where all the raw outputs generated during QC are saved. Each execution (if QC_EXPORT_RECORD is set to True ) will generate a directory within QC_DATA_FOLDER named: qc_execution_yyyy-mm-dd_hh-mm-ss Where yyyy-mm-dd_hh-mm-ss corresponds to the EXECUTION_DATETIME . This directory will store, for each cohort, the files containing the QC-related data. Thus, the general structure of QC_DATA_FOLDER could be something like this: QC_REPORT_FOLDER/ \u251c\u2500\u2500 qc_report.qmd # Quarto/md file to create the report. \u2514\u2500\u2500 QC_DATA_FOLDER/ \u251c\u2500\u2500 qc_execution_2024-08-12_13-42-42/ \u2502 \u251c\u2500\u2500 Liverscreen \u2502 \u2502 \u2514\u2500\u2500 <QC_data_files> \u2502 \u251c\u2500\u2500 Alcofib \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 qc_execution_2024-08-12_13-42-42/ \u2514\u2500\u2500 ... The resulting files for each cohort are as follows: raw_nan_record_df.csv : Contains the count of NaN values for the core/selected variables of the raw data ( raw_data ). Variable Liverscreen NaN Count total_records % NaN 0 age_0 0 23624 0.00% 1 ethnicity 37 23624 0.16% 2 gender 0 23624 0.00% 3 hypertension 82 23624 0.35% 4 centralobesity 19 23624 0.08% 5 hightrigly 488 23624 2.07% ... homogeneous_nan_record_df.csv : Contains the count of NaN values for the selected/core variables of the homogeneous data ( homogeneous_data ). Variable Liverscreen NaN Count total_records % NaN 0 age 0 23624 0.00% 1 ethnicity 37 23624 0.16% 2 gender 0 23624 0.00% 3 hypertension 82 23624 0.35% 4 centralobesity 19 23624 0.08% 5 hightrigly 488 23624 0.00% ... non_numeric_record_df.csv : Records non-numeric values (such as -inf, inf) found in the numeric variables of the homogeneous data. Variable non_numeric_values_count 0 age 0 1 weight 0 2 height 0 3 bmi 0 4 hip 0 5 te 0 ... nan_diff_record_dict.json : Contains the differences in NaN counts between the raw and homogeneous data for each variable. { \"age\": 0, \"ethnicity\": 0, \"gender\": 0, \"hypertension\": 0, \"centralobesity\": 0, ... } mean_difference_dict.json : Records the differences in means between the raw and homogeneous data for numeric variables, after applying the conversion factor. { \"age\": 0.0, \"weight\": 0.0, \"height\": 0.0, \"bmi\": 0.0, \"hip\": 0.0, \"te\": 0.0, ... } percentile_outliers.json : Records the precentiles specified during the execution of get_column_outliers_percentile method for each numeric variable. The format is the following: ```json { \"age\": { \"0.0\": 36.0, \"0.025\": 45.0, \"0.25\": 62.0, \"0.5\": 71.0, \"0.75\": 77.0, \"0.975\": 84.375, \"1.0\": 88.0 }, \"weight\": { \"0.0\": 44.0, \"0.025\": 53.625, \"0.25\": 69.0, \"0.5\": 79.0, \"0.75\": 89.0, \"0.975\": 117.0, \"1.0\": 160.0 }, ... } ``` z_score_outliers.json : Records both the outliers according to the z-score value (and the specifided threshold, default set to 2) and the count of not missing data for each variable (so the % of outliers can be calculated in the qc report) ```json { \"age\": { \"outliers_count\": 20, \"not_missings_count\": 466 }, \"weight\": { \"outliers_count\": 17, \"not_missings_count\": 466 }, \"height\": { \"outliers_count\": 18, \"not_missings_count\": 466 }, ... } ```","title":"Output Description"},{"location":"modules_documentation/qc_checks_utils_doc/#usage-in-main-execution","text":"The use of this module during the main execution of the code is through the instantiation of the QCChecker . Each time a Cohort class is instantiated (and the database is internally processed to obtain the homogeneous_data ), a QCChecker object is created for that cohort (i.e., for each cohort listed in COHORT_NAME_LIST from the cohort_config module). When QCChecker is instantiated, it automatically creates instances of all the classes responsible for various validations and checks as its attributes (see QCChecker class ). Each of these attributes will execute its respective validation and check methods automatically, storing the generated records in their own attributes. If the QC_EXPORT_RECORD variable from the main_config module is set to True , the export_qc_reports method of the QCChecker will be executed in the main code and quality control records for this specific cohort will be exported (see output descrption ). After all the cohorts have been created and merged into the final Liveraim cohort, another instance of the QCChecker class is created to perform a quality control check on this final cohort. In this case, quality control checks related to the structure of the final panels will also be applied through the panel_consistancy_datatypes_check method of the ConsistancyChecker class.","title":"Usage in main execution"},{"location":"modules_documentation/qc_checks_utils_doc/#other-complementary-functions","text":"Not yet implemented","title":"Other complementary functions"},{"location":"modules_documentation/sql_exporting_utils_doc/","text":"sql_exporting_utils Documentation Overview The SQLExporter module is responsible for creating the structure of a MySQL database, handling connections, and exporting processed data into the database. It relies on SQLAlchemy ORM for creating tables, establishing relationships, and managing connections to the database. The primary functionality of this module is centralized in the SQLExporter class. General Structure The module defines the following key components: Base : This is the base class used by SQLAlchemy for defining metadata and creating the ORM mappings. All table classes in this module inherit from this base class. For further information, visit the SQLAlchemy documentation . SQLExporter : This is the main class that manages the creation of tables, connection to the database, and the export of cohort data to the MySQL database. It uses SQLAlchemy's ORM to handle the database schema and pandas to export dataframes. Class Description SQLExporter Description: The SQLExporter class handles the creation of MySQL tables based on the cohort data, establishes a connection to the MySQL database, and exports processed cohort data to the database. It uses the SQLAlchemy ORM to define the table schema and manage database interactions. Attributes: panel_data_json (dict) : A dictionary containing metadata about the cohort panels (tables) and their columns. engine (Engine) : The SQLAlchemy engine object that manages the connection to the MySQL database. base (type) : The SQLAlchemy Base class used to define ORM mappings. metadata (MetaData) : SQLAlchemy's metadata object, which contains information about all the tables defined in the schema. type_mapping (dict) : A dictionary that maps Python data types to SQLAlchemy data types. Methods: set_engine() -> Engine : This method initializes and returns the SQLAlchemy engine for connecting to the MySQL database. The connection parameters are retrieved from environment variables using the connection_config module. Return : Engine : The SQLAlchemy engine for the MySQL connection. create_table_class(name: str, table_json: dict, type_mapping: dict) -> type : Dynamically creates a table class based on the provided metadata ( table_json ). The method defines columns, data types, and sets primary/foreign keys based on whether the table is in wide or long format. Arguments : name (str): The name of the table. table_json (dict): Metadata about the table, including columns and data types. type_mapping (dict): A mapping of Python data types to SQLAlchemy data types. Return : cls : The dynamically created class representing the table. is_long_format(table_json: dict) -> bool : Determines if a table is in long format (melted) based on the configuration data. Arguments : table_json (dict): configuration data for the table. Return : bool : Returns True if the table is in long format, False otherwise. get_value_datatype(table_json: dict) -> str : Retrieves the data type for the 'value' column in long-format tables by checking if there are any variables to be melted in the table_json configuration dictionary. Arguments : table_json (dict): Metadata for the table. Return : str : The data type of the 'value' column. define_final_columns(table_json: dict, long_format: bool) -> dict : Defines the final columns and their data types for a given table based on the metadata. If the table is in long format, additional columns like 'variable' and 'value' are added. Arguments : table_json (dict): configuration data for the table. long_format (bool): Whether the table is in long or wide format. Return : dict : A dictionary mapping column names to their data types. create_tables() -> None : For each table specified in panel_data_json (except the population panel), creates its corresponding ORM class by calling create_table_class . create_all() -> None : Creates all the ORM table classes specified in panel_data_json and calls the SQLAlchemy method metadata.create_all to create the tables in the MySQL database. export_data_to_sql(final_data: dict[str, pd.DataFrame], if_exists: Literal['fail', 'replace', 'append'] = 'append') -> None : Exports the final cohort data to the MySQL database. The data is exported using pandas' to_sql method, with options to handle existing data (fail, replace, append). Arguments : final_data (dict[str, pd.DataFrame]): A dictionary where keys are panel names and values are pandas DataFrames representing the panel data. if_exists (Literal['fail', 'replace', 'append']): Determines what happens if the table already exists in the database. Default is 'append'. export_df_to_sql(self, df: pd.DataFrame, table_name: str, if_exists: Literal['fail', 'replace', 'append'] =\"append\") -> None: A simple method to dump a single dataframe into the database. Uses the pandas method pd.to_sql and the engine created in the class. Arguments : - df (pd.DataFrame): dataframe to be dumped in the MySQL database - table_name (str): name of the table where the dataframe should be dumped. - if_exists (Literal['fail', 'replace', 'append'], optional): Specifies the behavior if the table already exists in the database: - ' fail ': If the table exists, an error is raised. - ' replace ': The existing table is dropped, and a new one is created. - ' append ': The new data is appended to the existing table. Default is 'append'. drop_all() -> None : Drops all the tables from the MySQL database. verify_tables() -> None : Verifies the existence of tables in the database and logs their names if present. test_connection() -> bool : Tests the connection to the MySQL database by executing a simple query. Returns True if the connection is successful, False otherwise. Logs In method set_engine : Logs an error if any enviroment variable needed for the connection couldn't be called. Logs if the engine was successfully created or there was an error while creating it. In create_table_class : Logs an error If any data type in the configuration data ( final_columns parameter) is not compatible (i.e. is not in the type_mapping attribute). Logs if the table ORM class has been succesfully created or if there has been an error while creating it (i.e. while calling metaclass type ) In get_value_datatype : Logs an error if there are two or more different datatypes in the variables that have to be melted (transformed into long format) in a specific panel. Logs an error if there are no variables to be melted. In export_data_to_sql : Logs if everte panel has been exported to MySQL properly or if, on the contrary, an error ocurred while exporting the data. In export_df_to_sql : Logs if the dataframe has been exported to MySQL properly or if, on the contrary, an error ocurred while exporting the data. In drop_all : If no tables are found in the database, it logs an info message. For every dropped table, it logs an info message. If a specific table to drop is not found in metadata, logs a message indicating that it skips this table. If an error ocurrs while dropping a table, it is logged. In verify_tables : If an error ocurrs while inspecting the database and getting the table names, it is logged. It logs the list of tables in the database (if any). If no tables are found, an info message is logged indiciating so. In test_connection : If the connection test was successful, logs a message indicating. Otherwise logs an error message. General operation It is important to mention that SQLExporter utilizes the ORM (Object-Relational Mapping) framework provided by SQLAlchemy. This means that future SQL database tables must be treated as Python classes in the code. For example, if you wish to create a table called population , you must first define a Python class (which, for technical reasons, should inherit from Base ), let's call it Population . This class will define the metadata for the table through its attributes. When an instance of the SQLExporter class is created (which requires an object of the panel_data type), an Engine object is automatically created. The Engine is responsible for managing database connections as they are needed. Note : The Engine object follows a \"lazy\" connection paradigm. This means that it does not establish a connection to the database until it is actually required. Therefore, when the Engine is created, no immediate connection is made. One consequence of this behavior is that, although the Engine can be created without errors, the actual database connection may still fail. To verify the connection, it is recommended to run the test_connection method or execute the module as the main program. Additionally, the SQLExporter class creates the attributes base (which uses the previously defined Base class) and metadata (an attribute of the Base class). For more detailed information on the internal workings of Base and metadata , refer to the SQLAlchemy documentation . In general terms, the Base class acts as a \"repository\" where all tables and relationships (defined as classes) are stored within the SQLExporter . To create a table in the database, it must first be declared as a class in Python. For the class to be registered in Base , it must inherit from Base . This allows Base to record all tables/classes and their configurations, so that they can later be created in the SQL database when the connection is established. When the SQLExporter class is instantiated, the Population table/class is stored within Base . After instantiating the class, the create_all method is called. This method dynamically creates the table/class for each panel in the panel_data_json . It does so using the methods previously defined. For each panel defined in the panel_data_json dictionary: It checks if the panel follows a long or wide format. If the panel is in long format, it extracts the data type of the future values column. It runs the define_final_columns method, which generates a dictionary containing the metadata for each final column (taking into account the final table structure), as well as relationships, primary keys, and foreign keys. It executes the create_table_class method, which creates a table/class using the parameters obtained from define_final_columns . Once the tables are created and stored in Base , the create_all method is called. At this point, the first explicit connection to the database is made: the tables stored in Base (along with their respective configurations) are now created in the actual MySQL database. Finally, the export_data_to_sql method (called from main ) is executed, taking as a parameter the final data that needs to be exported to SQL. This method reconnects to the database and loads the data into the corresponding tables. Example Usage Creating and Exporting Data from sql_exporter import SQLExporter import pandas as pd # Example metadata panel_data_json = { 'population': {...}, # Population panel metadata 'panel_1': {...}, # Panel 1 metadata # Other panels... } # Example data final_data = { 'population': pd.DataFrame(...), 'panel_1': pd.DataFrame(...), # Other panels... } # Instantiate SQLExporter sql_exporter = SQLExporter(panel_data_json) # Create the tables in MySQL sql_exporter.create_all() # Export the data to MySQL sql_exporter.export_data_to_sql(final_data) Testing the connection: You can test if the connection cnfiguration is correct by calling the method test_connection or by executiong the module as the main program. To do so, just execute the following comand in the terminal (make sure you are in the proper directory)\u00e7 python sql_exporting_utils.py Output Description Consulta la secci\u00f3n Usage in the main execution In the context of database creation, an instance of the SQLExporter class is created using liveraim.panel_data_json as a parameter. Next, the database tables are created, and finally, the final data (previously created and stored in the Liveraim.final_data attribute) is exported. if config.CREATE_SQL_DB: sql_exporter = SQLExporter(liveraim.panel_data_json) sql_exporter.create_all() sql_exporter.export_data_to_sql(liveraim.final_data)","title":"SQL Exporting utils"},{"location":"modules_documentation/sql_exporting_utils_doc/#sql_exporting_utils-documentation","text":"","title":"sql_exporting_utils Documentation"},{"location":"modules_documentation/sql_exporting_utils_doc/#overview","text":"The SQLExporter module is responsible for creating the structure of a MySQL database, handling connections, and exporting processed data into the database. It relies on SQLAlchemy ORM for creating tables, establishing relationships, and managing connections to the database. The primary functionality of this module is centralized in the SQLExporter class.","title":"Overview"},{"location":"modules_documentation/sql_exporting_utils_doc/#general-structure","text":"The module defines the following key components: Base : This is the base class used by SQLAlchemy for defining metadata and creating the ORM mappings. All table classes in this module inherit from this base class. For further information, visit the SQLAlchemy documentation . SQLExporter : This is the main class that manages the creation of tables, connection to the database, and the export of cohort data to the MySQL database. It uses SQLAlchemy's ORM to handle the database schema and pandas to export dataframes.","title":"General Structure"},{"location":"modules_documentation/sql_exporting_utils_doc/#class-description","text":"","title":"Class Description"},{"location":"modules_documentation/sql_exporting_utils_doc/#sqlexporter","text":"","title":"SQLExporter"},{"location":"modules_documentation/sql_exporting_utils_doc/#description","text":"The SQLExporter class handles the creation of MySQL tables based on the cohort data, establishes a connection to the MySQL database, and exports processed cohort data to the database. It uses the SQLAlchemy ORM to define the table schema and manage database interactions.","title":"Description:"},{"location":"modules_documentation/sql_exporting_utils_doc/#attributes","text":"panel_data_json (dict) : A dictionary containing metadata about the cohort panels (tables) and their columns. engine (Engine) : The SQLAlchemy engine object that manages the connection to the MySQL database. base (type) : The SQLAlchemy Base class used to define ORM mappings. metadata (MetaData) : SQLAlchemy's metadata object, which contains information about all the tables defined in the schema. type_mapping (dict) : A dictionary that maps Python data types to SQLAlchemy data types.","title":"Attributes:"},{"location":"modules_documentation/sql_exporting_utils_doc/#methods","text":"set_engine() -> Engine : This method initializes and returns the SQLAlchemy engine for connecting to the MySQL database. The connection parameters are retrieved from environment variables using the connection_config module. Return : Engine : The SQLAlchemy engine for the MySQL connection. create_table_class(name: str, table_json: dict, type_mapping: dict) -> type : Dynamically creates a table class based on the provided metadata ( table_json ). The method defines columns, data types, and sets primary/foreign keys based on whether the table is in wide or long format. Arguments : name (str): The name of the table. table_json (dict): Metadata about the table, including columns and data types. type_mapping (dict): A mapping of Python data types to SQLAlchemy data types. Return : cls : The dynamically created class representing the table. is_long_format(table_json: dict) -> bool : Determines if a table is in long format (melted) based on the configuration data. Arguments : table_json (dict): configuration data for the table. Return : bool : Returns True if the table is in long format, False otherwise. get_value_datatype(table_json: dict) -> str : Retrieves the data type for the 'value' column in long-format tables by checking if there are any variables to be melted in the table_json configuration dictionary. Arguments : table_json (dict): Metadata for the table. Return : str : The data type of the 'value' column. define_final_columns(table_json: dict, long_format: bool) -> dict : Defines the final columns and their data types for a given table based on the metadata. If the table is in long format, additional columns like 'variable' and 'value' are added. Arguments : table_json (dict): configuration data for the table. long_format (bool): Whether the table is in long or wide format. Return : dict : A dictionary mapping column names to their data types. create_tables() -> None : For each table specified in panel_data_json (except the population panel), creates its corresponding ORM class by calling create_table_class . create_all() -> None : Creates all the ORM table classes specified in panel_data_json and calls the SQLAlchemy method metadata.create_all to create the tables in the MySQL database. export_data_to_sql(final_data: dict[str, pd.DataFrame], if_exists: Literal['fail', 'replace', 'append'] = 'append') -> None : Exports the final cohort data to the MySQL database. The data is exported using pandas' to_sql method, with options to handle existing data (fail, replace, append). Arguments : final_data (dict[str, pd.DataFrame]): A dictionary where keys are panel names and values are pandas DataFrames representing the panel data. if_exists (Literal['fail', 'replace', 'append']): Determines what happens if the table already exists in the database. Default is 'append'. export_df_to_sql(self, df: pd.DataFrame, table_name: str, if_exists: Literal['fail', 'replace', 'append'] =\"append\") -> None: A simple method to dump a single dataframe into the database. Uses the pandas method pd.to_sql and the engine created in the class. Arguments : - df (pd.DataFrame): dataframe to be dumped in the MySQL database - table_name (str): name of the table where the dataframe should be dumped. - if_exists (Literal['fail', 'replace', 'append'], optional): Specifies the behavior if the table already exists in the database: - ' fail ': If the table exists, an error is raised. - ' replace ': The existing table is dropped, and a new one is created. - ' append ': The new data is appended to the existing table. Default is 'append'. drop_all() -> None : Drops all the tables from the MySQL database. verify_tables() -> None : Verifies the existence of tables in the database and logs their names if present. test_connection() -> bool : Tests the connection to the MySQL database by executing a simple query. Returns True if the connection is successful, False otherwise.","title":"Methods:"},{"location":"modules_documentation/sql_exporting_utils_doc/#logs","text":"In method set_engine : Logs an error if any enviroment variable needed for the connection couldn't be called. Logs if the engine was successfully created or there was an error while creating it. In create_table_class : Logs an error If any data type in the configuration data ( final_columns parameter) is not compatible (i.e. is not in the type_mapping attribute). Logs if the table ORM class has been succesfully created or if there has been an error while creating it (i.e. while calling metaclass type ) In get_value_datatype : Logs an error if there are two or more different datatypes in the variables that have to be melted (transformed into long format) in a specific panel. Logs an error if there are no variables to be melted. In export_data_to_sql : Logs if everte panel has been exported to MySQL properly or if, on the contrary, an error ocurred while exporting the data. In export_df_to_sql : Logs if the dataframe has been exported to MySQL properly or if, on the contrary, an error ocurred while exporting the data. In drop_all : If no tables are found in the database, it logs an info message. For every dropped table, it logs an info message. If a specific table to drop is not found in metadata, logs a message indicating that it skips this table. If an error ocurrs while dropping a table, it is logged. In verify_tables : If an error ocurrs while inspecting the database and getting the table names, it is logged. It logs the list of tables in the database (if any). If no tables are found, an info message is logged indiciating so. In test_connection : If the connection test was successful, logs a message indicating. Otherwise logs an error message.","title":"Logs"},{"location":"modules_documentation/sql_exporting_utils_doc/#general-operation","text":"It is important to mention that SQLExporter utilizes the ORM (Object-Relational Mapping) framework provided by SQLAlchemy. This means that future SQL database tables must be treated as Python classes in the code. For example, if you wish to create a table called population , you must first define a Python class (which, for technical reasons, should inherit from Base ), let's call it Population . This class will define the metadata for the table through its attributes. When an instance of the SQLExporter class is created (which requires an object of the panel_data type), an Engine object is automatically created. The Engine is responsible for managing database connections as they are needed. Note : The Engine object follows a \"lazy\" connection paradigm. This means that it does not establish a connection to the database until it is actually required. Therefore, when the Engine is created, no immediate connection is made. One consequence of this behavior is that, although the Engine can be created without errors, the actual database connection may still fail. To verify the connection, it is recommended to run the test_connection method or execute the module as the main program. Additionally, the SQLExporter class creates the attributes base (which uses the previously defined Base class) and metadata (an attribute of the Base class). For more detailed information on the internal workings of Base and metadata , refer to the SQLAlchemy documentation . In general terms, the Base class acts as a \"repository\" where all tables and relationships (defined as classes) are stored within the SQLExporter . To create a table in the database, it must first be declared as a class in Python. For the class to be registered in Base , it must inherit from Base . This allows Base to record all tables/classes and their configurations, so that they can later be created in the SQL database when the connection is established. When the SQLExporter class is instantiated, the Population table/class is stored within Base . After instantiating the class, the create_all method is called. This method dynamically creates the table/class for each panel in the panel_data_json . It does so using the methods previously defined. For each panel defined in the panel_data_json dictionary: It checks if the panel follows a long or wide format. If the panel is in long format, it extracts the data type of the future values column. It runs the define_final_columns method, which generates a dictionary containing the metadata for each final column (taking into account the final table structure), as well as relationships, primary keys, and foreign keys. It executes the create_table_class method, which creates a table/class using the parameters obtained from define_final_columns . Once the tables are created and stored in Base , the create_all method is called. At this point, the first explicit connection to the database is made: the tables stored in Base (along with their respective configurations) are now created in the actual MySQL database. Finally, the export_data_to_sql method (called from main ) is executed, taking as a parameter the final data that needs to be exported to SQL. This method reconnects to the database and loads the data into the corresponding tables.","title":"General operation"},{"location":"modules_documentation/sql_exporting_utils_doc/#example-usage","text":"","title":"Example Usage"},{"location":"modules_documentation/sql_exporting_utils_doc/#creating-and-exporting-data","text":"from sql_exporter import SQLExporter import pandas as pd # Example metadata panel_data_json = { 'population': {...}, # Population panel metadata 'panel_1': {...}, # Panel 1 metadata # Other panels... } # Example data final_data = { 'population': pd.DataFrame(...), 'panel_1': pd.DataFrame(...), # Other panels... } # Instantiate SQLExporter sql_exporter = SQLExporter(panel_data_json) # Create the tables in MySQL sql_exporter.create_all() # Export the data to MySQL sql_exporter.export_data_to_sql(final_data)","title":"Creating and Exporting Data"},{"location":"modules_documentation/sql_exporting_utils_doc/#testing-the-connection","text":"You can test if the connection cnfiguration is correct by calling the method test_connection or by executiong the module as the main program. To do so, just execute the following comand in the terminal (make sure you are in the proper directory)\u00e7 python sql_exporting_utils.py","title":"Testing the connection:"},{"location":"modules_documentation/sql_exporting_utils_doc/#output-description","text":"Consulta la secci\u00f3n","title":"Output Description"},{"location":"modules_documentation/sql_exporting_utils_doc/#usage-in-the-main-execution","text":"In the context of database creation, an instance of the SQLExporter class is created using liveraim.panel_data_json as a parameter. Next, the database tables are created, and finally, the final data (previously created and stored in the Liveraim.final_data attribute) is exported. if config.CREATE_SQL_DB: sql_exporter = SQLExporter(liveraim.panel_data_json) sql_exporter.create_all() sql_exporter.export_data_to_sql(liveraim.final_data)","title":"Usage in the main execution"}]}